{"version":3,"file":"index.js","sourceRoot":"","sources":["../../../../../src/generated/service/models/index.ts"],"names":[],"mappings":"AAAA;;;;;;GAMG","sourcesContent":["/*\n * Copyright (c) Microsoft Corporation. All rights reserved.\n * Licensed under the MIT License. See License.txt in the project root for license information.\n *\n * Code generated by Microsoft (R) AutoRest Code Generator.\n * Changes may cause incorrect behavior and will be lost if the code is regenerated.\n */\n\n\nimport * as coreHttp from \"@azure/core-http\";\n\n/**\n * Specifies some text and analysis components used to break that text into tokens.\n */\nexport interface AnalyzeRequest {\n  /**\n   * The text to break into tokens.\n   */\n  text: string;\n  /**\n   * The name of the analyzer to use to break the given text. If this parameter is not specified,\n   * you must specify a tokenizer instead. The tokenizer and analyzer parameters are mutually\n   * exclusive. KnownAnalyzerNames is an enum containing known values.\n   */\n  analyzer?: string;\n  /**\n   * The name of the tokenizer to use to break the given text. If this parameter is not specified,\n   * you must specify an analyzer instead. The tokenizer and analyzer parameters are mutually\n   * exclusive. KnownTokenizerNames is an enum containing known values.\n   */\n  tokenizer?: string;\n  /**\n   * An optional list of token filters to use when breaking the given text. This parameter can only\n   * be set when using the tokenizer parameter.\n   */\n  tokenFilters?: string[];\n  /**\n   * An optional list of character filters to use when breaking the given text. This parameter can\n   * only be set when using the tokenizer parameter.\n   */\n  charFilters?: string[];\n}\n\n/**\n * Information about a token returned by an analyzer.\n */\nexport interface AnalyzedTokenInfo {\n  /**\n   * The token returned by the analyzer.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly token: string;\n  /**\n   * The index of the first character of the token in the input text.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly startOffset: number;\n  /**\n   * The index of the last character of the token in the input text.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly endOffset: number;\n  /**\n   * The position of the token in the input text relative to other tokens. The first token in the\n   * input text has position 0, the next has position 1, and so on. Depending on the analyzer used,\n   * some tokens might have the same position, for example if they are synonyms of each other.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly position: number;\n}\n\n/**\n * The result of testing an analyzer on text.\n */\nexport interface AnalyzeResult {\n  /**\n   * The list of tokens returned by the analyzer specified in the request.\n   */\n  tokens: AnalyzedTokenInfo[];\n}\n\n/**\n * Contains the possible cases for LexicalAnalyzer.\n */\nexport type LexicalAnalyzerUnion = LexicalAnalyzer | CustomAnalyzer | PatternAnalyzer | LuceneStandardAnalyzer | StopAnalyzer;\n\n/**\n * Base type for analyzers.\n */\nexport interface LexicalAnalyzer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"LexicalAnalyzer\";\n  /**\n   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,\n   * can only start and end with alphanumeric characters, and is limited to 128 characters.\n   */\n  name: string;\n}\n\n/**\n * Allows you to take control over the process of converting text into indexable/searchable tokens.\n * It's a user-defined configuration consisting of a single predefined tokenizer and one or more\n * filters. The tokenizer is responsible for breaking text into tokens, and the filters for\n * modifying tokens emitted by the tokenizer.\n */\nexport interface CustomAnalyzer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.CustomAnalyzer\";\n  /**\n   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,\n   * can only start and end with alphanumeric characters, and is limited to 128 characters.\n   */\n  name: string;\n  /**\n   * The name of the tokenizer to use to divide continuous text into a sequence of tokens, such as\n   * breaking a sentence into words. KnownTokenizerNames is an enum containing known values.\n   */\n  tokenizer: string;\n  /**\n   * A list of token filters used to filter out or modify the tokens generated by a tokenizer. For\n   * example, you can specify a lowercase filter that converts all characters to lowercase. The\n   * filters are run in the order in which they are listed.\n   */\n  tokenFilters?: string[];\n  /**\n   * A list of character filters used to prepare input text before it is processed by the\n   * tokenizer. For instance, they can replace certain characters or symbols. The filters are run\n   * in the order in which they are listed.\n   */\n  charFilters?: string[];\n}\n\n/**\n * Flexibly separates text into terms via a regular expression pattern. This analyzer is\n * implemented using Apache Lucene.\n */\nexport interface PatternAnalyzer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.PatternAnalyzer\";\n  /**\n   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,\n   * can only start and end with alphanumeric characters, and is limited to 128 characters.\n   */\n  name: string;\n  /**\n   * A value indicating whether terms should be lower-cased. Default is true. Default value: true.\n   */\n  lowerCaseTerms?: boolean;\n  /**\n   * A regular expression pattern to match token separators. Default is an expression that matches\n   * one or more non-word characters. Default value: '\\W+'.\n   */\n  pattern?: string;\n  /**\n   * Regular expression flags.\n   */\n  flags?: string;\n  /**\n   * A list of stopwords.\n   */\n  stopwords?: string[];\n}\n\n/**\n * Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop\n * filter.\n */\nexport interface LuceneStandardAnalyzer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.StandardAnalyzer\";\n  /**\n   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,\n   * can only start and end with alphanumeric characters, and is limited to 128 characters.\n   */\n  name: string;\n  /**\n   * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The\n   * maximum token length that can be used is 300 characters. Default value: 255.\n   */\n  maxTokenLength?: number;\n  /**\n   * A list of stopwords.\n   */\n  stopwords?: string[];\n}\n\n/**\n * Divides text at non-letters; Applies the lowercase and stopword token filters. This analyzer is\n * implemented using Apache Lucene.\n */\nexport interface StopAnalyzer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.StopAnalyzer\";\n  /**\n   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,\n   * can only start and end with alphanumeric characters, and is limited to 128 characters.\n   */\n  name: string;\n  /**\n   * A list of stopwords.\n   */\n  stopwords?: string[];\n}\n\n/**\n * Contains the possible cases for LexicalTokenizer.\n */\nexport type LexicalTokenizerUnion = LexicalTokenizer | ClassicTokenizer | EdgeNGramTokenizer | KeywordTokenizer | KeywordTokenizerV2 | MicrosoftLanguageTokenizer | MicrosoftLanguageStemmingTokenizer | NGramTokenizer | PathHierarchyTokenizerV2 | PatternTokenizer | LuceneStandardTokenizer | LuceneStandardTokenizerV2 | UaxUrlEmailTokenizer;\n\n/**\n * Base type for tokenizers.\n */\nexport interface LexicalTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"LexicalTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n}\n\n/**\n * Grammar-based tokenizer that is suitable for processing most European-language documents. This\n * tokenizer is implemented using Apache Lucene.\n */\nexport interface ClassicTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.ClassicTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The\n   * maximum token length that can be used is 300 characters. Default value: 255.\n   */\n  maxTokenLength?: number;\n}\n\n/**\n * Tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is\n * implemented using Apache Lucene.\n */\nexport interface EdgeNGramTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.EdgeNGramTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of\n   * maxGram. Default value: 1.\n   */\n  minGram?: number;\n  /**\n   * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.\n   */\n  maxGram?: number;\n  /**\n   * Character classes to keep in the tokens.\n   */\n  tokenChars?: TokenCharacterKind[];\n}\n\n/**\n * Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.\n */\nexport interface KeywordTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.KeywordTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The read buffer size in bytes. Default is 256. Default value: 256.\n   */\n  bufferSize?: number;\n}\n\n/**\n * Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.\n */\nexport interface KeywordTokenizerV2 {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.KeywordTokenizerV2\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum token length. Default is 256. Tokens longer than the maximum length are split. The\n   * maximum token length that can be used is 300 characters. Default value: 256.\n   */\n  maxTokenLength?: number;\n}\n\n/**\n * Divides text using language-specific rules.\n */\nexport interface MicrosoftLanguageTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.MicrosoftLanguageTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum token length. Tokens longer than the maximum length are split. Maximum token\n   * length that can be used is 300 characters. Tokens longer than 300 characters are first split\n   * into tokens of length 300 and then each of those tokens is split based on the max token length\n   * set. Default is 255. Default value: 255.\n   */\n  maxTokenLength?: number;\n  /**\n   * A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set\n   * to false if used as the indexing tokenizer. Default is false. Default value: false.\n   */\n  isSearchTokenizer?: boolean;\n  /**\n   * The language to use. The default is English. Possible values include: 'Bangla', 'Bulgarian',\n   * 'Catalan', 'ChineseSimplified', 'ChineseTraditional', 'Croatian', 'Czech', 'Danish', 'Dutch',\n   * 'English', 'French', 'German', 'Greek', 'Gujarati', 'Hindi', 'Icelandic', 'Indonesian',\n   * 'Italian', 'Japanese', 'Kannada', 'Korean', 'Malay', 'Malayalam', 'Marathi',\n   * 'NorwegianBokmaal', 'Polish', 'Portuguese', 'PortugueseBrazilian', 'Punjabi', 'Romanian',\n   * 'Russian', 'SerbianCyrillic', 'SerbianLatin', 'Slovenian', 'Spanish', 'Swedish', 'Tamil',\n   * 'Telugu', 'Thai', 'Ukrainian', 'Urdu', 'Vietnamese'\n   */\n  language?: MicrosoftTokenizerLanguage;\n}\n\n/**\n * Divides text using language-specific rules and reduces words to their base forms.\n */\nexport interface MicrosoftLanguageStemmingTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum token length. Tokens longer than the maximum length are split. Maximum token\n   * length that can be used is 300 characters. Tokens longer than 300 characters are first split\n   * into tokens of length 300 and then each of those tokens is split based on the max token length\n   * set. Default is 255. Default value: 255.\n   */\n  maxTokenLength?: number;\n  /**\n   * A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set\n   * to false if used as the indexing tokenizer. Default is false. Default value: false.\n   */\n  isSearchTokenizer?: boolean;\n  /**\n   * The language to use. The default is English. Possible values include: 'Arabic', 'Bangla',\n   * 'Bulgarian', 'Catalan', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian',\n   * 'Finnish', 'French', 'German', 'Greek', 'Gujarati', 'Hebrew', 'Hindi', 'Hungarian',\n   * 'Icelandic', 'Indonesian', 'Italian', 'Kannada', 'Latvian', 'Lithuanian', 'Malay',\n   * 'Malayalam', 'Marathi', 'NorwegianBokmaal', 'Polish', 'Portuguese', 'PortugueseBrazilian',\n   * 'Punjabi', 'Romanian', 'Russian', 'SerbianCyrillic', 'SerbianLatin', 'Slovak', 'Slovenian',\n   * 'Spanish', 'Swedish', 'Tamil', 'Telugu', 'Turkish', 'Ukrainian', 'Urdu'\n   */\n  language?: MicrosoftStemmingTokenizerLanguage;\n}\n\n/**\n * Tokenizes the input into n-grams of the given size(s). This tokenizer is implemented using\n * Apache Lucene.\n */\nexport interface NGramTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.NGramTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of\n   * maxGram. Default value: 1.\n   */\n  minGram?: number;\n  /**\n   * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.\n   */\n  maxGram?: number;\n  /**\n   * Character classes to keep in the tokens.\n   */\n  tokenChars?: TokenCharacterKind[];\n}\n\n/**\n * Tokenizer for path-like hierarchies. This tokenizer is implemented using Apache Lucene.\n */\nexport interface PathHierarchyTokenizerV2 {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.PathHierarchyTokenizerV2\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The delimiter character to use. Default is \"/\". Default value: '/'.\n   */\n  delimiter?: string;\n  /**\n   * A value that, if set, replaces the delimiter character. Default is \"/\". Default value: '/'.\n   */\n  replacement?: string;\n  /**\n   * The maximum token length. Default and maximum is 300. Default value: 300.\n   */\n  maxTokenLength?: number;\n  /**\n   * A value indicating whether to generate tokens in reverse order. Default is false. Default\n   * value: false.\n   */\n  reverseTokenOrder?: boolean;\n  /**\n   * The number of initial tokens to skip. Default is 0. Default value: 0.\n   */\n  numberOfTokensToSkip?: number;\n}\n\n/**\n * Tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer is\n * implemented using Apache Lucene.\n */\nexport interface PatternTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.PatternTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A regular expression pattern to match token separators. Default is an expression that matches\n   * one or more non-word characters. Default value: '\\W+'.\n   */\n  pattern?: string;\n  /**\n   * Regular expression flags.\n   */\n  flags?: string;\n  /**\n   * The zero-based ordinal of the matching group in the regular expression pattern to extract into\n   * tokens. Use -1 if you want to use the entire pattern to split the input into tokens,\n   * irrespective of matching groups. Default is -1. Default value: -1.\n   */\n  group?: number;\n}\n\n/**\n * Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using\n * Apache Lucene.\n */\nexport interface LuceneStandardTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.StandardTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum token length. Default is 255. Tokens longer than the maximum length are split.\n   * Default value: 255.\n   */\n  maxTokenLength?: number;\n}\n\n/**\n * Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using\n * Apache Lucene.\n */\nexport interface LuceneStandardTokenizerV2 {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.StandardTokenizerV2\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The\n   * maximum token length that can be used is 300 characters. Default value: 255.\n   */\n  maxTokenLength?: number;\n}\n\n/**\n * Tokenizes urls and emails as one token. This tokenizer is implemented using Apache Lucene.\n */\nexport interface UaxUrlEmailTokenizer {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.UaxUrlEmailTokenizer\";\n  /**\n   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The\n   * maximum token length that can be used is 300 characters. Default value: 255.\n   */\n  maxTokenLength?: number;\n}\n\n/**\n * Contains the possible cases for TokenFilter.\n */\nexport type TokenFilterUnion = TokenFilter | AsciiFoldingTokenFilter | CjkBigramTokenFilter | CommonGramTokenFilter | DictionaryDecompounderTokenFilter | EdgeNGramTokenFilter | EdgeNGramTokenFilterV2 | ElisionTokenFilter | KeepTokenFilter | KeywordMarkerTokenFilter | LengthTokenFilter | LimitTokenFilter | NGramTokenFilter | NGramTokenFilterV2 | PatternCaptureTokenFilter | PatternReplaceTokenFilter | PhoneticTokenFilter | ShingleTokenFilter | SnowballTokenFilter | StemmerTokenFilter | StemmerOverrideTokenFilter | StopwordsTokenFilter | SynonymTokenFilter | TruncateTokenFilter | UniqueTokenFilter | WordDelimiterTokenFilter;\n\n/**\n * Base type for token filters.\n */\nexport interface TokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"TokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n}\n\n/**\n * Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127\n * ASCII characters (the \"Basic Latin\" Unicode block) into their ASCII equivalents, if such\n * equivalents exist. This token filter is implemented using Apache Lucene.\n */\nexport interface AsciiFoldingTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.AsciiFoldingTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A value indicating whether the original token will be kept. Default is false. Default value:\n   * false.\n   */\n  preserveOriginal?: boolean;\n}\n\n/**\n * Forms bigrams of CJK terms that are generated from the standard tokenizer. This token filter is\n * implemented using Apache Lucene.\n */\nexport interface CjkBigramTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.CjkBigramTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The scripts to ignore.\n   */\n  ignoreScripts?: CjkBigramTokenFilterScripts[];\n  /**\n   * A value indicating whether to output both unigrams and bigrams (if true), or just bigrams (if\n   * false). Default is false. Default value: false.\n   */\n  outputUnigrams?: boolean;\n}\n\n/**\n * Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed\n * too, with bigrams overlaid. This token filter is implemented using Apache Lucene.\n */\nexport interface CommonGramTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.CommonGramTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The set of common words.\n   */\n  commonWords: string[];\n  /**\n   * A value indicating whether common words matching will be case insensitive. Default is false.\n   * Default value: false.\n   */\n  ignoreCase?: boolean;\n  /**\n   * A value that indicates whether the token filter is in query mode. When in query mode, the\n   * token filter generates bigrams and then removes common words and single terms followed by a\n   * common word. Default is false. Default value: false.\n   */\n  useQueryMode?: boolean;\n}\n\n/**\n * Decomposes compound words found in many Germanic languages. This token filter is implemented\n * using Apache Lucene.\n */\nexport interface DictionaryDecompounderTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.DictionaryDecompounderTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The list of words to match against.\n   */\n  wordList: string[];\n  /**\n   * The minimum word size. Only words longer than this get processed. Default is 5. Maximum is\n   * 300. Default value: 5.\n   */\n  minWordSize?: number;\n  /**\n   * The minimum subword size. Only subwords longer than this are outputted. Default is 2. Maximum\n   * is 300. Default value: 2.\n   */\n  minSubwordSize?: number;\n  /**\n   * The maximum subword size. Only subwords shorter than this are outputted. Default is 15.\n   * Maximum is 300. Default value: 15.\n   */\n  maxSubwordSize?: number;\n  /**\n   * A value indicating whether to add only the longest matching subword to the output. Default is\n   * false. Default value: false.\n   */\n  onlyLongestMatch?: boolean;\n}\n\n/**\n * Generates n-grams of the given size(s) starting from the front or the back of an input token.\n * This token filter is implemented using Apache Lucene.\n */\nexport interface EdgeNGramTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.EdgeNGramTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The minimum n-gram length. Default is 1. Must be less than the value of maxGram. Default\n   * value: 1.\n   */\n  minGram?: number;\n  /**\n   * The maximum n-gram length. Default is 2. Default value: 2.\n   */\n  maxGram?: number;\n  /**\n   * Specifies which side of the input the n-gram should be generated from. Default is \"front\".\n   * Possible values include: 'Front', 'Back'\n   */\n  side?: EdgeNGramTokenFilterSide;\n}\n\n/**\n * Generates n-grams of the given size(s) starting from the front or the back of an input token.\n * This token filter is implemented using Apache Lucene.\n */\nexport interface EdgeNGramTokenFilterV2 {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.EdgeNGramTokenFilterV2\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of\n   * maxGram. Default value: 1.\n   */\n  minGram?: number;\n  /**\n   * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.\n   */\n  maxGram?: number;\n  /**\n   * Specifies which side of the input the n-gram should be generated from. Default is \"front\".\n   * Possible values include: 'Front', 'Back'\n   */\n  side?: EdgeNGramTokenFilterSide;\n}\n\n/**\n * Removes elisions. For example, \"l'avion\" (the plane) will be converted to \"avion\" (plane). This\n * token filter is implemented using Apache Lucene.\n */\nexport interface ElisionTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.ElisionTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The set of articles to remove.\n   */\n  articles?: string[];\n}\n\n/**\n * A token filter that only keeps tokens with text contained in a specified list of words. This\n * token filter is implemented using Apache Lucene.\n */\nexport interface KeepTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.KeepTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The list of words to keep.\n   */\n  keepWords: string[];\n  /**\n   * A value indicating whether to lower case all words first. Default is false. Default value:\n   * false.\n   */\n  lowerCaseKeepWords?: boolean;\n}\n\n/**\n * Marks terms as keywords. This token filter is implemented using Apache Lucene.\n */\nexport interface KeywordMarkerTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.KeywordMarkerTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A list of words to mark as keywords.\n   */\n  keywords: string[];\n  /**\n   * A value indicating whether to ignore case. If true, all words are converted to lower case\n   * first. Default is false. Default value: false.\n   */\n  ignoreCase?: boolean;\n}\n\n/**\n * Removes words that are too long or too short. This token filter is implemented using Apache\n * Lucene.\n */\nexport interface LengthTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.LengthTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The minimum length in characters. Default is 0. Maximum is 300. Must be less than the value of\n   * max. Default value: 0.\n   */\n  minLength?: number;\n  /**\n   * The maximum length in characters. Default and maximum is 300. Default value: 300.\n   */\n  maxLength?: number;\n}\n\n/**\n * Limits the number of tokens while indexing. This token filter is implemented using Apache\n * Lucene.\n */\nexport interface LimitTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.LimitTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum number of tokens to produce. Default is 1. Default value: 1.\n   */\n  maxTokenCount?: number;\n  /**\n   * A value indicating whether all tokens from the input must be consumed even if maxTokenCount is\n   * reached. Default is false. Default value: false.\n   */\n  consumeAllTokens?: boolean;\n}\n\n/**\n * Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.\n */\nexport interface NGramTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.NGramTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The minimum n-gram length. Default is 1. Must be less than the value of maxGram. Default\n   * value: 1.\n   */\n  minGram?: number;\n  /**\n   * The maximum n-gram length. Default is 2. Default value: 2.\n   */\n  maxGram?: number;\n}\n\n/**\n * Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.\n */\nexport interface NGramTokenFilterV2 {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.NGramTokenFilterV2\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of\n   * maxGram. Default value: 1.\n   */\n  minGram?: number;\n  /**\n   * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.\n   */\n  maxGram?: number;\n}\n\n/**\n * Uses Java regexes to emit multiple tokens - one for each capture group in one or more patterns.\n * This token filter is implemented using Apache Lucene.\n */\nexport interface PatternCaptureTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.PatternCaptureTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A list of patterns to match against each token.\n   */\n  patterns: string[];\n  /**\n   * A value indicating whether to return the original token even if one of the patterns matches.\n   * Default is true. Default value: true.\n   */\n  preserveOriginal?: boolean;\n}\n\n/**\n * A character filter that replaces characters in the input string. It uses a regular expression to\n * identify character sequences to preserve and a replacement pattern to identify characters to\n * replace. For example, given the input text \"aa bb aa bb\", pattern \"(aa)\\s+(bb)\", and replacement\n * \"$1#$2\", the result would be \"aa#bb aa#bb\". This token filter is implemented using Apache\n * Lucene.\n */\nexport interface PatternReplaceTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.PatternReplaceTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A regular expression pattern.\n   */\n  pattern: string;\n  /**\n   * The replacement text.\n   */\n  replacement: string;\n}\n\n/**\n * Create tokens for phonetic matches. This token filter is implemented using Apache Lucene.\n */\nexport interface PhoneticTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.PhoneticTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The phonetic encoder to use. Default is \"metaphone\". Possible values include: 'Metaphone',\n   * 'DoubleMetaphone', 'Soundex', 'RefinedSoundex', 'Caverphone1', 'Caverphone2', 'Cologne',\n   * 'Nysiis', 'KoelnerPhonetik', 'HaasePhonetik', 'BeiderMorse'\n   */\n  encoder?: PhoneticEncoder;\n  /**\n   * A value indicating whether encoded tokens should replace original tokens. If false, encoded\n   * tokens are added as synonyms. Default is true. Default value: true.\n   */\n  replaceOriginalTokens?: boolean;\n}\n\n/**\n * Creates combinations of tokens as a single token. This token filter is implemented using Apache\n * Lucene.\n */\nexport interface ShingleTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.ShingleTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The maximum shingle size. Default and minimum value is 2. Default value: 2.\n   */\n  maxShingleSize?: number;\n  /**\n   * The minimum shingle size. Default and minimum value is 2. Must be less than the value of\n   * maxShingleSize. Default value: 2.\n   */\n  minShingleSize?: number;\n  /**\n   * A value indicating whether the output stream will contain the input tokens (unigrams) as well\n   * as shingles. Default is true. Default value: true.\n   */\n  outputUnigrams?: boolean;\n  /**\n   * A value indicating whether to output unigrams for those times when no shingles are available.\n   * This property takes precedence when outputUnigrams is set to false. Default is false. Default\n   * value: false.\n   */\n  outputUnigramsIfNoShingles?: boolean;\n  /**\n   * The string to use when joining adjacent tokens to form a shingle. Default is a single space (\"\n   * \"). Default value: ''.\n   */\n  tokenSeparator?: string;\n  /**\n   * The string to insert for each position at which there is no token. Default is an underscore\n   * (\"_\"). Default value: '_'.\n   */\n  filterToken?: string;\n}\n\n/**\n * A filter that stems words using a Snowball-generated stemmer. This token filter is implemented\n * using Apache Lucene.\n */\nexport interface SnowballTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.SnowballTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The language to use. Possible values include: 'Armenian', 'Basque', 'Catalan', 'Danish',\n   * 'Dutch', 'English', 'Finnish', 'French', 'German', 'German2', 'Hungarian', 'Italian', 'Kp',\n   * 'Lovins', 'Norwegian', 'Porter', 'Portuguese', 'Romanian', 'Russian', 'Spanish', 'Swedish',\n   * 'Turkish'\n   */\n  language: SnowballTokenFilterLanguage;\n}\n\n/**\n * Language specific stemming filter. This token filter is implemented using Apache Lucene.\n */\nexport interface StemmerTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.StemmerTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The language to use. Possible values include: 'Arabic', 'Armenian', 'Basque', 'Brazilian',\n   * 'Bulgarian', 'Catalan', 'Czech', 'Danish', 'Dutch', 'DutchKp', 'English', 'LightEnglish',\n   * 'MinimalEnglish', 'PossessiveEnglish', 'Porter2', 'Lovins', 'Finnish', 'LightFinnish',\n   * 'French', 'LightFrench', 'MinimalFrench', 'Galician', 'MinimalGalician', 'German', 'German2',\n   * 'LightGerman', 'MinimalGerman', 'Greek', 'Hindi', 'Hungarian', 'LightHungarian', 'Indonesian',\n   * 'Irish', 'Italian', 'LightItalian', 'Sorani', 'Latvian', 'Norwegian', 'LightNorwegian',\n   * 'MinimalNorwegian', 'LightNynorsk', 'MinimalNynorsk', 'Portuguese', 'LightPortuguese',\n   * 'MinimalPortuguese', 'PortugueseRslp', 'Romanian', 'Russian', 'LightRussian', 'Spanish',\n   * 'LightSpanish', 'Swedish', 'LightSwedish', 'Turkish'\n   */\n  language: StemmerTokenFilterLanguage;\n}\n\n/**\n * Provides the ability to override other stemming filters with custom dictionary-based stemming.\n * Any dictionary-stemmed terms will be marked as keywords so that they will not be stemmed with\n * stemmers down the chain. Must be placed before any stemming filters. This token filter is\n * implemented using Apache Lucene.\n */\nexport interface StemmerOverrideTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.StemmerOverrideTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A list of stemming rules in the following format: \"word => stem\", for example: \"ran => run\".\n   */\n  rules: string[];\n}\n\n/**\n * Removes stop words from a token stream. This token filter is implemented using Apache Lucene.\n */\nexport interface StopwordsTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.StopwordsTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The list of stopwords. This property and the stopwords list property cannot both be set.\n   */\n  stopwords?: string[];\n  /**\n   * A predefined list of stopwords to use. This property and the stopwords property cannot both be\n   * set. Default is English. Possible values include: 'Arabic', 'Armenian', 'Basque', 'Brazilian',\n   * 'Bulgarian', 'Catalan', 'Czech', 'Danish', 'Dutch', 'English', 'Finnish', 'French',\n   * 'Galician', 'German', 'Greek', 'Hindi', 'Hungarian', 'Indonesian', 'Irish', 'Italian',\n   * 'Latvian', 'Norwegian', 'Persian', 'Portuguese', 'Romanian', 'Russian', 'Sorani', 'Spanish',\n   * 'Swedish', 'Thai', 'Turkish'\n   */\n  stopwordsList?: StopwordsList;\n  /**\n   * A value indicating whether to ignore case. If true, all words are converted to lower case\n   * first. Default is false. Default value: false.\n   */\n  ignoreCase?: boolean;\n  /**\n   * A value indicating whether to ignore the last search term if it's a stop word. Default is\n   * true. Default value: true.\n   */\n  removeTrailingStopWords?: boolean;\n}\n\n/**\n * Matches single or multi-word synonyms in a token stream. This token filter is implemented using\n * Apache Lucene.\n */\nexport interface SynonymTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.SynonymTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A list of synonyms in following one of two formats: 1. incredible, unbelievable, fabulous =>\n   * amazing - all terms on the left side of => symbol will be replaced with all terms on its right\n   * side; 2. incredible, unbelievable, fabulous, amazing - comma separated list of equivalent\n   * words. Set the expand option to change how this list is interpreted.\n   */\n  synonyms: string[];\n  /**\n   * A value indicating whether to case-fold input for matching. Default is false. Default value:\n   * false.\n   */\n  ignoreCase?: boolean;\n  /**\n   * A value indicating whether all words in the list of synonyms (if => notation is not used) will\n   * map to one another. If true, all words in the list of synonyms (if => notation is not used)\n   * will map to one another. The following list: incredible, unbelievable, fabulous, amazing is\n   * equivalent to: incredible, unbelievable, fabulous, amazing => incredible, unbelievable,\n   * fabulous, amazing. If false, the following list: incredible, unbelievable, fabulous, amazing\n   * will be equivalent to: incredible, unbelievable, fabulous, amazing => incredible. Default is\n   * true. Default value: true.\n   */\n  expand?: boolean;\n}\n\n/**\n * Truncates the terms to a specific length. This token filter is implemented using Apache Lucene.\n */\nexport interface TruncateTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.TruncateTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * The length at which terms will be truncated. Default and maximum is 300. Default value: 300.\n   */\n  length?: number;\n}\n\n/**\n * Filters out tokens with same text as the previous token. This token filter is implemented using\n * Apache Lucene.\n */\nexport interface UniqueTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.UniqueTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A value indicating whether to remove duplicates only at the same position. Default is false.\n   * Default value: false.\n   */\n  onlyOnSamePosition?: boolean;\n}\n\n/**\n * Splits words into subwords and performs optional transformations on subword groups. This token\n * filter is implemented using Apache Lucene.\n */\nexport interface WordDelimiterTokenFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.WordDelimiterTokenFilter\";\n  /**\n   * The name of the token filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A value indicating whether to generate part words. If set, causes parts of words to be\n   * generated; for example \"AzureSearch\" becomes \"Azure\" \"Search\". Default is true. Default value:\n   * true.\n   */\n  generateWordParts?: boolean;\n  /**\n   * A value indicating whether to generate number subwords. Default is true. Default value: true.\n   */\n  generateNumberParts?: boolean;\n  /**\n   * A value indicating whether maximum runs of word parts will be catenated. For example, if this\n   * is set to true, \"Azure-Search\" becomes \"AzureSearch\". Default is false. Default value: false.\n   */\n  catenateWords?: boolean;\n  /**\n   * A value indicating whether maximum runs of number parts will be catenated. For example, if\n   * this is set to true, \"1-2\" becomes \"12\". Default is false. Default value: false.\n   */\n  catenateNumbers?: boolean;\n  /**\n   * A value indicating whether all subword parts will be catenated. For example, if this is set to\n   * true, \"Azure-Search-1\" becomes \"AzureSearch1\". Default is false. Default value: false.\n   */\n  catenateAll?: boolean;\n  /**\n   * A value indicating whether to split words on caseChange. For example, if this is set to true,\n   * \"AzureSearch\" becomes \"Azure\" \"Search\". Default is true. Default value: true.\n   */\n  splitOnCaseChange?: boolean;\n  /**\n   * A value indicating whether original words will be preserved and added to the subword list.\n   * Default is false. Default value: false.\n   */\n  preserveOriginal?: boolean;\n  /**\n   * A value indicating whether to split on numbers. For example, if this is set to true,\n   * \"Azure1Search\" becomes \"Azure\" \"1\" \"Search\". Default is true. Default value: true.\n   */\n  splitOnNumerics?: boolean;\n  /**\n   * A value indicating whether to remove trailing \"'s\" for each subword. Default is true. Default\n   * value: true.\n   */\n  stemEnglishPossessive?: boolean;\n  /**\n   * A list of tokens to protect from being delimited.\n   */\n  protectedWords?: string[];\n}\n\n/**\n * Contains the possible cases for CharFilter.\n */\nexport type CharFilterUnion = CharFilter | MappingCharFilter | PatternReplaceCharFilter;\n\n/**\n * Base type for character filters.\n */\nexport interface CharFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"CharFilter\";\n  /**\n   * The name of the char filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n}\n\n/**\n * A character filter that applies mappings defined with the mappings option. Matching is greedy\n * (longest pattern matching at a given point wins). Replacement is allowed to be the empty string.\n * This character filter is implemented using Apache Lucene.\n */\nexport interface MappingCharFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.MappingCharFilter\";\n  /**\n   * The name of the char filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A list of mappings of the following format: \"a=>b\" (all occurrences of the character \"a\" will\n   * be replaced with character \"b\").\n   */\n  mappings: string[];\n}\n\n/**\n * A character filter that replaces characters in the input string. It uses a regular expression to\n * identify character sequences to preserve and a replacement pattern to identify characters to\n * replace. For example, given the input text \"aa bb aa bb\", pattern \"(aa)\\s+(bb)\", and replacement\n * \"$1#$2\", the result would be \"aa#bb aa#bb\". This character filter is implemented using Apache\n * Lucene.\n */\nexport interface PatternReplaceCharFilter {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.PatternReplaceCharFilter\";\n  /**\n   * The name of the char filter. It must only contain letters, digits, spaces, dashes or\n   * underscores, can only start and end with alphanumeric characters, and is limited to 128\n   * characters.\n   */\n  name: string;\n  /**\n   * A regular expression pattern.\n   */\n  pattern: string;\n  /**\n   * The replacement text.\n   */\n  replacement: string;\n}\n\n/**\n * Contains the possible cases for Similarity.\n */\nexport type SimilarityUnion = Similarity | ClassicSimilarity | BM25Similarity;\n\n/**\n * Base type for similarity algorithms. Similarity algorithms are used to calculate scores that tie\n * queries to documents. The higher the score, the more relevant the document is to that specific\n * query. Those scores are used to rank the search results.\n */\nexport interface Similarity {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"Similarity\";\n}\n\n/**\n * Legacy similarity algorithm which uses the Lucene TFIDFSimilarity implementation of TF-IDF. This\n * variation of TF-IDF introduces static document length normalization as well as coordinating\n * factors that penalize documents that only partially match the searched queries.\n */\nexport interface ClassicSimilarity {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.ClassicSimilarity\";\n}\n\n/**\n * Ranking function based on the Okapi BM25 similarity algorithm. BM25 is a TF-IDF-like algorithm\n * that includes length normalization (controlled by the 'b' parameter) as well as term frequency\n * saturation (controlled by the 'k1' parameter).\n */\nexport interface BM25Similarity {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.BM25Similarity\";\n  /**\n   * This property controls the scaling function between the term frequency of each matching terms\n   * and the final relevance score of a document-query pair. By default, a value of 1.2 is used. A\n   * value of 0.0 means the score does not scale with an increase in term frequency.\n   */\n  k1?: number;\n  /**\n   * This property controls how the length of a document affects the relevance score. By default, a\n   * value of 0.75 is used. A value of 0.0 means no length normalization is applied, while a value\n   * of 1.0 means the score is fully normalized by the length of the document.\n   */\n  b?: number;\n}\n\n/**\n * Represents credentials that can be used to connect to a datasource.\n */\nexport interface DataSourceCredentials {\n  /**\n   * The connection string for the datasource.\n   */\n  connectionString?: string;\n}\n\n/**\n * Represents information about the entity (such as Azure SQL table or CosmosDB collection) that\n * will be indexed.\n */\nexport interface SearchIndexerDataContainer {\n  /**\n   * The name of the table or view (for Azure SQL data source) or collection (for CosmosDB data\n   * source) that will be indexed.\n   */\n  name: string;\n  /**\n   * A query that is applied to this data container. The syntax and meaning of this parameter is\n   * datasource-specific. Not supported by Azure SQL datasources.\n   */\n  query?: string;\n}\n\n/**\n * Contains the possible cases for DataChangeDetectionPolicy.\n */\nexport type DataChangeDetectionPolicyUnion = DataChangeDetectionPolicy | HighWaterMarkChangeDetectionPolicy | SqlIntegratedChangeTrackingPolicy;\n\n/**\n * Base type for data change detection policies.\n */\nexport interface DataChangeDetectionPolicy {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"DataChangeDetectionPolicy\";\n}\n\n/**\n * Defines a data change detection policy that captures changes based on the value of a high water\n * mark column.\n */\nexport interface HighWaterMarkChangeDetectionPolicy {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.HighWaterMarkChangeDetectionPolicy\";\n  /**\n   * The name of the high water mark column.\n   */\n  highWaterMarkColumnName: string;\n}\n\n/**\n * Defines a data change detection policy that captures changes using the Integrated Change\n * Tracking feature of Azure SQL Database.\n */\nexport interface SqlIntegratedChangeTrackingPolicy {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.SqlIntegratedChangeTrackingPolicy\";\n}\n\n/**\n * Contains the possible cases for DataDeletionDetectionPolicy.\n */\nexport type DataDeletionDetectionPolicyUnion = DataDeletionDetectionPolicy | SoftDeleteColumnDeletionDetectionPolicy;\n\n/**\n * Base type for data deletion detection policies.\n */\nexport interface DataDeletionDetectionPolicy {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"DataDeletionDetectionPolicy\";\n}\n\n/**\n * Defines a data deletion detection policy that implements a soft-deletion strategy. It determines\n * whether an item should be deleted based on the value of a designated 'soft delete' column.\n */\nexport interface SoftDeleteColumnDeletionDetectionPolicy {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy\";\n  /**\n   * The name of the column to use for soft-deletion detection.\n   */\n  softDeleteColumnName?: string;\n  /**\n   * The marker value that identifies an item as deleted.\n   */\n  softDeleteMarkerValue?: string;\n}\n\n/**\n * Represents a datasource definition, which can be used to configure an indexer.\n */\nexport interface SearchIndexerDataSource {\n  /**\n   * The name of the datasource.\n   */\n  name: string;\n  /**\n   * The description of the datasource.\n   */\n  description?: string;\n  /**\n   * The type of the datasource. Possible values include: 'AzureSql', 'CosmosDb', 'AzureBlob',\n   * 'AzureTable', 'MySql'\n   */\n  type: SearchIndexerDataSourceType;\n  /**\n   * Credentials for the datasource.\n   */\n  credentials: DataSourceCredentials;\n  /**\n   * The data container for the datasource.\n   */\n  container: SearchIndexerDataContainer;\n  /**\n   * The data change detection policy for the datasource.\n   */\n  dataChangeDetectionPolicy?: DataChangeDetectionPolicyUnion;\n  /**\n   * The data deletion detection policy for the datasource.\n   */\n  dataDeletionDetectionPolicy?: DataDeletionDetectionPolicyUnion;\n  /**\n   * The ETag of the data source.\n   */\n  etag?: string;\n}\n\n/**\n * Response from a List Datasources request. If successful, it includes the full definitions of all\n * datasources.\n */\nexport interface ListDataSourcesResult {\n  /**\n   * The datasources in the Search service.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly dataSources: SearchIndexerDataSource[];\n}\n\n/**\n * Represents a schedule for indexer execution.\n */\nexport interface IndexingSchedule {\n  /**\n   * The interval of time between indexer executions.\n   */\n  interval: string;\n  /**\n   * The time when an indexer should start running.\n   */\n  startTime?: Date;\n}\n\n/**\n * Represents parameters for indexer execution.\n */\nexport interface IndexingParameters {\n  /**\n   * The number of items that are read from the data source and indexed as a single batch in order\n   * to improve performance. The default depends on the data source type.\n   */\n  batchSize?: number;\n  /**\n   * The maximum number of items that can fail indexing for indexer execution to still be\n   * considered successful. -1 means no limit. Default is 0. Default value: 0.\n   */\n  maxFailedItems?: number;\n  /**\n   * The maximum number of items in a single batch that can fail indexing for the batch to still be\n   * considered successful. -1 means no limit. Default is 0. Default value: 0.\n   */\n  maxFailedItemsPerBatch?: number;\n  /**\n   * A dictionary of indexer-specific configuration properties. Each name is the name of a specific\n   * property. Each value must be of a primitive type.\n   */\n  configuration?: { [propertyName: string]: any };\n}\n\n/**\n * Represents a function that transforms a value from a data source before indexing.\n */\nexport interface FieldMappingFunction {\n  /**\n   * The name of the field mapping function.\n   */\n  name: string;\n  /**\n   * A dictionary of parameter name/value pairs to pass to the function. Each value must be of a\n   * primitive type.\n   */\n  parameters?: { [propertyName: string]: any };\n}\n\n/**\n * Defines a mapping between a field in a data source and a target field in an index.\n */\nexport interface FieldMapping {\n  /**\n   * The name of the field in the data source.\n   */\n  sourceFieldName: string;\n  /**\n   * The name of the target field in the index. Same as the source field name by default.\n   */\n  targetFieldName?: string;\n  /**\n   * A function to apply to each source field value before indexing.\n   */\n  mappingFunction?: FieldMappingFunction;\n}\n\n/**\n * Represents an indexer.\n */\nexport interface SearchIndexer {\n  /**\n   * The name of the indexer.\n   */\n  name: string;\n  /**\n   * The description of the indexer.\n   */\n  description?: string;\n  /**\n   * The name of the datasource from which this indexer reads data.\n   */\n  dataSourceName: string;\n  /**\n   * The name of the skillset executing with this indexer.\n   */\n  skillsetName?: string;\n  /**\n   * The name of the index to which this indexer writes data.\n   */\n  targetIndexName: string;\n  /**\n   * The schedule for this indexer.\n   */\n  schedule?: IndexingSchedule;\n  /**\n   * Parameters for indexer execution.\n   */\n  parameters?: IndexingParameters;\n  /**\n   * Defines mappings between fields in the data source and corresponding target fields in the\n   * index.\n   */\n  fieldMappings?: FieldMapping[];\n  /**\n   * Output field mappings are applied after enrichment and immediately before indexing.\n   */\n  outputFieldMappings?: FieldMapping[];\n  /**\n   * A value indicating whether the indexer is disabled. Default is false. Default value: false.\n   */\n  isDisabled?: boolean;\n  /**\n   * The ETag of the indexer.\n   */\n  etag?: string;\n}\n\n/**\n * Response from a List Indexers request. If successful, it includes the full definitions of all\n * indexers.\n */\nexport interface ListIndexersResult {\n  /**\n   * The indexers in the Search service.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly indexers: SearchIndexer[];\n}\n\n/**\n * Represents an item- or document-level indexing error.\n */\nexport interface SearchIndexerError {\n  /**\n   * The key of the item for which indexing failed.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly key?: string;\n  /**\n   * The message describing the error that occurred while processing the item.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly errorMessage: string;\n  /**\n   * The status code indicating why the indexing operation failed. Possible values include: 400 for\n   * a malformed input document, 404 for document not found, 409 for a version conflict, 422 when\n   * the index is temporarily unavailable, or 503 for when the service is too busy.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly statusCode: number;\n  /**\n   * The name of the source at which the error originated. For example, this could refer to a\n   * particular skill in the attached skillset. This may not be always available.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly name?: string;\n  /**\n   * Additional, verbose details about the error to assist in debugging the indexer. This may not\n   * be always available.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly details?: string;\n  /**\n   * A link to a troubleshooting guide for these classes of errors. This may not be always\n   * available.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly documentationLink?: string;\n}\n\n/**\n * Represents an item-level warning.\n */\nexport interface SearchIndexerWarning {\n  /**\n   * The key of the item which generated a warning.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly key?: string;\n  /**\n   * The message describing the warning that occurred while processing the item.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly message: string;\n  /**\n   * The name of the source at which the warning originated. For example, this could refer to a\n   * particular skill in the attached skillset. This may not be always available.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly name?: string;\n  /**\n   * Additional, verbose details about the warning to assist in debugging the indexer. This may not\n   * be always available.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly details?: string;\n  /**\n   * A link to a troubleshooting guide for these classes of warnings. This may not be always\n   * available.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly documentationLink?: string;\n}\n\n/**\n * Represents the result of an individual indexer execution.\n */\nexport interface IndexerExecutionResult {\n  /**\n   * The outcome of this indexer execution. Possible values include: 'TransientFailure', 'Success',\n   * 'InProgress', 'Reset'\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly status: IndexerExecutionStatus;\n  /**\n   * The error message indicating the top-level error, if any.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly errorMessage?: string;\n  /**\n   * The start time of this indexer execution.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly startTime?: Date;\n  /**\n   * The end time of this indexer execution, if the execution has already completed.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly endTime?: Date;\n  /**\n   * The item-level indexing errors.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly errors: SearchIndexerError[];\n  /**\n   * The item-level indexing warnings.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly warnings: SearchIndexerWarning[];\n  /**\n   * The number of items that were processed during this indexer execution. This includes both\n   * successfully processed items and items where indexing was attempted but failed.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly itemCount: number;\n  /**\n   * The number of items that failed to be indexed during this indexer execution.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly failedItemCount: number;\n  /**\n   * Change tracking state with which an indexer execution started.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly initialTrackingState?: string;\n  /**\n   * Change tracking state with which an indexer execution finished.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly finalTrackingState?: string;\n}\n\n/**\n * An interface representing SearchIndexerLimits.\n */\nexport interface SearchIndexerLimits {\n  /**\n   * The maximum duration that the indexer is permitted to run for one execution.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly maxRunTime?: string;\n  /**\n   * The maximum size of a document, in bytes, which will be considered valid for indexing.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly maxDocumentExtractionSize?: number;\n  /**\n   * The maximum number of characters that will be extracted from a document picked up for\n   * indexing.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly maxDocumentContentCharactersToExtract?: number;\n}\n\n/**\n * Represents the current status and execution history of an indexer.\n */\nexport interface SearchIndexerStatus {\n  /**\n   * Overall indexer status. Possible values include: 'Unknown', 'Error', 'Running'\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly status: IndexerStatus;\n  /**\n   * The result of the most recent or an in-progress indexer execution.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly lastResult?: IndexerExecutionResult;\n  /**\n   * History of the recent indexer executions, sorted in reverse chronological order.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly executionHistory: IndexerExecutionResult[];\n  /**\n   * The execution limits for the indexer.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly limits: SearchIndexerLimits;\n}\n\n/**\n * Represents a field in an index definition, which describes the name, data type, and search\n * behavior of a field.\n */\nexport interface SearchField {\n  /**\n   * The name of the field, which must be unique within the fields collection of the index or\n   * parent field.\n   */\n  name: string;\n  /**\n   * The data type of the field. Possible values include: 'String', 'Int32', 'Int64', 'Double',\n   * 'Boolean', 'DateTimeOffset', 'GeographyPoint', 'Complex', 'Collection(Edm.String)',\n   * 'Collection(Edm.Int32)', 'Collection(Edm.Int64)', 'Collection(Edm.Double)',\n   * 'Collection(Edm.Boolean)', 'Collection(Edm.DateTimeOffset)', 'Collection(Edm.GeographyPoint)',\n   * 'Collection(Edm.ComplexType)'\n   */\n  type: SearchFieldDataType;\n  /**\n   * A value indicating whether the field uniquely identifies documents in the index. Exactly one\n   * top-level field in each index must be chosen as the key field and it must be of type\n   * Edm.String. Key fields can be used to look up documents directly and update or delete specific\n   * documents. Default is false for simple fields and null for complex fields.\n   */\n  key?: boolean;\n  /**\n   * A value indicating whether the field can be returned in a search result. You can disable this\n   * option if you want to use a field (for example, margin) as a filter, sorting, or scoring\n   * mechanism but do not want the field to be visible to the end user. This property must be true\n   * for key fields, and it must be null for complex fields. This property can be changed on\n   * existing fields. Enabling this property does not cause any increase in index storage\n   * requirements. Default is true for simple fields and null for complex fields.\n   */\n  retrievable?: boolean;\n  /**\n   * A value indicating whether the field is full-text searchable. This means it will undergo\n   * analysis such as word-breaking during indexing. If you set a searchable field to a value like\n   * \"sunny day\", internally it will be split into the individual tokens \"sunny\" and \"day\". This\n   * enables full-text searches for these terms. Fields of type Edm.String or\n   * Collection(Edm.String) are searchable by default. This property must be false for simple\n   * fields of other non-string data types, and it must be null for complex fields. Note:\n   * searchable fields consume extra space in your index since Azure Cognitive Search will store an\n   * additional tokenized version of the field value for full-text searches. If you want to save\n   * space in your index and you don't need a field to be included in searches, set searchable to\n   * false.\n   */\n  searchable?: boolean;\n  /**\n   * A value indicating whether to enable the field to be referenced in $filter queries. filterable\n   * differs from searchable in how strings are handled. Fields of type Edm.String or\n   * Collection(Edm.String) that are filterable do not undergo word-breaking, so comparisons are\n   * for exact matches only. For example, if you set such a field f to \"sunny day\", $filter=f eq\n   * 'sunny' will find no matches, but $filter=f eq 'sunny day' will. This property must be null\n   * for complex fields. Default is true for simple fields and null for complex fields.\n   */\n  filterable?: boolean;\n  /**\n   * A value indicating whether to enable the field to be referenced in $orderby expressions. By\n   * default Azure Cognitive Search sorts results by score, but in many experiences users will want\n   * to sort by fields in the documents. A simple field can be sortable only if it is single-valued\n   * (it has a single value in the scope of the parent document). Simple collection fields cannot\n   * be sortable, since they are multi-valued. Simple sub-fields of complex collections are also\n   * multi-valued, and therefore cannot be sortable. This is true whether it's an immediate parent\n   * field, or an ancestor field, that's the complex collection. Complex fields cannot be sortable\n   * and the sortable property must be null for such fields. The default for sortable is true for\n   * single-valued simple fields, false for multi-valued simple fields, and null for complex\n   * fields.\n   */\n  sortable?: boolean;\n  /**\n   * A value indicating whether to enable the field to be referenced in facet queries. Typically\n   * used in a presentation of search results that includes hit count by category (for example,\n   * search for digital cameras and see hits by brand, by megapixels, by price, and so on). This\n   * property must be null for complex fields. Fields of type Edm.GeographyPoint or\n   * Collection(Edm.GeographyPoint) cannot be facetable. Default is true for all other simple\n   * fields.\n   */\n  facetable?: boolean;\n  /**\n   * The name of the analyzer to use for the field. This option can be used only with searchable\n   * fields and it can't be set together with either searchAnalyzer or indexAnalyzer. Once the\n   * analyzer is chosen, it cannot be changed for the field. Must be null for complex fields.\n   * Possible values include: 'ArMicrosoft', 'ArLucene', 'HyLucene', 'BnMicrosoft', 'EuLucene',\n   * 'BgMicrosoft', 'BgLucene', 'CaMicrosoft', 'CaLucene', 'ZhHansMicrosoft', 'ZhHansLucene',\n   * 'ZhHantMicrosoft', 'ZhHantLucene', 'HrMicrosoft', 'CsMicrosoft', 'CsLucene', 'DaMicrosoft',\n   * 'DaLucene', 'NlMicrosoft', 'NlLucene', 'EnMicrosoft', 'EnLucene', 'EtMicrosoft',\n   * 'FiMicrosoft', 'FiLucene', 'FrMicrosoft', 'FrLucene', 'GlLucene', 'DeMicrosoft', 'DeLucene',\n   * 'ElMicrosoft', 'ElLucene', 'GuMicrosoft', 'HeMicrosoft', 'HiMicrosoft', 'HiLucene',\n   * 'HuMicrosoft', 'HuLucene', 'IsMicrosoft', 'IdMicrosoft', 'IdLucene', 'GaLucene',\n   * 'ItMicrosoft', 'ItLucene', 'JaMicrosoft', 'JaLucene', 'KnMicrosoft', 'KoMicrosoft',\n   * 'KoLucene', 'LvMicrosoft', 'LvLucene', 'LtMicrosoft', 'MlMicrosoft', 'MsMicrosoft',\n   * 'MrMicrosoft', 'NbMicrosoft', 'NoLucene', 'FaLucene', 'PlMicrosoft', 'PlLucene',\n   * 'PtBrMicrosoft', 'PtBrLucene', 'PtPtMicrosoft', 'PtPtLucene', 'PaMicrosoft', 'RoMicrosoft',\n   * 'RoLucene', 'RuMicrosoft', 'RuLucene', 'SrCyrillicMicrosoft', 'SrLatinMicrosoft',\n   * 'SkMicrosoft', 'SlMicrosoft', 'EsMicrosoft', 'EsLucene', 'SvMicrosoft', 'SvLucene',\n   * 'TaMicrosoft', 'TeMicrosoft', 'ThMicrosoft', 'ThLucene', 'TrMicrosoft', 'TrLucene',\n   * 'UkMicrosoft', 'UrMicrosoft', 'ViMicrosoft', 'StandardLucene', 'StandardAsciiFoldingLucene',\n   * 'Keyword', 'Pattern', 'Simple', 'Stop', 'Whitespace'\n   */\n  analyzer?: LexicalAnalyzerName;\n  /**\n   * The name of the analyzer used at search time for the field. This option can be used only with\n   * searchable fields. It must be set together with indexAnalyzer and it cannot be set together\n   * with the analyzer option. This property cannot be set to the name of a language analyzer; use\n   * the analyzer property instead if you need a language analyzer. This analyzer can be updated on\n   * an existing field. Must be null for complex fields. Possible values include: 'ArMicrosoft',\n   * 'ArLucene', 'HyLucene', 'BnMicrosoft', 'EuLucene', 'BgMicrosoft', 'BgLucene', 'CaMicrosoft',\n   * 'CaLucene', 'ZhHansMicrosoft', 'ZhHansLucene', 'ZhHantMicrosoft', 'ZhHantLucene',\n   * 'HrMicrosoft', 'CsMicrosoft', 'CsLucene', 'DaMicrosoft', 'DaLucene', 'NlMicrosoft',\n   * 'NlLucene', 'EnMicrosoft', 'EnLucene', 'EtMicrosoft', 'FiMicrosoft', 'FiLucene',\n   * 'FrMicrosoft', 'FrLucene', 'GlLucene', 'DeMicrosoft', 'DeLucene', 'ElMicrosoft', 'ElLucene',\n   * 'GuMicrosoft', 'HeMicrosoft', 'HiMicrosoft', 'HiLucene', 'HuMicrosoft', 'HuLucene',\n   * 'IsMicrosoft', 'IdMicrosoft', 'IdLucene', 'GaLucene', 'ItMicrosoft', 'ItLucene',\n   * 'JaMicrosoft', 'JaLucene', 'KnMicrosoft', 'KoMicrosoft', 'KoLucene', 'LvMicrosoft',\n   * 'LvLucene', 'LtMicrosoft', 'MlMicrosoft', 'MsMicrosoft', 'MrMicrosoft', 'NbMicrosoft',\n   * 'NoLucene', 'FaLucene', 'PlMicrosoft', 'PlLucene', 'PtBrMicrosoft', 'PtBrLucene',\n   * 'PtPtMicrosoft', 'PtPtLucene', 'PaMicrosoft', 'RoMicrosoft', 'RoLucene', 'RuMicrosoft',\n   * 'RuLucene', 'SrCyrillicMicrosoft', 'SrLatinMicrosoft', 'SkMicrosoft', 'SlMicrosoft',\n   * 'EsMicrosoft', 'EsLucene', 'SvMicrosoft', 'SvLucene', 'TaMicrosoft', 'TeMicrosoft',\n   * 'ThMicrosoft', 'ThLucene', 'TrMicrosoft', 'TrLucene', 'UkMicrosoft', 'UrMicrosoft',\n   * 'ViMicrosoft', 'StandardLucene', 'StandardAsciiFoldingLucene', 'Keyword', 'Pattern', 'Simple',\n   * 'Stop', 'Whitespace'\n   */\n  searchAnalyzer?: LexicalAnalyzerName;\n  /**\n   * The name of the analyzer used at indexing time for the field. This option can be used only\n   * with searchable fields. It must be set together with searchAnalyzer and it cannot be set\n   * together with the analyzer option.  This property cannot be set to the name of a language\n   * analyzer; use the analyzer property instead if you need a language analyzer. Once the analyzer\n   * is chosen, it cannot be changed for the field. Must be null for complex fields. Possible\n   * values include: 'ArMicrosoft', 'ArLucene', 'HyLucene', 'BnMicrosoft', 'EuLucene',\n   * 'BgMicrosoft', 'BgLucene', 'CaMicrosoft', 'CaLucene', 'ZhHansMicrosoft', 'ZhHansLucene',\n   * 'ZhHantMicrosoft', 'ZhHantLucene', 'HrMicrosoft', 'CsMicrosoft', 'CsLucene', 'DaMicrosoft',\n   * 'DaLucene', 'NlMicrosoft', 'NlLucene', 'EnMicrosoft', 'EnLucene', 'EtMicrosoft',\n   * 'FiMicrosoft', 'FiLucene', 'FrMicrosoft', 'FrLucene', 'GlLucene', 'DeMicrosoft', 'DeLucene',\n   * 'ElMicrosoft', 'ElLucene', 'GuMicrosoft', 'HeMicrosoft', 'HiMicrosoft', 'HiLucene',\n   * 'HuMicrosoft', 'HuLucene', 'IsMicrosoft', 'IdMicrosoft', 'IdLucene', 'GaLucene',\n   * 'ItMicrosoft', 'ItLucene', 'JaMicrosoft', 'JaLucene', 'KnMicrosoft', 'KoMicrosoft',\n   * 'KoLucene', 'LvMicrosoft', 'LvLucene', 'LtMicrosoft', 'MlMicrosoft', 'MsMicrosoft',\n   * 'MrMicrosoft', 'NbMicrosoft', 'NoLucene', 'FaLucene', 'PlMicrosoft', 'PlLucene',\n   * 'PtBrMicrosoft', 'PtBrLucene', 'PtPtMicrosoft', 'PtPtLucene', 'PaMicrosoft', 'RoMicrosoft',\n   * 'RoLucene', 'RuMicrosoft', 'RuLucene', 'SrCyrillicMicrosoft', 'SrLatinMicrosoft',\n   * 'SkMicrosoft', 'SlMicrosoft', 'EsMicrosoft', 'EsLucene', 'SvMicrosoft', 'SvLucene',\n   * 'TaMicrosoft', 'TeMicrosoft', 'ThMicrosoft', 'ThLucene', 'TrMicrosoft', 'TrLucene',\n   * 'UkMicrosoft', 'UrMicrosoft', 'ViMicrosoft', 'StandardLucene', 'StandardAsciiFoldingLucene',\n   * 'Keyword', 'Pattern', 'Simple', 'Stop', 'Whitespace'\n   */\n  indexAnalyzer?: LexicalAnalyzerName;\n  /**\n   * A list of the names of synonym maps to associate with this field. This option can be used only\n   * with searchable fields. Currently only one synonym map per field is supported. Assigning a\n   * synonym map to a field ensures that query terms targeting that field are expanded at\n   * query-time using the rules in the synonym map. This attribute can be changed on existing\n   * fields. Must be null or an empty collection for complex fields.\n   */\n  synonymMaps?: string[];\n  /**\n   * A list of sub-fields if this is a field of type Edm.ComplexType or\n   * Collection(Edm.ComplexType). Must be null or empty for simple fields.\n   */\n  fields?: SearchField[];\n}\n\n/**\n * Defines weights on index fields for which matches should boost scoring in search queries.\n */\nexport interface TextWeights {\n  /**\n   * The dictionary of per-field weights to boost document scoring. The keys are field names and\n   * the values are the weights for each field.\n   */\n  weights: { [propertyName: string]: number };\n}\n\n/**\n * Contains the possible cases for ScoringFunction.\n */\nexport type ScoringFunctionUnion = ScoringFunction | DistanceScoringFunction | FreshnessScoringFunction | MagnitudeScoringFunction | TagScoringFunction;\n\n/**\n * Base type for functions that can modify document scores during ranking.\n */\nexport interface ScoringFunction {\n  /**\n   * Polymorphic Discriminator\n   */\n  type: \"ScoringFunction\";\n  /**\n   * The name of the field used as input to the scoring function.\n   */\n  fieldName: string;\n  /**\n   * A multiplier for the raw score. Must be a positive number not equal to 1.0.\n   */\n  boost: number;\n  /**\n   * A value indicating how boosting will be interpolated across document scores; defaults to\n   * \"Linear\". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'\n   */\n  interpolation?: ScoringFunctionInterpolation;\n}\n\n/**\n * Provides parameter values to a distance scoring function.\n */\nexport interface DistanceScoringParameters {\n  /**\n   * The name of the parameter passed in search queries to specify the reference location.\n   */\n  referencePointParameter: string;\n  /**\n   * The distance in kilometers from the reference location where the boosting range ends.\n   */\n  boostingDistance: number;\n}\n\n/**\n * Defines a function that boosts scores based on distance from a geographic location.\n */\nexport interface DistanceScoringFunction {\n  /**\n   * Polymorphic Discriminator\n   */\n  type: \"distance\";\n  /**\n   * The name of the field used as input to the scoring function.\n   */\n  fieldName: string;\n  /**\n   * A multiplier for the raw score. Must be a positive number not equal to 1.0.\n   */\n  boost: number;\n  /**\n   * A value indicating how boosting will be interpolated across document scores; defaults to\n   * \"Linear\". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'\n   */\n  interpolation?: ScoringFunctionInterpolation;\n  /**\n   * Parameter values for the distance scoring function.\n   */\n  parameters: DistanceScoringParameters;\n}\n\n/**\n * Provides parameter values to a freshness scoring function.\n */\nexport interface FreshnessScoringParameters {\n  /**\n   * The expiration period after which boosting will stop for a particular document.\n   */\n  boostingDuration: string;\n}\n\n/**\n * Defines a function that boosts scores based on the value of a date-time field.\n */\nexport interface FreshnessScoringFunction {\n  /**\n   * Polymorphic Discriminator\n   */\n  type: \"freshness\";\n  /**\n   * The name of the field used as input to the scoring function.\n   */\n  fieldName: string;\n  /**\n   * A multiplier for the raw score. Must be a positive number not equal to 1.0.\n   */\n  boost: number;\n  /**\n   * A value indicating how boosting will be interpolated across document scores; defaults to\n   * \"Linear\". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'\n   */\n  interpolation?: ScoringFunctionInterpolation;\n  /**\n   * Parameter values for the freshness scoring function.\n   */\n  parameters: FreshnessScoringParameters;\n}\n\n/**\n * Provides parameter values to a magnitude scoring function.\n */\nexport interface MagnitudeScoringParameters {\n  /**\n   * The field value at which boosting starts.\n   */\n  boostingRangeStart: number;\n  /**\n   * The field value at which boosting ends.\n   */\n  boostingRangeEnd: number;\n  /**\n   * A value indicating whether to apply a constant boost for field values beyond the range end\n   * value; default is false.\n   */\n  shouldBoostBeyondRangeByConstant?: boolean;\n}\n\n/**\n * Defines a function that boosts scores based on the magnitude of a numeric field.\n */\nexport interface MagnitudeScoringFunction {\n  /**\n   * Polymorphic Discriminator\n   */\n  type: \"magnitude\";\n  /**\n   * The name of the field used as input to the scoring function.\n   */\n  fieldName: string;\n  /**\n   * A multiplier for the raw score. Must be a positive number not equal to 1.0.\n   */\n  boost: number;\n  /**\n   * A value indicating how boosting will be interpolated across document scores; defaults to\n   * \"Linear\". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'\n   */\n  interpolation?: ScoringFunctionInterpolation;\n  /**\n   * Parameter values for the magnitude scoring function.\n   */\n  parameters: MagnitudeScoringParameters;\n}\n\n/**\n * Provides parameter values to a tag scoring function.\n */\nexport interface TagScoringParameters {\n  /**\n   * The name of the parameter passed in search queries to specify the list of tags to compare\n   * against the target field.\n   */\n  tagsParameter: string;\n}\n\n/**\n * Defines a function that boosts scores of documents with string values matching a given list of\n * tags.\n */\nexport interface TagScoringFunction {\n  /**\n   * Polymorphic Discriminator\n   */\n  type: \"tag\";\n  /**\n   * The name of the field used as input to the scoring function.\n   */\n  fieldName: string;\n  /**\n   * A multiplier for the raw score. Must be a positive number not equal to 1.0.\n   */\n  boost: number;\n  /**\n   * A value indicating how boosting will be interpolated across document scores; defaults to\n   * \"Linear\". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'\n   */\n  interpolation?: ScoringFunctionInterpolation;\n  /**\n   * Parameter values for the tag scoring function.\n   */\n  parameters: TagScoringParameters;\n}\n\n/**\n * Defines parameters for a search index that influence scoring in search queries.\n */\nexport interface ScoringProfile {\n  /**\n   * The name of the scoring profile.\n   */\n  name: string;\n  /**\n   * Parameters that boost scoring based on text matches in certain index fields.\n   */\n  textWeights?: TextWeights;\n  /**\n   * The collection of functions that influence the scoring of documents.\n   */\n  functions?: ScoringFunctionUnion[];\n  /**\n   * A value indicating how the results of individual scoring functions should be combined.\n   * Defaults to \"Sum\". Ignored if there are no scoring functions. Possible values include: 'Sum',\n   * 'Average', 'Minimum', 'Maximum', 'FirstMatching'\n   */\n  functionAggregation?: ScoringFunctionAggregation;\n}\n\n/**\n * Defines options to control Cross-Origin Resource Sharing (CORS) for an index.\n */\nexport interface CorsOptions {\n  /**\n   * The list of origins from which JavaScript code will be granted access to your index. Can\n   * contain a list of hosts of the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a\n   * single '*' to allow all origins (not recommended).\n   */\n  allowedOrigins: string[];\n  /**\n   * The duration for which browsers should cache CORS preflight responses. Defaults to 5 minutes.\n   */\n  maxAgeInSeconds?: number;\n}\n\n/**\n * Defines how the Suggest API should apply to a group of fields in the index.\n */\nexport interface Suggester {\n  /**\n   * The name of the suggester.\n   */\n  name: string;\n  /**\n   * The list of field names to which the suggester applies. Each field must be searchable.\n   */\n  sourceFields: string[];\n}\n\n/**\n * Credentials of a registered application created for your search service, used for authenticated\n * access to the encryption keys stored in Azure Key Vault.\n */\nexport interface AzureActiveDirectoryApplicationCredentials {\n  /**\n   * An AAD Application ID that was granted the required access permissions to the Azure Key Vault\n   * that is to be used when encrypting your data at rest. The Application ID should not be\n   * confused with the Object ID for your AAD Application.\n   */\n  applicationId: string;\n  /**\n   * The authentication key of the specified AAD application.\n   */\n  applicationSecret?: string;\n}\n\n/**\n * A customer-managed encryption key in Azure Key Vault. Keys that you create and manage can be\n * used to encrypt or decrypt data-at-rest in Azure Cognitive Search, such as indexes and synonym\n * maps.\n */\nexport interface SearchResourceEncryptionKey {\n  /**\n   * The name of your Azure Key Vault key to be used to encrypt your data at rest.\n   */\n  keyName: string;\n  /**\n   * The version of your Azure Key Vault key to be used to encrypt your data at rest.\n   */\n  keyVersion: string;\n  /**\n   * The URI of your Azure Key Vault, also referred to as DNS name, that contains the key to be\n   * used to encrypt your data at rest. An example URI might be\n   * https://my-keyvault-name.vault.azure.net.\n   */\n  vaultUri: string;\n  /**\n   * Optional Azure Active Directory credentials used for accessing your Azure Key Vault. Not\n   * required if using managed identity instead.\n   */\n  accessCredentials?: AzureActiveDirectoryApplicationCredentials;\n}\n\n/**\n * Represents a search index definition, which describes the fields and search behavior of an\n * index.\n */\nexport interface SearchIndex {\n  /**\n   * The name of the index.\n   */\n  name: string;\n  /**\n   * The fields of the index.\n   */\n  fields: SearchField[];\n  /**\n   * The scoring profiles for the index.\n   */\n  scoringProfiles?: ScoringProfile[];\n  /**\n   * The name of the scoring profile to use if none is specified in the query. If this property is\n   * not set and no scoring profile is specified in the query, then default scoring (tf-idf) will\n   * be used.\n   */\n  defaultScoringProfile?: string;\n  /**\n   * Options to control Cross-Origin Resource Sharing (CORS) for the index.\n   */\n  corsOptions?: CorsOptions;\n  /**\n   * The suggesters for the index.\n   */\n  suggesters?: Suggester[];\n  /**\n   * The analyzers for the index.\n   */\n  analyzers?: LexicalAnalyzerUnion[];\n  /**\n   * The tokenizers for the index.\n   */\n  tokenizers?: LexicalTokenizerUnion[];\n  /**\n   * The token filters for the index.\n   */\n  tokenFilters?: TokenFilterUnion[];\n  /**\n   * The character filters for the index.\n   */\n  charFilters?: CharFilterUnion[];\n  /**\n   * A description of an encryption key that you create in Azure Key Vault. This key is used to\n   * provide an additional level of encryption-at-rest for your data when you want full assurance\n   * that no one, not even Microsoft, can decrypt your data in Azure Cognitive Search. Once you\n   * have encrypted your data, it will always remain encrypted. Azure Cognitive Search will ignore\n   * attempts to set this property to null. You can change this property as needed if you want to\n   * rotate your encryption key; Your data will be unaffected. Encryption with customer-managed\n   * keys is not available for free search services, and is only available for paid services\n   * created on or after January 1, 2019.\n   */\n  encryptionKey?: SearchResourceEncryptionKey;\n  /**\n   * The type of similarity algorithm to be used when scoring and ranking the documents matching a\n   * search query. The similarity algorithm can only be defined at index creation time and cannot\n   * be modified on existing indexes. If null, the ClassicSimilarity algorithm is used.\n   */\n  similarity?: SimilarityUnion;\n  /**\n   * The ETag of the index.\n   */\n  etag?: string;\n}\n\n/**\n * Statistics for a given index. Statistics are collected periodically and are not guaranteed to\n * always be up-to-date.\n */\nexport interface GetIndexStatisticsResult {\n  /**\n   * The number of documents in the index.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly documentCount: number;\n  /**\n   * The amount of storage in bytes consumed by the index.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly storageSize: number;\n}\n\n/**\n * Response from a List Indexes request. If successful, it includes the full definitions of all\n * indexes.\n */\nexport interface ListIndexesResult {\n  /**\n   * The indexes in the Search service.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly indexes: SearchIndex[];\n}\n\n/**\n * Input field mapping for a skill.\n */\nexport interface InputFieldMappingEntry {\n  /**\n   * The name of the input.\n   */\n  name: string;\n  /**\n   * The source of the input.\n   */\n  source?: string;\n  /**\n   * The source context used for selecting recursive inputs.\n   */\n  sourceContext?: string;\n  /**\n   * The recursive inputs used when creating a complex type.\n   */\n  inputs?: InputFieldMappingEntry[];\n}\n\n/**\n * Output field mapping for a skill.\n */\nexport interface OutputFieldMappingEntry {\n  /**\n   * The name of the output defined by the skill.\n   */\n  name: string;\n  /**\n   * The target name of the output. It is optional and default to name.\n   */\n  targetName?: string;\n}\n\n/**\n * Contains the possible cases for SearchIndexerSkill.\n */\nexport type SearchIndexerSkillUnion = SearchIndexerSkill | ConditionalSkill | KeyPhraseExtractionSkill | OcrSkill | ImageAnalysisSkill | LanguageDetectionSkill | ShaperSkill | MergeSkill | EntityRecognitionSkill | SentimentSkill | SplitSkill | TextTranslationSkill | WebApiSkill;\n\n/**\n * Base type for skills.\n */\nexport interface SearchIndexerSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"SearchIndexerSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n}\n\n/**\n * Contains the possible cases for CognitiveServicesAccount.\n */\nexport type CognitiveServicesAccountUnion = CognitiveServicesAccount | DefaultCognitiveServicesAccount | CognitiveServicesAccountKey;\n\n/**\n * Base type for describing any cognitive service resource attached to a skillset.\n */\nexport interface CognitiveServicesAccount {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"CognitiveServicesAccount\";\n  /**\n   * Description of the cognitive service resource attached to a skillset.\n   */\n  description?: string;\n}\n\n/**\n * A list of skills.\n */\nexport interface SearchIndexerSkillset {\n  /**\n   * The name of the skillset.\n   */\n  name: string;\n  /**\n   * The description of the skillset.\n   */\n  description?: string;\n  /**\n   * A list of skills in the skillset.\n   */\n  skills: SearchIndexerSkillUnion[];\n  /**\n   * Details about cognitive services to be used when running skills.\n   */\n  cognitiveServicesAccount?: CognitiveServicesAccountUnion;\n  /**\n   * The ETag of the skillset.\n   */\n  etag?: string;\n}\n\n/**\n * An empty object that represents the default cognitive service resource for a skillset.\n */\nexport interface DefaultCognitiveServicesAccount {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.DefaultCognitiveServices\";\n  /**\n   * Description of the cognitive service resource attached to a skillset.\n   */\n  description?: string;\n}\n\n/**\n * A cognitive service resource provisioned with a key that is attached to a skillset.\n */\nexport interface CognitiveServicesAccountKey {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Azure.Search.CognitiveServicesByKey\";\n  /**\n   * Description of the cognitive service resource attached to a skillset.\n   */\n  description?: string;\n  /**\n   * The key used to provision the cognitive service resource attached to a skillset.\n   */\n  key: string;\n}\n\n/**\n * A skill that enables scenarios that require a Boolean operation to determine the data to assign\n * to an output.\n */\nexport interface ConditionalSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Util.ConditionalSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n}\n\n/**\n * A skill that uses text analytics for key phrase extraction.\n */\nexport interface KeyPhraseExtractionSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Text.KeyPhraseExtractionSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * A value indicating which language code to use. Default is en. Possible values include: 'da',\n   * 'nl', 'en', 'fi', 'fr', 'de', 'it', 'ja', 'ko', 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv'\n   */\n  defaultLanguageCode?: KeyPhraseExtractionSkillLanguage;\n  /**\n   * A number indicating how many key phrases to return. If absent, all identified key phrases will\n   * be returned.\n   */\n  maxKeyPhraseCount?: number;\n}\n\n/**\n * A skill that extracts text from image files.\n */\nexport interface OcrSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Vision.OcrSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * A value indicating which language code to use. Default is en. Possible values include:\n   * 'zh-Hans', 'zh-Hant', 'cs', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'hu', 'it', 'ja', 'ko',\n   * 'nb', 'pl', 'pt', 'ru', 'es', 'sv', 'tr', 'ar', 'ro', 'sr-Cyrl', 'sr-Latn', 'sk'\n   */\n  defaultLanguageCode?: OcrSkillLanguage;\n  /**\n   * A value indicating to turn orientation detection on or not. Default is false. Default value:\n   * false.\n   */\n  shouldDetectOrientation?: boolean;\n}\n\n/**\n * A skill that analyzes image files. It extracts a rich set of visual features based on the image\n * content.\n */\nexport interface ImageAnalysisSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Vision.ImageAnalysisSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * A value indicating which language code to use. Default is en. Possible values include: 'en',\n   * 'es', 'ja', 'pt', 'zh'\n   */\n  defaultLanguageCode?: ImageAnalysisSkillLanguage;\n  /**\n   * A list of visual features.\n   */\n  visualFeatures?: VisualFeature[];\n  /**\n   * A string indicating which domain-specific details to return.\n   */\n  details?: ImageDetail[];\n}\n\n/**\n * A skill that detects the language of input text and reports a single language code for every\n * document submitted on the request. The language code is paired with a score indicating the\n * confidence of the analysis.\n */\nexport interface LanguageDetectionSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Text.LanguageDetectionSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n}\n\n/**\n * A skill for reshaping the outputs. It creates a complex type to support composite fields (also\n * known as multipart fields).\n */\nexport interface ShaperSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Util.ShaperSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n}\n\n/**\n * A skill for merging two or more strings into a single unified string, with an optional\n * user-defined delimiter separating each component part.\n */\nexport interface MergeSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Text.MergeSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * The tag indicates the start of the merged text. By default, the tag is an empty space. Default\n   * value: ''.\n   */\n  insertPreTag?: string;\n  /**\n   * The tag indicates the end of the merged text. By default, the tag is an empty space. Default\n   * value: ''.\n   */\n  insertPostTag?: string;\n}\n\n/**\n * Text analytics entity recognition.\n */\nexport interface EntityRecognitionSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Text.EntityRecognitionSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * A list of entity categories that should be extracted.\n   */\n  categories?: EntityCategory[];\n  /**\n   * A value indicating which language code to use. Default is en. Possible values include: 'ar',\n   * 'cs', 'zh-Hans', 'zh-Hant', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'hu', 'it', 'ja', 'ko',\n   * 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv', 'tr'\n   */\n  defaultLanguageCode?: EntityRecognitionSkillLanguage;\n  /**\n   * Determines whether or not to include entities which are well known but don't conform to a\n   * pre-defined type. If this configuration is not set (default), set to null or set to false,\n   * entities which don't conform to one of the pre-defined types will not be surfaced.\n   */\n  includeTypelessEntities?: boolean;\n  /**\n   * A value between 0 and 1 that be used to only include entities whose confidence score is\n   * greater than the value specified. If not set (default), or if explicitly set to null, all\n   * entities will be included.\n   */\n  minimumPrecision?: number;\n}\n\n/**\n * Text analytics positive-negative sentiment analysis, scored as a floating point value in a range\n * of zero to 1.\n */\nexport interface SentimentSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Text.SentimentSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * A value indicating which language code to use. Default is en. Possible values include: 'da',\n   * 'nl', 'en', 'fi', 'fr', 'de', 'el', 'it', 'no', 'pl', 'pt-PT', 'ru', 'es', 'sv', 'tr'\n   */\n  defaultLanguageCode?: SentimentSkillLanguage;\n}\n\n/**\n * A skill to split a string into chunks of text.\n */\nexport interface SplitSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Text.SplitSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * A value indicating which language code to use. Default is en. Possible values include: 'da',\n   * 'de', 'en', 'es', 'fi', 'fr', 'it', 'ko', 'pt'\n   */\n  defaultLanguageCode?: SplitSkillLanguage;\n  /**\n   * A value indicating which split mode to perform. Possible values include: 'Pages', 'Sentences'\n   */\n  textSplitMode?: TextSplitMode;\n  /**\n   * The desired maximum page length. Default is 10000.\n   */\n  maxPageLength?: number;\n}\n\n/**\n * A skill to translate text from one language to another.\n */\nexport interface TextTranslationSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Text.TranslationSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * The language code to translate documents into for documents that don't specify the to language\n   * explicitly. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca', 'zh-Hans',\n   * 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el', 'ht',\n   * 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg', 'ms',\n   * 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl', 'es',\n   * 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'\n   */\n  defaultToLanguageCode: TextTranslationSkillLanguage;\n  /**\n   * The language code to translate documents from for documents that don't specify the from\n   * language explicitly. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca',\n   * 'zh-Hans', 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el',\n   * 'ht', 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg',\n   * 'ms', 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl',\n   * 'es', 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'\n   */\n  defaultFromLanguageCode?: TextTranslationSkillLanguage;\n  /**\n   * The language code to translate documents from when neither the fromLanguageCode input nor the\n   * defaultFromLanguageCode parameter are provided, and the automatic language detection is\n   * unsuccessful. Default is en. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue',\n   * 'ca', 'zh-Hans', 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de',\n   * 'el', 'ht', 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt',\n   * 'mg', 'ms', 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk',\n   * 'sl', 'es', 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'\n   */\n  suggestedFrom?: TextTranslationSkillLanguage;\n}\n\n/**\n * A skill that can call a Web API endpoint, allowing you to extend a skillset by having it call\n * your custom code.\n */\nexport interface WebApiSkill {\n  /**\n   * Polymorphic Discriminator\n   */\n  odatatype: \"#Microsoft.Skills.Custom.WebApiSkill\";\n  /**\n   * The name of the skill which uniquely identifies it within the skillset. A skill with no name\n   * defined will be given a default name of its 1-based index in the skills array, prefixed with\n   * the character '#'.\n   */\n  name?: string;\n  /**\n   * The description of the skill which describes the inputs, outputs, and usage of the skill.\n   */\n  description?: string;\n  /**\n   * Represents the level at which operations take place, such as the document root or document\n   * content (for example, /document or /document/content). The default is /document.\n   */\n  context?: string;\n  /**\n   * Inputs of the skills could be a column in the source data set, or the output of an upstream\n   * skill.\n   */\n  inputs: InputFieldMappingEntry[];\n  /**\n   * The output of a skill is either a field in a search index, or a value that can be consumed as\n   * an input by another skill.\n   */\n  outputs: OutputFieldMappingEntry[];\n  /**\n   * The url for the Web API.\n   */\n  uri: string;\n  /**\n   * The headers required to make the http request.\n   */\n  httpHeaders?: { [propertyName: string]: string };\n  /**\n   * The method for the http request.\n   */\n  httpMethod?: string;\n  /**\n   * The desired timeout for the request. Default is 30 seconds.\n   */\n  timeout?: string;\n  /**\n   * The desired batch size which indicates number of documents.\n   */\n  batchSize?: number;\n  /**\n   * If set, the number of parallel calls that can be made to the Web API.\n   */\n  degreeOfParallelism?: number;\n}\n\n/**\n * Response from a list skillset request. If successful, it includes the full definitions of all\n * skillsets.\n */\nexport interface ListSkillsetsResult {\n  /**\n   * The skillsets defined in the Search service.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly skillsets: SearchIndexerSkillset[];\n}\n\n/**\n * Represents a synonym map definition.\n */\nexport interface SynonymMap {\n  /**\n   * The name of the synonym map.\n   */\n  name: string;\n  /**\n   * A series of synonym rules in the specified synonym map format. The rules must be separated by\n   * newlines.\n   */\n  synonyms: string;\n  /**\n   * A description of an encryption key that you create in Azure Key Vault. This key is used to\n   * provide an additional level of encryption-at-rest for your data when you want full assurance\n   * that no one, not even Microsoft, can decrypt your data in Azure Cognitive Search. Once you\n   * have encrypted your data, it will always remain encrypted. Azure Cognitive Search will ignore\n   * attempts to set this property to null. You can change this property as needed if you want to\n   * rotate your encryption key; Your data will be unaffected. Encryption with customer-managed\n   * keys is not available for free search services, and is only available for paid services\n   * created on or after January 1, 2019.\n   */\n  encryptionKey?: SearchResourceEncryptionKey;\n  /**\n   * The ETag of the synonym map.\n   */\n  etag?: string;\n}\n\n/**\n * Response from a List SynonymMaps request. If successful, it includes the full definitions of all\n * synonym maps.\n */\nexport interface ListSynonymMapsResult {\n  /**\n   * The synonym maps in the Search service.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly synonymMaps: SynonymMap[];\n}\n\n/**\n * Represents a resource's usage and quota.\n */\nexport interface ResourceCounter {\n  /**\n   * The resource usage amount.\n   */\n  usage: number;\n  /**\n   * The resource amount quota.\n   */\n  quota?: number;\n}\n\n/**\n * Represents service-level resource counters and quotas.\n */\nexport interface ServiceCounters {\n  /**\n   * Total number of documents across all indexes in the service.\n   */\n  documentCounter: ResourceCounter;\n  /**\n   * Total number of indexes.\n   */\n  indexCounter: ResourceCounter;\n  /**\n   * Total number of indexers.\n   */\n  indexerCounter: ResourceCounter;\n  /**\n   * Total number of data sources.\n   */\n  dataSourceCounter: ResourceCounter;\n  /**\n   * Total size of used storage in bytes.\n   */\n  storageSizeCounter: ResourceCounter;\n  /**\n   * Total number of synonym maps.\n   */\n  synonymMapCounter: ResourceCounter;\n}\n\n/**\n * Represents various service level limits.\n */\nexport interface ServiceLimits {\n  /**\n   * The maximum allowed fields per index.\n   */\n  maxFieldsPerIndex?: number;\n  /**\n   * The maximum depth which you can nest sub-fields in an index, including the top-level complex\n   * field. For example, a/b/c has a nesting depth of 3.\n   */\n  maxFieldNestingDepthPerIndex?: number;\n  /**\n   * The maximum number of fields of type Collection(Edm.ComplexType) allowed in an index.\n   */\n  maxComplexCollectionFieldsPerIndex?: number;\n  /**\n   * The maximum number of objects in complex collections allowed per document.\n   */\n  maxComplexObjectsInCollectionsPerDocument?: number;\n}\n\n/**\n * Response from a get service statistics request. If successful, it includes service level\n * counters and limits.\n */\nexport interface ServiceStatistics {\n  /**\n   * Service level resource counters.\n   */\n  counters: ServiceCounters;\n  /**\n   * Service level general limits.\n   */\n  limits: ServiceLimits;\n}\n\n/**\n * Describes an error condition for the Azure Cognitive Search API.\n */\nexport interface SearchError {\n  /**\n   * One of a server-defined set of error codes.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly code?: string;\n  /**\n   * A human-readable representation of the error.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly message: string;\n  /**\n   * An array of details about specific errors that led to this reported error.\n   * **NOTE: This property will not be serialized. It can only be populated by the server.**\n   */\n  readonly details?: SearchError[];\n}\n\n/**\n * Optional Parameters.\n */\nexport interface DataSourcesCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface DataSourcesDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface DataSourcesListOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Selects which top-level properties of the data sources to retrieve. Specified as a\n   * comma-separated list of JSON property names, or '*' for all properties. The default is all\n   * properties.\n   */\n  select?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface IndexersCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface IndexersDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface IndexersListOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Selects which top-level properties of the indexers to retrieve. Specified as a comma-separated\n   * list of JSON property names, or '*' for all properties. The default is all properties.\n   */\n  select?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface SkillsetsCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface SkillsetsDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface SkillsetsListOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Selects which top-level properties of the skillsets to retrieve. Specified as a\n   * comma-separated list of JSON property names, or '*' for all properties. The default is all\n   * properties.\n   */\n  select?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface SynonymMapsCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface SynonymMapsDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface SynonymMapsListOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Selects which top-level properties of the synonym maps to retrieve. Specified as a\n   * comma-separated list of JSON property names, or '*' for all properties. The default is all\n   * properties.\n   */\n  select?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface IndexesListOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Selects which top-level properties of the index definitions to retrieve. Specified as a\n   * comma-separated list of JSON property names, or '*' for all properties. The default is all\n   * properties.\n   */\n  select?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface IndexesCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Allows new analyzers, tokenizers, token filters, or char filters to be added to an index by\n   * taking the index offline for at least a few seconds. This temporarily causes indexing and\n   * query requests to fail. Performance and write availability of the index can be impaired for\n   * several minutes after the index is updated, or longer for very large indexes.\n   */\n  allowIndexDowntime?: boolean;\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Optional Parameters.\n */\nexport interface IndexesDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {\n  /**\n   * Defines the If-Match condition. The operation will be performed only if the ETag on the server\n   * matches this value.\n   */\n  ifMatch?: string;\n  /**\n   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the\n   * server does not match this value.\n   */\n  ifNoneMatch?: string;\n}\n\n/**\n * Defines values for LexicalAnalyzerName.\n * Possible values include: 'ArMicrosoft', 'ArLucene', 'HyLucene', 'BnMicrosoft', 'EuLucene',\n * 'BgMicrosoft', 'BgLucene', 'CaMicrosoft', 'CaLucene', 'ZhHansMicrosoft', 'ZhHansLucene',\n * 'ZhHantMicrosoft', 'ZhHantLucene', 'HrMicrosoft', 'CsMicrosoft', 'CsLucene', 'DaMicrosoft',\n * 'DaLucene', 'NlMicrosoft', 'NlLucene', 'EnMicrosoft', 'EnLucene', 'EtMicrosoft', 'FiMicrosoft',\n * 'FiLucene', 'FrMicrosoft', 'FrLucene', 'GlLucene', 'DeMicrosoft', 'DeLucene', 'ElMicrosoft',\n * 'ElLucene', 'GuMicrosoft', 'HeMicrosoft', 'HiMicrosoft', 'HiLucene', 'HuMicrosoft', 'HuLucene',\n * 'IsMicrosoft', 'IdMicrosoft', 'IdLucene', 'GaLucene', 'ItMicrosoft', 'ItLucene', 'JaMicrosoft',\n * 'JaLucene', 'KnMicrosoft', 'KoMicrosoft', 'KoLucene', 'LvMicrosoft', 'LvLucene', 'LtMicrosoft',\n * 'MlMicrosoft', 'MsMicrosoft', 'MrMicrosoft', 'NbMicrosoft', 'NoLucene', 'FaLucene',\n * 'PlMicrosoft', 'PlLucene', 'PtBrMicrosoft', 'PtBrLucene', 'PtPtMicrosoft', 'PtPtLucene',\n * 'PaMicrosoft', 'RoMicrosoft', 'RoLucene', 'RuMicrosoft', 'RuLucene', 'SrCyrillicMicrosoft',\n * 'SrLatinMicrosoft', 'SkMicrosoft', 'SlMicrosoft', 'EsMicrosoft', 'EsLucene', 'SvMicrosoft',\n * 'SvLucene', 'TaMicrosoft', 'TeMicrosoft', 'ThMicrosoft', 'ThLucene', 'TrMicrosoft', 'TrLucene',\n * 'UkMicrosoft', 'UrMicrosoft', 'ViMicrosoft', 'StandardLucene', 'StandardAsciiFoldingLucene',\n * 'Keyword', 'Pattern', 'Simple', 'Stop', 'Whitespace'\n * @readonly\n * @enum {string}\n */\nexport type LexicalAnalyzerName = 'ar.microsoft' | 'ar.lucene' | 'hy.lucene' | 'bn.microsoft' | 'eu.lucene' | 'bg.microsoft' | 'bg.lucene' | 'ca.microsoft' | 'ca.lucene' | 'zh-Hans.microsoft' | 'zh-Hans.lucene' | 'zh-Hant.microsoft' | 'zh-Hant.lucene' | 'hr.microsoft' | 'cs.microsoft' | 'cs.lucene' | 'da.microsoft' | 'da.lucene' | 'nl.microsoft' | 'nl.lucene' | 'en.microsoft' | 'en.lucene' | 'et.microsoft' | 'fi.microsoft' | 'fi.lucene' | 'fr.microsoft' | 'fr.lucene' | 'gl.lucene' | 'de.microsoft' | 'de.lucene' | 'el.microsoft' | 'el.lucene' | 'gu.microsoft' | 'he.microsoft' | 'hi.microsoft' | 'hi.lucene' | 'hu.microsoft' | 'hu.lucene' | 'is.microsoft' | 'id.microsoft' | 'id.lucene' | 'ga.lucene' | 'it.microsoft' | 'it.lucene' | 'ja.microsoft' | 'ja.lucene' | 'kn.microsoft' | 'ko.microsoft' | 'ko.lucene' | 'lv.microsoft' | 'lv.lucene' | 'lt.microsoft' | 'ml.microsoft' | 'ms.microsoft' | 'mr.microsoft' | 'nb.microsoft' | 'no.lucene' | 'fa.lucene' | 'pl.microsoft' | 'pl.lucene' | 'pt-BR.microsoft' | 'pt-BR.lucene' | 'pt-PT.microsoft' | 'pt-PT.lucene' | 'pa.microsoft' | 'ro.microsoft' | 'ro.lucene' | 'ru.microsoft' | 'ru.lucene' | 'sr-cyrillic.microsoft' | 'sr-latin.microsoft' | 'sk.microsoft' | 'sl.microsoft' | 'es.microsoft' | 'es.lucene' | 'sv.microsoft' | 'sv.lucene' | 'ta.microsoft' | 'te.microsoft' | 'th.microsoft' | 'th.lucene' | 'tr.microsoft' | 'tr.lucene' | 'uk.microsoft' | 'ur.microsoft' | 'vi.microsoft' | 'standard.lucene' | 'standardasciifolding.lucene' | 'keyword' | 'pattern' | 'simple' | 'stop' | 'whitespace';\n\n/**\n * Defines values for LexicalTokenizerName.\n * Possible values include: 'Classic', 'EdgeNGram', 'Keyword', 'Letter', 'Lowercase',\n * 'MicrosoftLanguageTokenizer', 'MicrosoftLanguageStemmingTokenizer', 'NGram', 'PathHierarchy',\n * 'Pattern', 'Standard', 'UaxUrlEmail', 'Whitespace'\n * @readonly\n * @enum {string}\n */\nexport type LexicalTokenizerName = 'classic' | 'edgeNGram' | 'keyword_v2' | 'letter' | 'lowercase' | 'microsoft_language_tokenizer' | 'microsoft_language_stemming_tokenizer' | 'nGram' | 'path_hierarchy_v2' | 'pattern' | 'standard_v2' | 'uax_url_email' | 'whitespace';\n\n/**\n * Defines values for TokenFilterName.\n * Possible values include: 'ArabicNormalization', 'Apostrophe', 'AsciiFolding', 'CjkBigram',\n * 'CjkWidth', 'Classic', 'CommonGram', 'EdgeNGram', 'Elision', 'GermanNormalization',\n * 'HindiNormalization', 'IndicNormalization', 'KeywordRepeat', 'KStem', 'Length', 'Limit',\n * 'Lowercase', 'NGram', 'PersianNormalization', 'Phonetic', 'PorterStem', 'Reverse',\n * 'ScandinavianNormalization', 'ScandinavianFoldingNormalization', 'Shingle', 'Snowball',\n * 'SoraniNormalization', 'Stemmer', 'Stopwords', 'Trim', 'Truncate', 'Unique', 'Uppercase',\n * 'WordDelimiter'\n * @readonly\n * @enum {string}\n */\nexport type TokenFilterName = 'arabic_normalization' | 'apostrophe' | 'asciifolding' | 'cjk_bigram' | 'cjk_width' | 'classic' | 'common_grams' | 'edgeNGram_v2' | 'elision' | 'german_normalization' | 'hindi_normalization' | 'indic_normalization' | 'keyword_repeat' | 'kstem' | 'length' | 'limit' | 'lowercase' | 'nGram_v2' | 'persian_normalization' | 'phonetic' | 'porter_stem' | 'reverse' | 'scandinavian_normalization' | 'scandinavian_folding' | 'shingle' | 'snowball' | 'sorani_normalization' | 'stemmer' | 'stopwords' | 'trim' | 'truncate' | 'unique' | 'uppercase' | 'word_delimiter';\n\n/**\n * Defines values for CharFilterName.\n * Possible values include: 'HtmlStrip'\n * @readonly\n * @enum {string}\n */\nexport type CharFilterName = 'html_strip';\n\n/**\n * Defines values for RegexFlags.\n * Possible values include: 'CanonEq', 'CaseInsensitive', 'Comments', 'DotAll', 'Literal',\n * 'Multiline', 'UnicodeCase', 'UnixLines'\n * @readonly\n * @enum {string}\n */\nexport type RegexFlags = 'CANON_EQ' | 'CASE_INSENSITIVE' | 'COMMENTS' | 'DOTALL' | 'LITERAL' | 'MULTILINE' | 'UNICODE_CASE' | 'UNIX_LINES';\n\n/**\n * Defines values for SearchFieldDataType.\n * Possible values include: 'String', 'Int32', 'Int64', 'Double', 'Boolean', 'DateTimeOffset',\n * 'GeographyPoint', 'Complex', 'Collection(Edm.String)', 'Collection(Edm.Int32)',\n * 'Collection(Edm.Int64)', 'Collection(Edm.Double)', 'Collection(Edm.Boolean)',\n * 'Collection(Edm.DateTimeOffset)', 'Collection(Edm.GeographyPoint)',\n * 'Collection(Edm.ComplexType)'\n * @readonly\n * @enum {string}\n */\nexport type SearchFieldDataType = 'Edm.String' | 'Edm.Int32' | 'Edm.Int64' | 'Edm.Double' | 'Edm.Boolean' | 'Edm.DateTimeOffset' | 'Edm.GeographyPoint' | 'Edm.ComplexType' | 'Collection(Edm.String)' | 'Collection(Edm.Int32)' | 'Collection(Edm.Int64)' | 'Collection(Edm.Double)' | 'Collection(Edm.Boolean)' | 'Collection(Edm.DateTimeOffset)' | 'Collection(Edm.GeographyPoint)' | 'Collection(Edm.ComplexType)';\n\n/**\n * Defines values for TokenCharacterKind.\n * Possible values include: 'Letter', 'Digit', 'Whitespace', 'Punctuation', 'Symbol'\n * @readonly\n * @enum {string}\n */\nexport type TokenCharacterKind = 'letter' | 'digit' | 'whitespace' | 'punctuation' | 'symbol';\n\n/**\n * Defines values for MicrosoftTokenizerLanguage.\n * Possible values include: 'Bangla', 'Bulgarian', 'Catalan', 'ChineseSimplified',\n * 'ChineseTraditional', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'French', 'German',\n * 'Greek', 'Gujarati', 'Hindi', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Kannada',\n * 'Korean', 'Malay', 'Malayalam', 'Marathi', 'NorwegianBokmaal', 'Polish', 'Portuguese',\n * 'PortugueseBrazilian', 'Punjabi', 'Romanian', 'Russian', 'SerbianCyrillic', 'SerbianLatin',\n * 'Slovenian', 'Spanish', 'Swedish', 'Tamil', 'Telugu', 'Thai', 'Ukrainian', 'Urdu', 'Vietnamese'\n * @readonly\n * @enum {string}\n */\nexport type MicrosoftTokenizerLanguage = 'bangla' | 'bulgarian' | 'catalan' | 'chineseSimplified' | 'chineseTraditional' | 'croatian' | 'czech' | 'danish' | 'dutch' | 'english' | 'french' | 'german' | 'greek' | 'gujarati' | 'hindi' | 'icelandic' | 'indonesian' | 'italian' | 'japanese' | 'kannada' | 'korean' | 'malay' | 'malayalam' | 'marathi' | 'norwegianBokmaal' | 'polish' | 'portuguese' | 'portugueseBrazilian' | 'punjabi' | 'romanian' | 'russian' | 'serbianCyrillic' | 'serbianLatin' | 'slovenian' | 'spanish' | 'swedish' | 'tamil' | 'telugu' | 'thai' | 'ukrainian' | 'urdu' | 'vietnamese';\n\n/**\n * Defines values for MicrosoftStemmingTokenizerLanguage.\n * Possible values include: 'Arabic', 'Bangla', 'Bulgarian', 'Catalan', 'Croatian', 'Czech',\n * 'Danish', 'Dutch', 'English', 'Estonian', 'Finnish', 'French', 'German', 'Greek', 'Gujarati',\n * 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Kannada', 'Latvian',\n * 'Lithuanian', 'Malay', 'Malayalam', 'Marathi', 'NorwegianBokmaal', 'Polish', 'Portuguese',\n * 'PortugueseBrazilian', 'Punjabi', 'Romanian', 'Russian', 'SerbianCyrillic', 'SerbianLatin',\n * 'Slovak', 'Slovenian', 'Spanish', 'Swedish', 'Tamil', 'Telugu', 'Turkish', 'Ukrainian', 'Urdu'\n * @readonly\n * @enum {string}\n */\nexport type MicrosoftStemmingTokenizerLanguage = 'arabic' | 'bangla' | 'bulgarian' | 'catalan' | 'croatian' | 'czech' | 'danish' | 'dutch' | 'english' | 'estonian' | 'finnish' | 'french' | 'german' | 'greek' | 'gujarati' | 'hebrew' | 'hindi' | 'hungarian' | 'icelandic' | 'indonesian' | 'italian' | 'kannada' | 'latvian' | 'lithuanian' | 'malay' | 'malayalam' | 'marathi' | 'norwegianBokmaal' | 'polish' | 'portuguese' | 'portugueseBrazilian' | 'punjabi' | 'romanian' | 'russian' | 'serbianCyrillic' | 'serbianLatin' | 'slovak' | 'slovenian' | 'spanish' | 'swedish' | 'tamil' | 'telugu' | 'turkish' | 'ukrainian' | 'urdu';\n\n/**\n * Defines values for CjkBigramTokenFilterScripts.\n * Possible values include: 'Han', 'Hiragana', 'Katakana', 'Hangul'\n * @readonly\n * @enum {string}\n */\nexport type CjkBigramTokenFilterScripts = 'han' | 'hiragana' | 'katakana' | 'hangul';\n\n/**\n * Defines values for EdgeNGramTokenFilterSide.\n * Possible values include: 'Front', 'Back'\n * @readonly\n * @enum {string}\n */\nexport type EdgeNGramTokenFilterSide = 'front' | 'back';\n\n/**\n * Defines values for PhoneticEncoder.\n * Possible values include: 'Metaphone', 'DoubleMetaphone', 'Soundex', 'RefinedSoundex',\n * 'Caverphone1', 'Caverphone2', 'Cologne', 'Nysiis', 'KoelnerPhonetik', 'HaasePhonetik',\n * 'BeiderMorse'\n * @readonly\n * @enum {string}\n */\nexport type PhoneticEncoder = 'metaphone' | 'doubleMetaphone' | 'soundex' | 'refinedSoundex' | 'caverphone1' | 'caverphone2' | 'cologne' | 'nysiis' | 'koelnerPhonetik' | 'haasePhonetik' | 'beiderMorse';\n\n/**\n * Defines values for SnowballTokenFilterLanguage.\n * Possible values include: 'Armenian', 'Basque', 'Catalan', 'Danish', 'Dutch', 'English',\n * 'Finnish', 'French', 'German', 'German2', 'Hungarian', 'Italian', 'Kp', 'Lovins', 'Norwegian',\n * 'Porter', 'Portuguese', 'Romanian', 'Russian', 'Spanish', 'Swedish', 'Turkish'\n * @readonly\n * @enum {string}\n */\nexport type SnowballTokenFilterLanguage = 'armenian' | 'basque' | 'catalan' | 'danish' | 'dutch' | 'english' | 'finnish' | 'french' | 'german' | 'german2' | 'hungarian' | 'italian' | 'kp' | 'lovins' | 'norwegian' | 'porter' | 'portuguese' | 'romanian' | 'russian' | 'spanish' | 'swedish' | 'turkish';\n\n/**\n * Defines values for StemmerTokenFilterLanguage.\n * Possible values include: 'Arabic', 'Armenian', 'Basque', 'Brazilian', 'Bulgarian', 'Catalan',\n * 'Czech', 'Danish', 'Dutch', 'DutchKp', 'English', 'LightEnglish', 'MinimalEnglish',\n * 'PossessiveEnglish', 'Porter2', 'Lovins', 'Finnish', 'LightFinnish', 'French', 'LightFrench',\n * 'MinimalFrench', 'Galician', 'MinimalGalician', 'German', 'German2', 'LightGerman',\n * 'MinimalGerman', 'Greek', 'Hindi', 'Hungarian', 'LightHungarian', 'Indonesian', 'Irish',\n * 'Italian', 'LightItalian', 'Sorani', 'Latvian', 'Norwegian', 'LightNorwegian',\n * 'MinimalNorwegian', 'LightNynorsk', 'MinimalNynorsk', 'Portuguese', 'LightPortuguese',\n * 'MinimalPortuguese', 'PortugueseRslp', 'Romanian', 'Russian', 'LightRussian', 'Spanish',\n * 'LightSpanish', 'Swedish', 'LightSwedish', 'Turkish'\n * @readonly\n * @enum {string}\n */\nexport type StemmerTokenFilterLanguage = 'arabic' | 'armenian' | 'basque' | 'brazilian' | 'bulgarian' | 'catalan' | 'czech' | 'danish' | 'dutch' | 'dutchKp' | 'english' | 'lightEnglish' | 'minimalEnglish' | 'possessiveEnglish' | 'porter2' | 'lovins' | 'finnish' | 'lightFinnish' | 'french' | 'lightFrench' | 'minimalFrench' | 'galician' | 'minimalGalician' | 'german' | 'german2' | 'lightGerman' | 'minimalGerman' | 'greek' | 'hindi' | 'hungarian' | 'lightHungarian' | 'indonesian' | 'irish' | 'italian' | 'lightItalian' | 'sorani' | 'latvian' | 'norwegian' | 'lightNorwegian' | 'minimalNorwegian' | 'lightNynorsk' | 'minimalNynorsk' | 'portuguese' | 'lightPortuguese' | 'minimalPortuguese' | 'portugueseRslp' | 'romanian' | 'russian' | 'lightRussian' | 'spanish' | 'lightSpanish' | 'swedish' | 'lightSwedish' | 'turkish';\n\n/**\n * Defines values for StopwordsList.\n * Possible values include: 'Arabic', 'Armenian', 'Basque', 'Brazilian', 'Bulgarian', 'Catalan',\n * 'Czech', 'Danish', 'Dutch', 'English', 'Finnish', 'French', 'Galician', 'German', 'Greek',\n * 'Hindi', 'Hungarian', 'Indonesian', 'Irish', 'Italian', 'Latvian', 'Norwegian', 'Persian',\n * 'Portuguese', 'Romanian', 'Russian', 'Sorani', 'Spanish', 'Swedish', 'Thai', 'Turkish'\n * @readonly\n * @enum {string}\n */\nexport type StopwordsList = 'arabic' | 'armenian' | 'basque' | 'brazilian' | 'bulgarian' | 'catalan' | 'czech' | 'danish' | 'dutch' | 'english' | 'finnish' | 'french' | 'galician' | 'german' | 'greek' | 'hindi' | 'hungarian' | 'indonesian' | 'irish' | 'italian' | 'latvian' | 'norwegian' | 'persian' | 'portuguese' | 'romanian' | 'russian' | 'sorani' | 'spanish' | 'swedish' | 'thai' | 'turkish';\n\n/**\n * Defines values for SearchIndexerDataSourceType.\n * Possible values include: 'AzureSql', 'CosmosDb', 'AzureBlob', 'AzureTable', 'MySql'\n * @readonly\n * @enum {string}\n */\nexport type SearchIndexerDataSourceType = 'azuresql' | 'cosmosdb' | 'azureblob' | 'azuretable' | 'mysql';\n\n/**\n * Defines values for IndexerExecutionStatus.\n * Possible values include: 'TransientFailure', 'Success', 'InProgress', 'Reset'\n * @readonly\n * @enum {string}\n */\nexport type IndexerExecutionStatus = 'transientFailure' | 'success' | 'inProgress' | 'reset';\n\n/**\n * Defines values for IndexerStatus.\n * Possible values include: 'Unknown', 'Error', 'Running'\n * @readonly\n * @enum {string}\n */\nexport type IndexerStatus = 'unknown' | 'error' | 'running';\n\n/**\n * Defines values for ScoringFunctionInterpolation.\n * Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'\n * @readonly\n * @enum {string}\n */\nexport type ScoringFunctionInterpolation = 'linear' | 'constant' | 'quadratic' | 'logarithmic';\n\n/**\n * Defines values for ScoringFunctionAggregation.\n * Possible values include: 'Sum', 'Average', 'Minimum', 'Maximum', 'FirstMatching'\n * @readonly\n * @enum {string}\n */\nexport type ScoringFunctionAggregation = 'sum' | 'average' | 'minimum' | 'maximum' | 'firstMatching';\n\n/**\n * Defines values for KeyPhraseExtractionSkillLanguage.\n * Possible values include: 'da', 'nl', 'en', 'fi', 'fr', 'de', 'it', 'ja', 'ko', 'no', 'pl',\n * 'pt-PT', 'pt-BR', 'ru', 'es', 'sv'\n * @readonly\n * @enum {string}\n */\nexport type KeyPhraseExtractionSkillLanguage = 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'it' | 'ja' | 'ko' | 'no' | 'pl' | 'pt-PT' | 'pt-BR' | 'ru' | 'es' | 'sv';\n\n/**\n * Defines values for OcrSkillLanguage.\n * Possible values include: 'zh-Hans', 'zh-Hant', 'cs', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el',\n * 'hu', 'it', 'ja', 'ko', 'nb', 'pl', 'pt', 'ru', 'es', 'sv', 'tr', 'ar', 'ro', 'sr-Cyrl',\n * 'sr-Latn', 'sk'\n * @readonly\n * @enum {string}\n */\nexport type OcrSkillLanguage = 'zh-Hans' | 'zh-Hant' | 'cs' | 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'hu' | 'it' | 'ja' | 'ko' | 'nb' | 'pl' | 'pt' | 'ru' | 'es' | 'sv' | 'tr' | 'ar' | 'ro' | 'sr-Cyrl' | 'sr-Latn' | 'sk';\n\n/**\n * Defines values for ImageAnalysisSkillLanguage.\n * Possible values include: 'en', 'es', 'ja', 'pt', 'zh'\n * @readonly\n * @enum {string}\n */\nexport type ImageAnalysisSkillLanguage = 'en' | 'es' | 'ja' | 'pt' | 'zh';\n\n/**\n * Defines values for VisualFeature.\n * Possible values include: 'Adult', 'Brands', 'Categories', 'Description', 'Faces', 'Objects',\n * 'Tags'\n * @readonly\n * @enum {string}\n */\nexport type VisualFeature = 'adult' | 'brands' | 'categories' | 'description' | 'faces' | 'objects' | 'tags';\n\n/**\n * Defines values for ImageDetail.\n * Possible values include: 'Celebrities', 'Landmarks'\n * @readonly\n * @enum {string}\n */\nexport type ImageDetail = 'celebrities' | 'landmarks';\n\n/**\n * Defines values for EntityCategory.\n * Possible values include: 'Location', 'Organization', 'Person', 'Quantity', 'Datetime', 'Url',\n * 'Email'\n * @readonly\n * @enum {string}\n */\nexport type EntityCategory = 'location' | 'organization' | 'person' | 'quantity' | 'datetime' | 'url' | 'email';\n\n/**\n * Defines values for EntityRecognitionSkillLanguage.\n * Possible values include: 'ar', 'cs', 'zh-Hans', 'zh-Hant', 'da', 'nl', 'en', 'fi', 'fr', 'de',\n * 'el', 'hu', 'it', 'ja', 'ko', 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv', 'tr'\n * @readonly\n * @enum {string}\n */\nexport type EntityRecognitionSkillLanguage = 'ar' | 'cs' | 'zh-Hans' | 'zh-Hant' | 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'hu' | 'it' | 'ja' | 'ko' | 'no' | 'pl' | 'pt-PT' | 'pt-BR' | 'ru' | 'es' | 'sv' | 'tr';\n\n/**\n * Defines values for SentimentSkillLanguage.\n * Possible values include: 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'it', 'no', 'pl', 'pt-PT',\n * 'ru', 'es', 'sv', 'tr'\n * @readonly\n * @enum {string}\n */\nexport type SentimentSkillLanguage = 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'it' | 'no' | 'pl' | 'pt-PT' | 'ru' | 'es' | 'sv' | 'tr';\n\n/**\n * Defines values for SplitSkillLanguage.\n * Possible values include: 'da', 'de', 'en', 'es', 'fi', 'fr', 'it', 'ko', 'pt'\n * @readonly\n * @enum {string}\n */\nexport type SplitSkillLanguage = 'da' | 'de' | 'en' | 'es' | 'fi' | 'fr' | 'it' | 'ko' | 'pt';\n\n/**\n * Defines values for TextSplitMode.\n * Possible values include: 'Pages', 'Sentences'\n * @readonly\n * @enum {string}\n */\nexport type TextSplitMode = 'pages' | 'sentences';\n\n/**\n * Defines values for TextTranslationSkillLanguage.\n * Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca', 'zh-Hans', 'zh-Hant', 'hr',\n * 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el', 'ht', 'he', 'hi', 'mww',\n * 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg', 'ms', 'mt', 'nb', 'fa', 'pl',\n * 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl', 'es', 'sv', 'ty', 'ta', 'te',\n * 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'\n * @readonly\n * @enum {string}\n */\nexport type TextTranslationSkillLanguage = 'af' | 'ar' | 'bn' | 'bs' | 'bg' | 'yue' | 'ca' | 'zh-Hans' | 'zh-Hant' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fj' | 'fil' | 'fi' | 'fr' | 'de' | 'el' | 'ht' | 'he' | 'hi' | 'mww' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'sw' | 'tlh' | 'ko' | 'lv' | 'lt' | 'mg' | 'ms' | 'mt' | 'nb' | 'fa' | 'pl' | 'pt' | 'otq' | 'ro' | 'ru' | 'sm' | 'sr-Cyrl' | 'sr-Latn' | 'sk' | 'sl' | 'es' | 'sv' | 'ty' | 'ta' | 'te' | 'th' | 'to' | 'tr' | 'uk' | 'ur' | 'vi' | 'cy' | 'yua';\n\n/**\n * Contains response data for the createOrUpdate operation.\n */\nexport type DataSourcesCreateOrUpdateResponse = SearchIndexerDataSource & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexerDataSource;\n    };\n};\n\n/**\n * Contains response data for the get operation.\n */\nexport type DataSourcesGetResponse = SearchIndexerDataSource & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexerDataSource;\n    };\n};\n\n/**\n * Contains response data for the list operation.\n */\nexport type DataSourcesListResponse = ListDataSourcesResult & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: ListDataSourcesResult;\n    };\n};\n\n/**\n * Contains response data for the create operation.\n */\nexport type DataSourcesCreateResponse = SearchIndexerDataSource & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexerDataSource;\n    };\n};\n\n/**\n * Contains response data for the createOrUpdate operation.\n */\nexport type IndexersCreateOrUpdateResponse = SearchIndexer & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexer;\n    };\n};\n\n/**\n * Contains response data for the get operation.\n */\nexport type IndexersGetResponse = SearchIndexer & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexer;\n    };\n};\n\n/**\n * Contains response data for the list operation.\n */\nexport type IndexersListResponse = ListIndexersResult & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: ListIndexersResult;\n    };\n};\n\n/**\n * Contains response data for the create operation.\n */\nexport type IndexersCreateResponse = SearchIndexer & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexer;\n    };\n};\n\n/**\n * Contains response data for the getStatus operation.\n */\nexport type IndexersGetStatusResponse = SearchIndexerStatus & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexerStatus;\n    };\n};\n\n/**\n * Contains response data for the createOrUpdate operation.\n */\nexport type SkillsetsCreateOrUpdateResponse = SearchIndexerSkillset & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexerSkillset;\n    };\n};\n\n/**\n * Contains response data for the get operation.\n */\nexport type SkillsetsGetResponse = SearchIndexerSkillset & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexerSkillset;\n    };\n};\n\n/**\n * Contains response data for the list operation.\n */\nexport type SkillsetsListResponse = ListSkillsetsResult & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: ListSkillsetsResult;\n    };\n};\n\n/**\n * Contains response data for the create operation.\n */\nexport type SkillsetsCreateResponse = SearchIndexerSkillset & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndexerSkillset;\n    };\n};\n\n/**\n * Contains response data for the createOrUpdate operation.\n */\nexport type SynonymMapsCreateOrUpdateResponse = SynonymMap & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SynonymMap;\n    };\n};\n\n/**\n * Contains response data for the get operation.\n */\nexport type SynonymMapsGetResponse = SynonymMap & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SynonymMap;\n    };\n};\n\n/**\n * Contains response data for the list operation.\n */\nexport type SynonymMapsListResponse = ListSynonymMapsResult & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: ListSynonymMapsResult;\n    };\n};\n\n/**\n * Contains response data for the create operation.\n */\nexport type SynonymMapsCreateResponse = SynonymMap & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SynonymMap;\n    };\n};\n\n/**\n * Contains response data for the create operation.\n */\nexport type IndexesCreateResponse = SearchIndex & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndex;\n    };\n};\n\n/**\n * Contains response data for the list operation.\n */\nexport type IndexesListResponse = ListIndexesResult & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: ListIndexesResult;\n    };\n};\n\n/**\n * Contains response data for the createOrUpdate operation.\n */\nexport type IndexesCreateOrUpdateResponse = SearchIndex & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndex;\n    };\n};\n\n/**\n * Contains response data for the get operation.\n */\nexport type IndexesGetResponse = SearchIndex & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: SearchIndex;\n    };\n};\n\n/**\n * Contains response data for the getStatistics operation.\n */\nexport type IndexesGetStatisticsResponse = GetIndexStatisticsResult & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: GetIndexStatisticsResult;\n    };\n};\n\n/**\n * Contains response data for the analyze operation.\n */\nexport type IndexesAnalyzeResponse = AnalyzeResult & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: AnalyzeResult;\n    };\n};\n\n/**\n * Contains response data for the getServiceStatistics operation.\n */\nexport type GetServiceStatisticsResponse = ServiceStatistics & {\n  /**\n   * The underlying HTTP response.\n   */\n  _response: coreHttp.HttpResponse & {\n      /**\n       * The response body as text (string format)\n       */\n      bodyAsText: string;\n\n      /**\n       * The response body as parsed JSON or XML\n       */\n      parsedBody: ServiceStatistics;\n    };\n};\n"]}