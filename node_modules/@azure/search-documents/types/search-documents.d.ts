/// <reference lib="esnext.asynciterable" />
import { AzureKeyCredential } from '@azure/core-auth';
import { KeyCredential } from '@azure/core-auth';
import { OperationOptions } from '@azure/core-http';
import { PagedAsyncIterableIterator } from '@azure/core-paging';
import { PipelineOptions } from '@azure/core-http';

/**
 * Information about a token returned by an analyzer.
 */
export declare interface AnalyzedTokenInfo {
    /**
     * The token returned by the analyzer.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly token: string;
    /**
     * The index of the first character of the token in the input text.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly startOffset: number;
    /**
     * The index of the last character of the token in the input text.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly endOffset: number;
    /**
     * The position of the token in the input text relative to other tokens. The first token in the
     * input text has position 0, the next has position 1, and so on. Depending on the analyzer used,
     * some tokens might have the same position, for example if they are synonyms of each other.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly position: number;
}

/**
 * Specifies some text and analysis components used to break that text into tokens.
 */
export declare interface AnalyzeRequest {
    /**
     * The text to break into tokens.
     */
    text: string;
    /**
     * The name of the analyzer to use to break the given text. If this parameter is not specified,
     * you must specify a tokenizer instead. The tokenizer and analyzer parameters are mutually
     * exclusive. KnownAnalyzerNames is an enum containing known values.
     * NOTE: Either analyzerName or tokenizerName is required in an AnalyzeRequest.
     */
    analyzerName?: string;
    /**
     * The name of the tokenizer to use to break the given text. If this parameter is not specified,
     * you must specify an analyzer instead. The tokenizer and analyzer parameters are mutually
     * exclusive. KnownTokenizerNames is an enum containing known values.
     * NOTE: Either analyzerName or tokenizerName is required in an AnalyzeRequest.
     */
    tokenizerName?: string;
    /**
     * An optional list of token filters to use when breaking the given text. This parameter can only
     * be set when using the tokenizer parameter.
     */
    tokenFilters?: string[];
    /**
     * An optional list of character filters to use when breaking the given text. This parameter can
     * only be set when using the tokenizer parameter.
     */
    charFilters?: string[];
}

/**
 * The result of testing an analyzer on text.
 */
export declare interface AnalyzeResult {
    /**
     * The list of tokens returned by the analyzer specified in the request.
     */
    tokens: AnalyzedTokenInfo[];
}

/**
 * Options for analyze text operation.
 */
export declare type AnalyzeTextOptions = OperationOptions & AnalyzeRequest;

/**
 * Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127
 * ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such
 * equivalents exist. This token filter is implemented using Apache Lucene.
 */
export declare interface AsciiFoldingTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.AsciiFoldingTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A value indicating whether the original token will be kept. Default is false. Default value:
     * false.
     */
    preserveOriginal?: boolean;
}

/**
 * The result of Autocomplete requests.
 */
export declare interface AutocompleteItem {
    /**
     * The completed term.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly text: string;
    /**
     * The query along with the completed term.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly queryPlusText: string;
}

/**
 * Defines values for AutocompleteMode.
 * Possible values include: 'OneTerm', 'TwoTerms', 'OneTermWithContext'
 * @readonly
 * @enum {string}
 */
export declare type AutocompleteMode = 'oneTerm' | 'twoTerms' | 'oneTermWithContext';

/**
 * Options for retrieving completion text for a partial searchText.
 */
export declare type AutocompleteOptions<Fields> = OperationOptions & AutocompleteRequest<Fields>;

/**
 * Parameters for fuzzy matching, and other autocomplete query behaviors.
 */
export declare interface AutocompleteRequest<Fields> {
    /**
     * Specifies the mode for Autocomplete. The default is 'oneTerm'. Use 'twoTerms' to get shingles
     * and 'oneTermWithContext' to use the current context while producing auto-completed terms.
     * Possible values include: 'oneTerm', 'twoTerms', 'oneTermWithContext'
     */
    autocompleteMode?: AutocompleteMode;
    /**
     * An OData expression that filters the documents used to produce completed terms for the
     * Autocomplete result.
     */
    filter?: string;
    /**
     * A value indicating whether to use fuzzy matching for the autocomplete query. Default is false.
     * When set to true, the query will autocomplete terms even if there's a substituted or missing
     * character in the search text. While this provides a better experience in some scenarios, it
     * comes at a performance cost as fuzzy autocomplete queries are slower and consume more
     * resources.
     */
    useFuzzyMatching?: boolean;
    /**
     * A string tag that is appended to hit highlights. Must be set with highlightPreTag. If omitted,
     * hit highlighting is disabled.
     */
    highlightPostTag?: string;
    /**
     * A string tag that is prepended to hit highlights. Must be set with highlightPostTag. If
     * omitted, hit highlighting is disabled.
     */
    highlightPreTag?: string;
    /**
     * A number between 0 and 100 indicating the percentage of the index that must be covered by an
     * autocomplete query in order for the query to be reported as a success. This parameter can be
     * useful for ensuring search availability even for services with only one replica. The default
     * is 80.
     */
    minimumCoverage?: number;
    /**
     * The comma-separated list of field names to consider when querying for auto-completed terms.
     * Target fields must be included in the specified suggester.
     */
    searchFields?: Fields[];
    /**
     * The number of auto-completed terms to retrieve. This must be a value between 1 and 100. The
     * default is 5.
     */
    top?: number;
}

/**
 * The result of Autocomplete query.
 */
export declare interface AutocompleteResult {
    /**
     * A value indicating the percentage of the index that was considered by the autocomplete
     * request, or null if minimumCoverage was not specified in the request.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly coverage?: number;
    /**
     * The list of returned Autocompleted items.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly results: AutocompleteItem[];
}

/**
 * Credentials of a registered application created for your search service, used for authenticated
 * access to the encryption keys stored in Azure Key Vault.
 */
export declare interface AzureActiveDirectoryApplicationCredentials {
    /**
     * An AAD Application ID that was granted the required access permissions to the Azure Key Vault
     * that is to be used when encrypting your data at rest. The Application ID should not be
     * confused with the Object ID for your AAD Application.
     */
    applicationId: string;
    /**
     * The authentication key of the specified AAD application.
     */
    applicationSecret?: string;
}
export { AzureKeyCredential }

/**
 * Ranking function based on the Okapi BM25 similarity algorithm. BM25 is a TF-IDF-like algorithm
 * that includes length normalization (controlled by the 'b' parameter) as well as term frequency
 * saturation (controlled by the 'k1' parameter).
 */
export declare interface BM25Similarity {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.BM25Similarity";
    /**
     * This property controls the scaling function between the term frequency of each matching terms
     * and the final relevance score of a document-query pair. By default, a value of 1.2 is used. A
     * value of 0.0 means the score does not scale with an increase in term frequency.
     */
    k1?: number;
    /**
     * This property controls how the length of a document affects the relevance score. By default, a
     * value of 0.75 is used. A value of 0.0 means no length normalization is applied, while a value
     * of 1.0 means the score is fully normalized by the length of the document.
     */
    b?: number;
}

/**
 * Contains the possible cases for CharFilter.
 */
export declare type CharFilter = MappingCharFilter | PatternReplaceCharFilter;

/**
 * Forms bigrams of CJK terms that are generated from the standard tokenizer. This token filter is
 * implemented using Apache Lucene.
 */
export declare interface CjkBigramTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.CjkBigramTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The scripts to ignore.
     */
    ignoreScripts?: CjkBigramTokenFilterScripts[];
    /**
     * A value indicating whether to output both unigrams and bigrams (if true), or just bigrams (if
     * false). Default is false. Default value: false.
     */
    outputUnigrams?: boolean;
}

/**
 * Defines values for CjkBigramTokenFilterScripts.
 * Possible values include: 'Han', 'Hiragana', 'Katakana', 'Hangul'
 * @readonly
 * @enum {string}
 */
export declare type CjkBigramTokenFilterScripts = 'han' | 'hiragana' | 'katakana' | 'hangul';

/**
 * Legacy similarity algorithm which uses the Lucene TFIDFSimilarity implementation of TF-IDF. This
 * variation of TF-IDF introduces static document length normalization as well as coordinating
 * factors that penalize documents that only partially match the searched queries.
 */
export declare interface ClassicSimilarity {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.ClassicSimilarity";
}

/**
 * Grammar-based tokenizer that is suitable for processing most European-language documents. This
 * tokenizer is implemented using Apache Lucene.
 */
export declare interface ClassicTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.ClassicTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The
     * maximum token length that can be used is 300 characters. Default value: 255.
     */
    maxTokenLength?: number;
}

/**
 * Contains the possible cases for CognitiveServicesAccount.
 */
export declare type CognitiveServicesAccount = DefaultCognitiveServicesAccount | CognitiveServicesAccountKey;

/**
 * A cognitive service resource provisioned with a key that is attached to a skillset.
 */
export declare interface CognitiveServicesAccountKey {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.CognitiveServicesByKey";
    /**
     * Description of the cognitive service resource attached to a skillset.
     */
    description?: string;
    /**
     * The key used to provision the cognitive service resource attached to a skillset.
     */
    key: string;
}

/**
 * Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed
 * too, with bigrams overlaid. This token filter is implemented using Apache Lucene.
 */
export declare interface CommonGramTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.CommonGramTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The set of common words.
     */
    commonWords: string[];
    /**
     * A value indicating whether common words matching will be case insensitive. Default is false.
     * Default value: false.
     */
    ignoreCase?: boolean;
    /**
     * A value that indicates whether the token filter is in query mode. When in query mode, the
     * token filter generates bigrams and then removes common words and single terms followed by a
     * common word. Default is false. Default value: false.
     */
    useQueryMode?: boolean;
}

/**
 * Defines values for ComplexDataType.
 * Possible values include: 'Edm.ComplexType', 'Collection(Edm.ComplexType)'
 * @readonly
 * @enum {string}
 */
export declare type ComplexDataType = "Edm.ComplexType" | "Collection(Edm.ComplexType)";

/**
 * Represents a field in an index definition, which describes the name, data type, and search
 * behavior of a field.
 */
export declare interface ComplexField {
    /**
     * The name of the field, which must be unique within the fields collection of the index or
     * parent field.
     */
    name: string;
    /**
     * The data type of the field.
     * Possible values include: 'Edm.ComplexType','Collection(Edm.ComplexType)'
     */
    type: ComplexDataType;
    /**
     * A list of sub-fields.
     */
    fields: SearchField[];
}

/**
 * A skill that enables scenarios that require a Boolean operation to determine the data to assign
 * to an output.
 */
export declare interface ConditionalSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Util.ConditionalSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
}

/**
 * Defines options to control Cross-Origin Resource Sharing (CORS) for an index.
 */
export declare interface CorsOptions {
    /**
     * The list of origins from which JavaScript code will be granted access to your index. Can
     * contain a list of hosts of the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a
     * single '*' to allow all origins (not recommended).
     */
    allowedOrigins: string[];
    /**
     * The duration for which browsers should cache CORS preflight responses. Defaults to 5 minutes.
     */
    maxAgeInSeconds?: number;
}

/**
 * Options for performing the count operation on the index.
 */
export declare type CountDocumentsOptions = OperationOptions;

/**
 * Options for create datasource operation.
 */
export declare type CreateDataSourceConnectionOptions = OperationOptions;

/**
 * Options for create indexer operation.
 */
export declare type CreateIndexerOptions = OperationOptions;

/**
 * Options for create index operation.
 */
export declare type CreateIndexOptions = OperationOptions;

/**
 * Options for create/update datasource operation.
 */
export declare interface CreateorUpdateDataSourceConnectionOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for create/update indexer operation.
 */
export declare interface CreateorUpdateIndexerOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for create/update index operation.
 */
export declare interface CreateOrUpdateIndexOptions extends OperationOptions {
    /**
     * Allows new analyzers, tokenizers, token filters, or char filters to be added to an index by
     * taking the index offline for at least a few seconds. This temporarily causes indexing and
     * query requests to fail. Performance and write availability of the index can be impaired for
     * several minutes after the index is updated, or longer for very large indexes.
     */
    allowIndexDowntime?: boolean;
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for create/update skillset operation.
 */
export declare interface CreateOrUpdateSkillsetOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for create/update synonymmap operation.
 */
export declare interface CreateOrUpdateSynonymMapOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for create skillset operation.
 */
export declare type CreateSkillsetOptions = OperationOptions;

/**
 * Options for create synonymmap operation.
 */
export declare type CreateSynonymMapOptions = OperationOptions;

/**
 * Allows you to take control over the process of converting text into indexable/searchable tokens.
 * It's a user-defined configuration consisting of a single predefined tokenizer and one or more
 * filters. The tokenizer is responsible for breaking text into tokens, and the filters for
 * modifying tokens emitted by the tokenizer.
 */
export declare interface CustomAnalyzer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.CustomAnalyzer";
    /**
     * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
     * can only start and end with alphanumeric characters, and is limited to 128 characters.
     */
    name: string;
    /**
     * The name of the tokenizer to use to divide continuous text into a sequence of tokens, such as
     * breaking a sentence into words. KnownTokenizerNames is an enum containing known values.
     */
    tokenizerName: string;
    /**
     * A list of token filters used to filter out or modify the tokens generated by a tokenizer. For
     * example, you can specify a lowercase filter that converts all characters to lowercase. The
     * filters are run in the order in which they are listed.
     */
    tokenFilters?: string[];
    /**
     * A list of character filters used to prepare input text before it is processed by the
     * tokenizer. For instance, they can replace certain characters or symbols. The filters are run
     * in the order in which they are listed.
     */
    charFilters?: string[];
}

/**
 * Contains the possible cases for DataChangeDetectionPolicy.
 */
export declare type DataChangeDetectionPolicy = HighWaterMarkChangeDetectionPolicy | SqlIntegratedChangeTrackingPolicy;

/**
 * Contains the possible cases for DataDeletionDetectionPolicy.
 */
export declare type DataDeletionDetectionPolicy = SoftDeleteColumnDeletionDetectionPolicy;

/**
 * An empty object that represents the default cognitive service resource for a skillset.
 */
export declare interface DefaultCognitiveServicesAccount {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.DefaultCognitiveServices";
    /**
     * Description of the cognitive service resource attached to a skillset.
     */
    description?: string;
}

/**
 * Options for delete datasource operation.
 */
export declare interface DeleteDataSourceConnectionOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for the delete documents operation.
 */
export declare type DeleteDocumentsOptions = IndexDocumentsOptions;

/**
 * Options for delete indexer operation.
 */
export declare interface DeleteIndexerOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for delete index operation.
 */
export declare interface DeleteIndexOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for delete skillset operaion.
 */
export declare interface DeleteSkillsetOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Options for delete synonymmap operation.
 */
export declare interface DeleteSynonymMapOptions extends OperationOptions {
    /**
     * If set to true, Resource will be deleted only if the etag matches.
     */
    onlyIfUnchanged?: boolean;
}

/**
 * Decomposes compound words found in many Germanic languages. This token filter is implemented
 * using Apache Lucene.
 */
export declare interface DictionaryDecompounderTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.DictionaryDecompounderTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The list of words to match against.
     */
    wordList: string[];
    /**
     * The minimum word size. Only words longer than this get processed. Default is 5. Maximum is
     * 300. Default value: 5.
     */
    minWordSize?: number;
    /**
     * The minimum subword size. Only subwords longer than this are outputted. Default is 2. Maximum
     * is 300. Default value: 2.
     */
    minSubwordSize?: number;
    /**
     * The maximum subword size. Only subwords shorter than this are outputted. Default is 15.
     * Maximum is 300. Default value: 15.
     */
    maxSubwordSize?: number;
    /**
     * A value indicating whether to add only the longest matching subword to the output. Default is
     * false. Default value: false.
     */
    onlyLongestMatch?: boolean;
}

/**
 * Defines a function that boosts scores based on distance from a geographic location.
 */
export declare interface DistanceScoringFunction {
    /**
     * Polymorphic Discriminator
     */
    type: "distance";
    /**
     * The name of the field used as input to the scoring function.
     */
    fieldName: string;
    /**
     * A multiplier for the raw score. Must be a positive number not equal to 1.0.
     */
    boost: number;
    /**
     * A value indicating how boosting will be interpolated across document scores; defaults to
     * "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
     */
    interpolation?: ScoringFunctionInterpolation;
    /**
     * Parameter values for the distance scoring function.
     */
    parameters: DistanceScoringParameters;
}

/**
 * Provides parameter values to a distance scoring function.
 */
export declare interface DistanceScoringParameters {
    /**
     * The name of the parameter passed in search queries to specify the reference location.
     */
    referencePointParameter: string;
    /**
     * The distance in kilometers from the reference location where the boosting range ends.
     */
    boostingDistance: number;
}

/**
 * Generates n-grams of the given size(s) starting from the front or the back of an input token.
 * This token filter is implemented using Apache Lucene.
 */
export declare interface EdgeNGramTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.EdgeNGramTokenFilterV2" | "#Microsoft.Azure.Search.EdgeNGramTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of
     * maxGram. Default value: 1.
     */
    minGram?: number;
    /**
     * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.
     */
    maxGram?: number;
    /**
     * Specifies which side of the input the n-gram should be generated from. Default is "front".
     * Possible values include: 'Front', 'Back'
     */
    side?: EdgeNGramTokenFilterSide;
}

/**
 * Defines values for EdgeNGramTokenFilterSide.
 * Possible values include: 'Front', 'Back'
 * @readonly
 * @enum {string}
 */
export declare type EdgeNGramTokenFilterSide = 'front' | 'back';

/**
 * Tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is
 * implemented using Apache Lucene.
 */
export declare interface EdgeNGramTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.EdgeNGramTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of
     * maxGram. Default value: 1.
     */
    minGram?: number;
    /**
     * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.
     */
    maxGram?: number;
    /**
     * Character classes to keep in the tokens.
     */
    tokenChars?: TokenCharacterKind[];
}

/**
 * Removes elisions. For example, "l'avion" (the plane) will be converted to "avion" (plane). This
 * token filter is implemented using Apache Lucene.
 */
export declare interface ElisionTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.ElisionTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The set of articles to remove.
     */
    articles?: string[];
}

/**
 * Defines values for EntityCategory.
 * Possible values include: 'Location', 'Organization', 'Person', 'Quantity', 'Datetime', 'Url',
 * 'Email'
 * @readonly
 * @enum {string}
 */
export declare type EntityCategory = 'location' | 'organization' | 'person' | 'quantity' | 'datetime' | 'url' | 'email';

/**
 * Text analytics entity recognition.
 */
export declare interface EntityRecognitionSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Text.EntityRecognitionSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * A list of entity categories that should be extracted.
     */
    categories?: EntityCategory[];
    /**
     * A value indicating which language code to use. Default is en. Possible values include: 'ar',
     * 'cs', 'zh-Hans', 'zh-Hant', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'hu', 'it', 'ja', 'ko',
     * 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv', 'tr'
     */
    defaultLanguageCode?: EntityRecognitionSkillLanguage;
    /**
     * Determines whether or not to include entities which are well known but don't conform to a
     * pre-defined type. If this configuration is not set (default), set to null or set to false,
     * entities which don't conform to one of the pre-defined types will not be surfaced.
     */
    includeTypelessEntities?: boolean;
    /**
     * A value between 0 and 1 that be used to only include entities whose confidence score is
     * greater than the value specified. If not set (default), or if explicitly set to null, all
     * entities will be included.
     */
    minimumPrecision?: number;
}

/**
 * Defines values for EntityRecognitionSkillLanguage.
 * Possible values include: 'ar', 'cs', 'zh-Hans', 'zh-Hant', 'da', 'nl', 'en', 'fi', 'fr', 'de',
 * 'el', 'hu', 'it', 'ja', 'ko', 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv', 'tr'
 * @readonly
 * @enum {string}
 */
export declare type EntityRecognitionSkillLanguage = 'ar' | 'cs' | 'zh-Hans' | 'zh-Hant' | 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'hu' | 'it' | 'ja' | 'ko' | 'no' | 'pl' | 'pt-PT' | 'pt-BR' | 'ru' | 'es' | 'sv' | 'tr';

/**
 * A single bucket of a facet query result. Reports the number of documents with a field value
 * falling within a particular range or having a particular value or interval.
 */
export declare interface FacetResult {
    /**
     * The approximate count of documents falling within the bucket described by this facet.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly count?: number;
    /**
     * Describes unknown properties. The value of an unknown property can be of "any" type.
     */
    [property: string]: any;
}

/**
 * Defines a mapping between a field in a data source and a target field in an index.
 */
export declare interface FieldMapping {
    /**
     * The name of the field in the data source.
     */
    sourceFieldName: string;
    /**
     * The name of the target field in the index. Same as the source field name by default.
     */
    targetFieldName?: string;
    /**
     * A function to apply to each source field value before indexing.
     */
    mappingFunction?: FieldMappingFunction;
}

/**
 * Represents a function that transforms a value from a data source before indexing.
 */
export declare interface FieldMappingFunction {
    /**
     * The name of the field mapping function.
     */
    name: string;
    /**
     * A dictionary of parameter name/value pairs to pass to the function. Each value must be of a
     * primitive type.
     */
    parameters?: {
        [propertyName: string]: any;
    };
}

/**
 * Defines a function that boosts scores based on the value of a date-time field.
 */
export declare interface FreshnessScoringFunction {
    /**
     * Polymorphic Discriminator
     */
    type: "freshness";
    /**
     * The name of the field used as input to the scoring function.
     */
    fieldName: string;
    /**
     * A multiplier for the raw score. Must be a positive number not equal to 1.0.
     */
    boost: number;
    /**
     * A value indicating how boosting will be interpolated across document scores; defaults to
     * "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
     */
    interpolation?: ScoringFunctionInterpolation;
    /**
     * Parameter values for the freshness scoring function.
     */
    parameters: FreshnessScoringParameters;
}

/**
 * Provides parameter values to a freshness scoring function.
 */
export declare interface FreshnessScoringParameters {
    /**
     * The expiration period after which boosting will stop for a particular document.
     */
    boostingDuration: string;
}

/**
 * Represents a geographic point in global coordinates.
 */
export declare class GeographyPoint {
    /**
     * The latitude in decimal.
     */
    latitude: number;
    /**
     * The longitude in decimal.
     */
    longitude: number;
    /**
     * Constructs a new instance of GeographyPoint given
     * the specified coordinates.
     * @param latitude latitude value in decimal
     * @param longitude longitude value in decimal
     */
    constructor(latitude: number, longitude: number);
    /**
     * Used to serialize to a GeoJSON Point.
     */
    toJSON(): object;
}

/**
 * Options for get datasource operation.
 */
export declare type GetDataSourceConnectionOptions = OperationOptions;

/**
 * Options for retrieving a single document.
 */
export declare interface GetDocumentOptions<Fields> extends OperationOptions {
    /**
     * List of field names to retrieve for the document; Any field not retrieved will be missing from
     * the returned document.
     */
    selectedFields?: Fields[];
}

/**
 * Options for get indexer operation.
 */
export declare type GetIndexerOptions = OperationOptions;

/**
 * Options for get indexer status operation.
 */
export declare type GetIndexerStatusOptions = OperationOptions;

/**
 * Options for get index operation.
 */
export declare type GetIndexOptions = OperationOptions;

/**
 * Options for get index statistics operation.
 */
export declare type GetIndexStatisticsOptions = OperationOptions;

/**
 * Options for get service statistics operation.
 */
export declare type GetServiceStatisticsOptions = OperationOptions;

/**
 * Options for get skillset operation.
 */
export declare type GetSkillSetOptions = OperationOptions;

/**
 * Options for get synonymmaps operation.
 */
export declare type GetSynonymMapsOptions = OperationOptions;

/**
 * Defines a data change detection policy that captures changes based on the value of a high water
 * mark column.
 */
export declare interface HighWaterMarkChangeDetectionPolicy {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.HighWaterMarkChangeDetectionPolicy";
    /**
     * The name of the high water mark column.
     */
    highWaterMarkColumnName: string;
}

/**
 * A skill that analyzes image files. It extracts a rich set of visual features based on the image
 * content.
 */
export declare interface ImageAnalysisSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Vision.ImageAnalysisSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * A value indicating which language code to use. Default is en. Possible values include: 'en',
     * 'es', 'ja', 'pt', 'zh'
     */
    defaultLanguageCode?: ImageAnalysisSkillLanguage;
    /**
     * A list of visual features.
     */
    visualFeatures?: VisualFeature[];
    /**
     * A string indicating which domain-specific details to return.
     */
    details?: ImageDetail[];
}

/**
 * Defines values for ImageAnalysisSkillLanguage.
 * Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 * @readonly
 * @enum {string}
 */
export declare type ImageAnalysisSkillLanguage = 'en' | 'es' | 'ja' | 'pt' | 'zh';

/**
 * Defines values for ImageDetail.
 * Possible values include: 'Celebrities', 'Landmarks'
 * @readonly
 * @enum {string}
 */
export declare type ImageDetail = 'celebrities' | 'landmarks';

/**
 * Defines values for IndexActionType.
 * Possible values include: 'Upload', 'Merge', 'MergeOrUpload', 'Delete'
 * @readonly
 * @enum {string}
 */
export declare type IndexActionType = 'upload' | 'merge' | 'mergeOrUpload' | 'delete';

/**
 * Represents an index action that operates on a document.
 */
export declare type IndexDocumentsAction<T> = {
    /**
     * The operation to perform on a document in an indexing batch. Possible values include:
     * 'upload', 'merge', 'mergeOrUpload', 'delete'
     */
    __actionType: IndexActionType;
} & Partial<T>;

/**
 * Class used to perform batch operations
 * with multiple documents to the index.
 */
export declare class IndexDocumentsBatch<T> {
    /**
     * The set of actions taken in this batch.
     */
    readonly actions: IndexDocumentsAction<T>[];
    constructor(actions?: IndexDocumentsAction<T>[]);
    /**
     * Upload an array of documents to the index.
     * @param documents The documents to upload.
     */
    upload(documents: T[]): void;
    /**
     * Update a set of documents in the index.
     * For more details about how merging works, see https://docs.microsoft.com/en-us/rest/api/searchservice/AddUpdate-or-Delete-Documents
     * @param documents The updated documents.
     */
    merge(documents: T[]): void;
    /**
     * Update a set of documents in the index or uploads them if they don't exist.
     * For more details about how merging works, see https://docs.microsoft.com/en-us/rest/api/searchservice/AddUpdate-or-Delete-Documents
     * @param documents The new/updated documents.
     */
    mergeOrUpload(documents: T[]): void;
    /**
     * Delete a set of documents.
     * @param keyName The name of their primary key in the index.
     * @param keyValues The primary key values of documents to delete.
     */
    delete(keyName: keyof T, keyValues: string[]): void;
    /**
     * Delete a set of documents.
     * @param documents Documents to be deleted.
     */
    delete(documents: T[]): void;
}

/**
 * Options for the modify index batch operation.
 */
export declare interface IndexDocumentsOptions extends OperationOptions {
    /**
     * If true, will cause this operation to throw if any document operation
     * in the batch did not succeed.
     */
    throwOnAnyFailure?: boolean;
}

/**
 * Response containing the status of operations for all documents in the indexing request.
 */
export declare interface IndexDocumentsResult {
    /**
     * The list of status information for each document in the indexing request.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly results: IndexingResult[];
}

/**
 * Represents the result of an individual indexer execution.
 */
export declare interface IndexerExecutionResult {
    /**
     * The outcome of this indexer execution. Possible values include: 'TransientFailure', 'Success',
     * 'InProgress', 'Reset'
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly status: IndexerExecutionStatus;
    /**
     * The error message indicating the top-level error, if any.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly errorMessage?: string;
    /**
     * The start time of this indexer execution.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly startTime?: Date;
    /**
     * The end time of this indexer execution, if the execution has already completed.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly endTime?: Date;
    /**
     * The item-level indexing errors.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly errors: SearchIndexerError[];
    /**
     * The item-level indexing warnings.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly warnings: SearchIndexerWarning[];
    /**
     * The number of items that were processed during this indexer execution. This includes both
     * successfully processed items and items where indexing was attempted but failed.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly itemCount: number;
    /**
     * The number of items that failed to be indexed during this indexer execution.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly failedItemCount: number;
    /**
     * Change tracking state with which an indexer execution started.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly initialTrackingState?: string;
    /**
     * Change tracking state with which an indexer execution finished.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly finalTrackingState?: string;
}

/**
 * Defines values for IndexerExecutionStatus.
 * Possible values include: 'TransientFailure', 'Success', 'InProgress', 'Reset'
 * @readonly
 * @enum {string}
 */
export declare type IndexerExecutionStatus = 'transientFailure' | 'success' | 'inProgress' | 'reset';

/**
 * Defines values for IndexerStatus.
 * Possible values include: 'Unknown', 'Error', 'Running'
 * @readonly
 * @enum {string}
 */
export declare type IndexerStatus = 'unknown' | 'error' | 'running';

/**
 * Represents parameters for indexer execution.
 */
export declare interface IndexingParameters {
    /**
     * The number of items that are read from the data source and indexed as a single batch in order
     * to improve performance. The default depends on the data source type.
     */
    batchSize?: number;
    /**
     * The maximum number of items that can fail indexing for indexer execution to still be
     * considered successful. -1 means no limit. Default is 0. Default value: 0.
     */
    maxFailedItems?: number;
    /**
     * The maximum number of items in a single batch that can fail indexing for the batch to still be
     * considered successful. -1 means no limit. Default is 0. Default value: 0.
     */
    maxFailedItemsPerBatch?: number;
    /**
     * A dictionary of indexer-specific configuration properties. Each name is the name of a specific
     * property. Each value must be of a primitive type.
     */
    configuration?: {
        [propertyName: string]: any;
    };
}

/**
 * Status of an indexing operation for a single document.
 */
export declare interface IndexingResult {
    /**
     * The key of a document that was in the indexing request.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly key: string;
    /**
     * The error message explaining why the indexing operation failed for the document identified by
     * the key; null if indexing succeeded.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly errorMessage?: string;
    /**
     * A value indicating whether the indexing operation succeeded for the document identified by the
     * key.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly succeeded: boolean;
    /**
     * The status code of the indexing operation. Possible values include: 200 for a successful
     * update or delete, 201 for successful document creation, 400 for a malformed input document,
     * 404 for document not found, 409 for a version conflict, 422 when the index is temporarily
     * unavailable, or 503 for when the service is too busy.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly statusCode: number;
}

/**
 * Represents a schedule for indexer execution.
 */
export declare interface IndexingSchedule {
    /**
     * The interval of time between indexer executions.
     */
    interval: string;
    /**
     * The time when an indexer should start running.
     */
    startTime?: Date;
}

/**
 * An iterator for listing the indexes that exist in the Search service. Will make requests
 * as needed during iteration. Use .byPage() to make one request to the server
 * per iteration.
 */
export declare type IndexIterator = PagedAsyncIterableIterator<SearchIndex, SearchIndex[], {}>;

/**
 * An iterator for listing the indexes that exist in the Search service. Will make requests
 * as needed during iteration. Use .byPage() to make one request to the server
 * per iteration.
 */
export declare type IndexNameIterator = PagedAsyncIterableIterator<string, string[], {}>;

/**
 * Input field mapping for a skill.
 */
export declare interface InputFieldMappingEntry {
    /**
     * The name of the input.
     */
    name: string;
    /**
     * The source of the input.
     */
    source?: string;
    /**
     * The source context used for selecting recursive inputs.
     */
    sourceContext?: string;
    /**
     * The recursive inputs used when creating a complex type.
     */
    inputs?: InputFieldMappingEntry[];
}

/**
 * A token filter that only keeps tokens with text contained in a specified list of words. This
 * token filter is implemented using Apache Lucene.
 */
export declare interface KeepTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.KeepTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The list of words to keep.
     */
    keepWords: string[];
    /**
     * A value indicating whether to lower case all words first. Default is false. Default value:
     * false.
     */
    lowerCaseKeepWords?: boolean;
}

/**
 * A skill that uses text analytics for key phrase extraction.
 */
export declare interface KeyPhraseExtractionSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Text.KeyPhraseExtractionSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * A value indicating which language code to use. Default is en. Possible values include: 'da',
     * 'nl', 'en', 'fi', 'fr', 'de', 'it', 'ja', 'ko', 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv'
     */
    defaultLanguageCode?: KeyPhraseExtractionSkillLanguage;
    /**
     * A number indicating how many key phrases to return. If absent, all identified key phrases will
     * be returned.
     */
    maxKeyPhraseCount?: number;
}

/**
 * Defines values for KeyPhraseExtractionSkillLanguage.
 * Possible values include: 'da', 'nl', 'en', 'fi', 'fr', 'de', 'it', 'ja', 'ko', 'no', 'pl',
 * 'pt-PT', 'pt-BR', 'ru', 'es', 'sv'
 * @readonly
 * @enum {string}
 */
export declare type KeyPhraseExtractionSkillLanguage = 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'it' | 'ja' | 'ko' | 'no' | 'pl' | 'pt-PT' | 'pt-BR' | 'ru' | 'es' | 'sv';

/**
 * Marks terms as keywords. This token filter is implemented using Apache Lucene.
 */
export declare interface KeywordMarkerTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.KeywordMarkerTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A list of words to mark as keywords.
     */
    keywords: string[];
    /**
     * A value indicating whether to ignore case. If true, all words are converted to lower case
     * first. Default is false. Default value: false.
     */
    ignoreCase?: boolean;
}

/**
 * Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.
 */
export declare interface KeywordTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.KeywordTokenizerV2" | "#Microsoft.Azure.Search.KeywordTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The maximum token length. Default is 256. Tokens longer than the maximum length are split. The
     * maximum token length that can be used is 300 characters. Default value: 256.
     */
    maxTokenLength?: number;
}

/**
 * Defines values for AnalyzerName.
 * See https://docs.microsoft.com/rest/api/searchservice/Language-support
 * @readonly
 * @enum {string}
 */
export declare enum KnownAnalyzerNames {
    /**
     * Arabic
     */
    ArMicrosoft = "ar.microsoft",
    /**
     * Arabic
     */
    ArLucene = "ar.lucene",
    /**
     * Armenian
     */
    HyLucene = "hy.lucene",
    /**
     * Bangla
     */
    BnMicrosoft = "bn.microsoft",
    /**
     * Basque
     */
    EuLucene = "eu.lucene",
    /**
     * Bulgarian
     */
    BgMicrosoft = "bg.microsoft",
    /**
     * Bulgarian
     */
    BgLucene = "bg.lucene",
    /**
     * Catalan
     */
    CaMicrosoft = "ca.microsoft",
    /**
     * Catalan
     */
    CaLucene = "ca.lucene",
    /**
     * Chinese Simplified
     */
    ZhHansMicrosoft = "zh-Hans.microsoft",
    /**
     * Chinese Simplified
     */
    ZhHansLucene = "zh-Hans.lucene",
    /**
     * Chinese Traditional
     */
    ZhHantMicrosoft = "zh-Hant.microsoft",
    /**
     * Chinese Traditional
     */
    ZhHantLucene = "zh-Hant.lucene",
    /**
     * Croatian
     */
    HrMicrosoft = "hr.microsoft",
    /**
     * Czech
     */
    CsMicrosoft = "cs.microsoft",
    /**
     * Czech
     */
    CsLucene = "cs.lucene",
    /**
     * Danish
     */
    DaMicrosoft = "da.microsoft",
    /**
     * Danish
     */
    DaLucene = "da.lucene",
    /**
     * Dutch
     */
    NlMicrosoft = "nl.microsoft",
    /**
     * Dutch
     */
    NlLucene = "nl.lucene",
    /**
     * English
     */
    EnMicrosoft = "en.microsoft",
    /**
     * English
     */
    EnLucene = "en.lucene",
    /**
     * Estonian
     */
    EtMicrosoft = "et.microsoft",
    /**
     * Finnish
     */
    FiMicrosoft = "fi.microsoft",
    /**
     * Finnish
     */
    FiLucene = "fi.lucene",
    /**
     * French
     */
    FrMicrosoft = "fr.microsoft",
    /**
     * French
     */
    FrLucene = "fr.lucene",
    /**
     * Galician
     */
    GlLucene = "gl.lucene",
    /**
     * German
     */
    DeMicrosoft = "de.microsoft",
    /**
     * German
     */
    DeLucene = "de.lucene",
    /**
     * Greek
     */
    ElMicrosoft = "el.microsoft",
    /**
     * Greek
     */
    ElLucene = "el.lucene",
    /**
     * Gujarati
     */
    GuMicrosoft = "gu.microsoft",
    /**
     * Hebrew
     */
    HeMicrosoft = "he.microsoft",
    /**
     * Hindi
     */
    HiMicrosoft = "hi.microsoft",
    /**
     * Hindi
     */
    HiLucene = "hi.lucene",
    /**
     * Hungarian
     */
    HuMicrosoft = "hu.microsoft",
    /**
     * Hungarian
     */
    HuLucene = "hu.lucene",
    /**
     * Icelandic
     */
    IsMicrosoft = "is.microsoft",
    /**
     * Indonesian (Bahasa)
     */
    IdMicrosoft = "id.microsoft",
    /**
     * Indonesian (Bahasa)
     */
    IdLucene = "id.lucene",
    /**
     * Irish
     */
    GaLucene = "ga.lucene",
    /**
     * Italian
     */
    ItMicrosoft = "it.microsoft",
    /**
     * Italian
     */
    ItLucene = "it.lucene",
    /**
     * Japanese
     */
    JaMicrosoft = "ja.microsoft",
    /**
     * Japanese
     */
    JaLucene = "ja.lucene",
    /**
     * Kannada
     */
    KnMicrosoft = "kn.microsoft",
    /**
     * Korean
     */
    KoMicrosoft = "ko.microsoft",
    /**
     * Korean
     */
    KoLucene = "ko.lucene",
    /**
     * Latvian
     */
    LvMicrosoft = "lv.microsoft",
    /**
     * Latvian
     */
    LvLucene = "lv.lucene",
    /**
     * Lithuanian
     */
    LtMicrosoft = "lt.microsoft",
    /**
     * Malayalam
     */
    MlMicrosoft = "ml.microsoft",
    /**
     * Malay (Latin)
     */
    MsMicrosoft = "ms.microsoft",
    /**
     * Marathi
     */
    MrMicrosoft = "mr.microsoft",
    /**
     * Norwegian
     */
    NbMicrosoft = "nb.microsoft",
    /**
     * Norwegian
     */
    NoLucene = "no.lucene",
    /**
     * Persian
     */
    FaLucene = "fa.lucene",
    /**
     * Polish
     */
    PlMicrosoft = "pl.microsoft",
    /**
     * Polish
     */
    PlLucene = "pl.lucene",
    /**
     * Portuguese (Brazil)
     */
    PtBRMicrosoft = "pt-BR.microsoft",
    /**
     * Portuguese (Brazil)
     */
    PtBRLucene = "pt-BR.lucene",
    /**
     * Portuguese (Portugal)
     */
    PtPTMicrosoft = "pt-PT.microsoft",
    /**
     * Portuguese (Portugal)
     */
    PtPTLucene = "pt-PT.lucene",
    /**
     * Punjabi
     */ PaMicrosoft = "pa.microsoft",
    /**
     * Romanian
     */
    RoMicrosoft = "ro.microsoft",
    /**
     * Romanian
     */
    RoLucene = "ro.lucene",
    /**
     * Russian
     */
    RuMicrosoft = "ru.microsoft",
    /**
     * Russian
     */
    RuLucene = "ru.lucene",
    /**
     * Serbian (Cyrillic)
     */
    SrCyrillicMicrosoft = "sr-cyrillic.microsoft",
    /**
     * Serbian (Latin)
     */
    SrLatinMicrosoft = "sr-latin.microsoft",
    /**
     * Slovak
     */
    SkMicrosoft = "sk.microsoft",
    /**
     * Slovenian
     */
    SlMicrosoft = "sl.microsoft",
    /**
     * Spanish
     */
    EsMicrosoft = "es.microsoft",
    /**
     * Spanish
     */
    EsLucene = "es.lucene",
    /**
     * Swedish
     */
    SvMicrosoft = "sv.microsoft",
    /**
     * Swedish
     */
    SvLucene = "sv.lucene",
    /**
     * Tamil
     */
    TaMicrosoft = "ta.microsoft",
    /**
     * Telugu
     */
    TeMicrosoft = "te.microsoft",
    /**
     * Thai
     */
    ThMicrosoft = "th.microsoft",
    /**
     * Thai
     */
    ThLucene = "th.lucene",
    /**
     * Turkish
     */
    TrMicrosoft = "tr.microsoft",
    /**
     * Turkish
     */
    TrLucene = "tr.lucene",
    /**
     * Ukrainian
     */
    UkMicrosoft = "uk.microsoft",
    /**
     * Urdu
     */
    UrMicrosoft = "ur.microsoft",
    /**
     * Vietnamese
     */
    ViMicrosoft = "vi.microsoft",
    /**
     * See: https://lucene.apache.org/core/6_6_1/core/org/apache/lucene/analysis/standard/StandardAnalyzer.html
     */
    StandardLucene = "standard.lucene",
    /**
     * See https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html
     */
    StandardAsciiFoldingLucene = "standardasciifolding.lucene",
    /**
     * Treats the entire content of a field as a single token. This is useful for data like zip codes, ids, and some product names.
     */
    Keyword = "keyword",
    /**
     * Flexibly separates text into terms via a regular expression pattern.
     */
    Pattern = "pattern",
    /**
     * Divides text at non-letters and converts them to lower case.
     */
    Simple = "simple",
    /**
     * Divides text at non-letters; Applies the lowercase and stopword token filters.
     */
    Stop = "stop",
    /**
     * An analyzer that uses the whitespace tokenizer.
     */
    Whitespace = "whitespace"
}

/**
 * Defines values for CharFilterName.
 * @readonly
 * @enum {string}
 */
export declare enum KnownCharFilterNames {
    /**
     * A character filter that attempts to strip out HTML constructs. See
     * https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.html
     */
    HtmlStrip = "html_strip"
}

/**
 * Defines values for TokenFilterName.
 * @readonly
 * @enum {string}
 */
export declare enum KnownTokenFilterNames {
    /**
     * A token filter that applies the Arabic normalizer to normalize the orthography. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ar/ArabicNormalizationFilter.html
     */
    ArabicNormalization = "arabic_normalization",
    /**
     * Strips all characters after an apostrophe (including the apostrophe itself). See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/tr/ApostropheFilter.html
     */
    Apostrophe = "apostrophe",
    /**
     * Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127
     * ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such
     * equivalents exist. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html
     */
    AsciiFolding = "asciifolding",
    /**
     * Forms bigrams of CJK terms that are generated from StandardTokenizer. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKBigramFilter.html
     */
    CjkBigram = "cjk_bigram",
    /**
     * Normalizes CJK width differences. Folds fullwidth ASCII variants into the equivalent basic
     * Latin, and half-width Katakana variants into the equivalent Kana. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKWidthFilter.html
     */
    CjkWidth = "cjk_width",
    /**
     * Removes English possessives, and dots from acronyms. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicFilter.html
     */
    Classic = "classic",
    /**
     * Construct bigrams for frequently occurring terms while indexing. Single terms are still
     * indexed too, with bigrams overlaid. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/commongrams/CommonGramsFilter.html
     */
    CommonGram = "common_grams",
    /**
     * Generates n-grams of the given size(s) starting from the front or the back of an input token.
     * See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html
     */
    EdgeNGram = "edgeNGram_v2",
    /**
     * Removes elisions. For example, "l'avion" (the plane) will be converted to "avion" (plane). See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html
     */
    Elision = "elision",
    /**
     * Normalizes German characters according to the heuristics of the German2 snowball algorithm.
     * See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/de/GermanNormalizationFilter.html
     */
    GermanNormalization = "german_normalization",
    /**
     * Normalizes text in Hindi to remove some differences in spelling variations. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/hi/HindiNormalizationFilter.html
     */
    HindiNormalization = "hindi_normalization",
    /**
     * Normalizes the Unicode representation of text in Indian languages. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/in/IndicNormalizationFilter.html
     */
    IndicNormalization = "indic_normalization",
    /**
     * Emits each incoming token twice, once as keyword and once as non-keyword. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeywordRepeatFilter.html
     */
    KeywordRepeat = "keyword_repeat",
    /**
     * A high-performance kstem filter for English. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/en/KStemFilter.html
     */
    KStem = "kstem",
    /**
     * Removes words that are too long or too short. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LengthFilter.html
     */
    Length = "length",
    /**
     * Limits the number of tokens while indexing. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilter.html
     */
    Limit = "limit",
    /**
     * Normalizes token text to lower case. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.htm
     */
    Lowercase = "lowercase",
    /**
     * Generates n-grams of the given size(s). See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html
     */
    NGram = "nGram_v2",
    /**
     * Applies normalization for Persian. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/fa/PersianNormalizationFilter.html
     */
    PersianNormalization = "persian_normalization",
    /**
     * Create tokens for phonetic matches. See
     * https://lucene.apache.org/core/4_10_3/analyzers-phonetic/org/apache/lucene/analysis/phonetic/package-tree.html
     */
    Phonetic = "phonetic",
    /**
     * Uses the Porter stemming algorithm to transform the token stream. See
     * http://tartarus.org/~martin/PorterStemmer
     */
    PorterStem = "porter_stem",
    /**
     * Reverses the token string. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/reverse/ReverseStringFilter.html
     */
    Reverse = "reverse",
    /**
     * Normalizes use of the interchangeable Scandinavian characters. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.html
     */
    ScandinavianNormalization = "scandinavian_normalization",
    /**
     * Folds Scandinavian characters åÅäæÄÆ-&gt;a and öÖøØ-&gt;o. It also discriminates against use
     * of double vowels aa, ae, ao, oe and oo, leaving just the first one. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.html
     */
    ScandinavianFoldingNormalization = "scandinavian_folding",
    /**
     * Creates combinations of tokens as a single token. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html
     */
    Shingle = "shingle",
    /**
     * A filter that stems words using a Snowball-generated stemmer. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/snowball/SnowballFilter.html
     */
    Snowball = "snowball",
    /**
     * Normalizes the Unicode representation of Sorani text. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ckb/SoraniNormalizationFilter.html
     */
    SoraniNormalization = "sorani_normalization",
    /**
     * Language specific stemming filter. See
     * https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search#TokenFilters
     */
    Stemmer = "stemmer",
    /**
     * Removes stop words from a token stream. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/StopFilter.html
     */
    Stopwords = "stopwords",
    /**
     * Trims leading and trailing whitespace from tokens. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TrimFilter.html
     */
    Trim = "trim",
    /**
     * Truncates the terms to a specific length. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.html
     */
    Truncate = "truncate",
    /**
     * Filters out tokens with same text as the previous token. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.html
     */
    Unique = "unique",
    /**
     * Normalizes token text to upper case. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/UpperCaseFilter.html
     */
    Uppercase = "uppercase",
    /**
     * Splits words into subwords and performs optional transformations on subword groups.
     */
    WordDelimiter = "word_delimiter"
}

/**
 * Defines values for TokenizerName.
 * @readonly
 * @enum {string}
 */
export declare enum KnownTokenizerNames {
    /**
     * Grammar-based tokenizer that is suitable for processing most European-language documents. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html
     */
    Classic = "classic",
    /**
     * Tokenizes the input from an edge into n-grams of the given size(s). See
     * https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html
     */
    EdgeNGram = "edgeNGram",
    /**
     * Emits the entire input as a single token. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html
     */
    Keyword = "keyword_v2",
    /**
     * Divides text at non-letters. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html
     */
    Letter = "letter",
    /**
     * Divides text at non-letters and converts them to lower case. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseTokenizer.html
     */
    Lowercase = "lowercase",
    /**
     * Divides text using language-specific rules.
     */
    MicrosoftLanguageTokenizer = "microsoft_language_tokenizer",
    /**
     * Divides text using language-specific rules and reduces words to their base forms.
     */
    MicrosoftLanguageStemmingTokenizer = "microsoft_language_stemming_tokenizer",
    /**
     * Tokenizes the input into n-grams of the given size(s). See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html
     */
    NGram = "nGram",
    /**
     * Tokenizer for path-like hierarchies. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html
     */
    PathHierarchy = "path_hierarchy_v2",
    /**
     * Tokenizer that uses regex pattern matching to construct distinct tokens. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html
     */
    Pattern = "pattern",
    /**
     * Standard Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop
     * filter. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html
     */
    Standard = "standard_v2",
    /**
     * Tokenizes urls and emails as one token. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html
     */
    UaxUrlEmail = "uax_url_email",
    /**
     * Divides text at whitespace. See
     * http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html
     */
    Whitespace = "whitespace"
}

/**
 * A skill that detects the language of input text and reports a single language code for every
 * document submitted on the request. The language code is paired with a score indicating the
 * confidence of the analysis.
 */
export declare interface LanguageDetectionSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Text.LanguageDetectionSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
}

/**
 * Removes words that are too long or too short. This token filter is implemented using Apache
 * Lucene.
 */
export declare interface LengthTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.LengthTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The minimum length in characters. Default is 0. Maximum is 300. Must be less than the value of
     * max. Default value: 0.
     */
    minLength?: number;
    /**
     * The maximum length in characters. Default and maximum is 300. Default value: 300.
     */
    maxLength?: number;
}

/**
 * Contains the possible cases for Analyzer.
 */
export declare type LexicalAnalyzer = CustomAnalyzer | PatternAnalyzer | LuceneStandardAnalyzer | StopAnalyzer;

/**
 * Defines values for LexicalAnalyzerName.
 * Possible values include: 'ArMicrosoft', 'ArLucene', 'HyLucene', 'BnMicrosoft', 'EuLucene',
 * 'BgMicrosoft', 'BgLucene', 'CaMicrosoft', 'CaLucene', 'ZhHansMicrosoft', 'ZhHansLucene',
 * 'ZhHantMicrosoft', 'ZhHantLucene', 'HrMicrosoft', 'CsMicrosoft', 'CsLucene', 'DaMicrosoft',
 * 'DaLucene', 'NlMicrosoft', 'NlLucene', 'EnMicrosoft', 'EnLucene', 'EtMicrosoft', 'FiMicrosoft',
 * 'FiLucene', 'FrMicrosoft', 'FrLucene', 'GlLucene', 'DeMicrosoft', 'DeLucene', 'ElMicrosoft',
 * 'ElLucene', 'GuMicrosoft', 'HeMicrosoft', 'HiMicrosoft', 'HiLucene', 'HuMicrosoft', 'HuLucene',
 * 'IsMicrosoft', 'IdMicrosoft', 'IdLucene', 'GaLucene', 'ItMicrosoft', 'ItLucene', 'JaMicrosoft',
 * 'JaLucene', 'KnMicrosoft', 'KoMicrosoft', 'KoLucene', 'LvMicrosoft', 'LvLucene', 'LtMicrosoft',
 * 'MlMicrosoft', 'MsMicrosoft', 'MrMicrosoft', 'NbMicrosoft', 'NoLucene', 'FaLucene',
 * 'PlMicrosoft', 'PlLucene', 'PtBrMicrosoft', 'PtBrLucene', 'PtPtMicrosoft', 'PtPtLucene',
 * 'PaMicrosoft', 'RoMicrosoft', 'RoLucene', 'RuMicrosoft', 'RuLucene', 'SrCyrillicMicrosoft',
 * 'SrLatinMicrosoft', 'SkMicrosoft', 'SlMicrosoft', 'EsMicrosoft', 'EsLucene', 'SvMicrosoft',
 * 'SvLucene', 'TaMicrosoft', 'TeMicrosoft', 'ThMicrosoft', 'ThLucene', 'TrMicrosoft', 'TrLucene',
 * 'UkMicrosoft', 'UrMicrosoft', 'ViMicrosoft', 'StandardLucene', 'StandardAsciiFoldingLucene',
 * 'Keyword', 'Pattern', 'Simple', 'Stop', 'Whitespace'
 * @readonly
 * @enum {string}
 */
export declare type LexicalAnalyzerName = 'ar.microsoft' | 'ar.lucene' | 'hy.lucene' | 'bn.microsoft' | 'eu.lucene' | 'bg.microsoft' | 'bg.lucene' | 'ca.microsoft' | 'ca.lucene' | 'zh-Hans.microsoft' | 'zh-Hans.lucene' | 'zh-Hant.microsoft' | 'zh-Hant.lucene' | 'hr.microsoft' | 'cs.microsoft' | 'cs.lucene' | 'da.microsoft' | 'da.lucene' | 'nl.microsoft' | 'nl.lucene' | 'en.microsoft' | 'en.lucene' | 'et.microsoft' | 'fi.microsoft' | 'fi.lucene' | 'fr.microsoft' | 'fr.lucene' | 'gl.lucene' | 'de.microsoft' | 'de.lucene' | 'el.microsoft' | 'el.lucene' | 'gu.microsoft' | 'he.microsoft' | 'hi.microsoft' | 'hi.lucene' | 'hu.microsoft' | 'hu.lucene' | 'is.microsoft' | 'id.microsoft' | 'id.lucene' | 'ga.lucene' | 'it.microsoft' | 'it.lucene' | 'ja.microsoft' | 'ja.lucene' | 'kn.microsoft' | 'ko.microsoft' | 'ko.lucene' | 'lv.microsoft' | 'lv.lucene' | 'lt.microsoft' | 'ml.microsoft' | 'ms.microsoft' | 'mr.microsoft' | 'nb.microsoft' | 'no.lucene' | 'fa.lucene' | 'pl.microsoft' | 'pl.lucene' | 'pt-BR.microsoft' | 'pt-BR.lucene' | 'pt-PT.microsoft' | 'pt-PT.lucene' | 'pa.microsoft' | 'ro.microsoft' | 'ro.lucene' | 'ru.microsoft' | 'ru.lucene' | 'sr-cyrillic.microsoft' | 'sr-latin.microsoft' | 'sk.microsoft' | 'sl.microsoft' | 'es.microsoft' | 'es.lucene' | 'sv.microsoft' | 'sv.lucene' | 'ta.microsoft' | 'te.microsoft' | 'th.microsoft' | 'th.lucene' | 'tr.microsoft' | 'tr.lucene' | 'uk.microsoft' | 'ur.microsoft' | 'vi.microsoft' | 'standard.lucene' | 'standardasciifolding.lucene' | 'keyword' | 'pattern' | 'simple' | 'stop' | 'whitespace';

/**
 * Contains the possible cases for Tokenizer.
 */
export declare type LexicalTokenizer = ClassicTokenizer | EdgeNGramTokenizer | KeywordTokenizer | MicrosoftLanguageTokenizer | MicrosoftLanguageStemmingTokenizer | NGramTokenizer | PathHierarchyTokenizer | PatternTokenizer | LuceneStandardTokenizer | UaxUrlEmailTokenizer;

/**
 * Limits the number of tokens while indexing. This token filter is implemented using Apache
 * Lucene.
 */
export declare interface LimitTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.LimitTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The maximum number of tokens to produce. Default is 1. Default value: 1.
     */
    maxTokenCount?: number;
    /**
     * A value indicating whether all tokens from the input must be consumed even if maxTokenCount is
     * reached. Default is false. Default value: false.
     */
    consumeAllTokens?: boolean;
}

/**
 * Options for a list data sources operation.
 */
export declare type ListDataSourceConnectionsOptions = OperationOptions;

/**
 * Options for a list indexers operation.
 */
export declare type ListIndexersOptions = OperationOptions;

/**
 * Options for a list indexes operation.
 */
export declare type ListIndexesOptions = OperationOptions;

/**
 * Arguments for retrieving the next page of search results.
 */
export declare interface ListSearchResultsPageSettings {
    /**
     * A token used for retrieving the next page of results when the server
     * enforces pagination.
     */
    continuationToken?: string;
}

/**
 * Options for a list skillsets operation.
 */
export declare type ListSkillsetsOptions = OperationOptions;

/**
 * Options for a list synonymMaps operation.
 */
export declare type ListSynonymMapsOptions = OperationOptions;

/**
 * Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop
 * filter.
 */
export declare interface LuceneStandardAnalyzer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.StandardAnalyzer";
    /**
     * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
     * can only start and end with alphanumeric characters, and is limited to 128 characters.
     */
    name: string;
    /**
     * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The
     * maximum token length that can be used is 300 characters. Default value: 255.
     */
    maxTokenLength?: number;
    /**
     * A list of stopwords.
     */
    stopwords?: string[];
}

/**
 * Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using
 * Apache Lucene.
 */
export declare interface LuceneStandardTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.StandardTokenizerV2" | "#Microsoft.Azure.Search.StandardTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The
     * maximum token length that can be used is 300 characters. Default value: 255.
     */
    maxTokenLength?: number;
}

/**
 * Defines a function that boosts scores based on the magnitude of a numeric field.
 */
export declare interface MagnitudeScoringFunction {
    /**
     * Polymorphic Discriminator
     */
    type: "magnitude";
    /**
     * The name of the field used as input to the scoring function.
     */
    fieldName: string;
    /**
     * A multiplier for the raw score. Must be a positive number not equal to 1.0.
     */
    boost: number;
    /**
     * A value indicating how boosting will be interpolated across document scores; defaults to
     * "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
     */
    interpolation?: ScoringFunctionInterpolation;
    /**
     * Parameter values for the magnitude scoring function.
     */
    parameters: MagnitudeScoringParameters;
}

/**
 * Provides parameter values to a magnitude scoring function.
 */
export declare interface MagnitudeScoringParameters {
    /**
     * The field value at which boosting starts.
     */
    boostingRangeStart: number;
    /**
     * The field value at which boosting ends.
     */
    boostingRangeEnd: number;
    /**
     * A value indicating whether to apply a constant boost for field values beyond the range end
     * value; default is false.
     */
    shouldBoostBeyondRangeByConstant?: boolean;
}

/**
 * A character filter that applies mappings defined with the mappings option. Matching is greedy
 * (longest pattern matching at a given point wins). Replacement is allowed to be the empty string.
 * This character filter is implemented using Apache Lucene.
 */
export declare interface MappingCharFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.MappingCharFilter";
    /**
     * The name of the char filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A list of mappings of the following format: "a=>b" (all occurrences of the character "a" will
     * be replaced with character "b").
     */
    mappings: string[];
}

/**
 * Options for the merge documents operation.
 */
export declare type MergeDocumentsOptions = IndexDocumentsOptions;

/**
 * Options for the merge or upload documents operation.
 */
export declare type MergeOrUploadDocumentsOptions = IndexDocumentsOptions;

/**
 * A skill for merging two or more strings into a single unified string, with an optional
 * user-defined delimiter separating each component part.
 */
export declare interface MergeSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Text.MergeSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * The tag indicates the start of the merged text. By default, the tag is an empty space. Default
     * value: ''.
     */
    insertPreTag?: string;
    /**
     * The tag indicates the end of the merged text. By default, the tag is an empty space. Default
     * value: ''.
     */
    insertPostTag?: string;
}

/**
 * Divides text using language-specific rules and reduces words to their base forms.
 */
export declare interface MicrosoftLanguageStemmingTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The maximum token length. Tokens longer than the maximum length are split. Maximum token
     * length that can be used is 300 characters. Tokens longer than 300 characters are first split
     * into tokens of length 300 and then each of those tokens is split based on the max token length
     * set. Default is 255. Default value: 255.
     */
    maxTokenLength?: number;
    /**
     * A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set
     * to false if used as the indexing tokenizer. Default is false. Default value: false.
     */
    isSearchTokenizer?: boolean;
    /**
     * The language to use. The default is English. Possible values include: 'Arabic', 'Bangla',
     * 'Bulgarian', 'Catalan', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian',
     * 'Finnish', 'French', 'German', 'Greek', 'Gujarati', 'Hebrew', 'Hindi', 'Hungarian',
     * 'Icelandic', 'Indonesian', 'Italian', 'Kannada', 'Latvian', 'Lithuanian', 'Malay',
     * 'Malayalam', 'Marathi', 'NorwegianBokmaal', 'Polish', 'Portuguese', 'PortugueseBrazilian',
     * 'Punjabi', 'Romanian', 'Russian', 'SerbianCyrillic', 'SerbianLatin', 'Slovak', 'Slovenian',
     * 'Spanish', 'Swedish', 'Tamil', 'Telugu', 'Turkish', 'Ukrainian', 'Urdu'
     */
    language?: MicrosoftStemmingTokenizerLanguage;
}

/**
 * Divides text using language-specific rules.
 */
export declare interface MicrosoftLanguageTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.MicrosoftLanguageTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The maximum token length. Tokens longer than the maximum length are split. Maximum token
     * length that can be used is 300 characters. Tokens longer than 300 characters are first split
     * into tokens of length 300 and then each of those tokens is split based on the max token length
     * set. Default is 255. Default value: 255.
     */
    maxTokenLength?: number;
    /**
     * A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set
     * to false if used as the indexing tokenizer. Default is false. Default value: false.
     */
    isSearchTokenizer?: boolean;
    /**
     * The language to use. The default is English. Possible values include: 'Bangla', 'Bulgarian',
     * 'Catalan', 'ChineseSimplified', 'ChineseTraditional', 'Croatian', 'Czech', 'Danish', 'Dutch',
     * 'English', 'French', 'German', 'Greek', 'Gujarati', 'Hindi', 'Icelandic', 'Indonesian',
     * 'Italian', 'Japanese', 'Kannada', 'Korean', 'Malay', 'Malayalam', 'Marathi',
     * 'NorwegianBokmaal', 'Polish', 'Portuguese', 'PortugueseBrazilian', 'Punjabi', 'Romanian',
     * 'Russian', 'SerbianCyrillic', 'SerbianLatin', 'Slovenian', 'Spanish', 'Swedish', 'Tamil',
     * 'Telugu', 'Thai', 'Ukrainian', 'Urdu', 'Vietnamese'
     */
    language?: MicrosoftTokenizerLanguage;
}

/**
 * Defines values for MicrosoftStemmingTokenizerLanguage.
 * Possible values include: 'Arabic', 'Bangla', 'Bulgarian', 'Catalan', 'Croatian', 'Czech',
 * 'Danish', 'Dutch', 'English', 'Estonian', 'Finnish', 'French', 'German', 'Greek', 'Gujarati',
 * 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Kannada', 'Latvian',
 * 'Lithuanian', 'Malay', 'Malayalam', 'Marathi', 'NorwegianBokmaal', 'Polish', 'Portuguese',
 * 'PortugueseBrazilian', 'Punjabi', 'Romanian', 'Russian', 'SerbianCyrillic', 'SerbianLatin',
 * 'Slovak', 'Slovenian', 'Spanish', 'Swedish', 'Tamil', 'Telugu', 'Turkish', 'Ukrainian', 'Urdu'
 * @readonly
 * @enum {string}
 */
export declare type MicrosoftStemmingTokenizerLanguage = 'arabic' | 'bangla' | 'bulgarian' | 'catalan' | 'croatian' | 'czech' | 'danish' | 'dutch' | 'english' | 'estonian' | 'finnish' | 'french' | 'german' | 'greek' | 'gujarati' | 'hebrew' | 'hindi' | 'hungarian' | 'icelandic' | 'indonesian' | 'italian' | 'kannada' | 'latvian' | 'lithuanian' | 'malay' | 'malayalam' | 'marathi' | 'norwegianBokmaal' | 'polish' | 'portuguese' | 'portugueseBrazilian' | 'punjabi' | 'romanian' | 'russian' | 'serbianCyrillic' | 'serbianLatin' | 'slovak' | 'slovenian' | 'spanish' | 'swedish' | 'tamil' | 'telugu' | 'turkish' | 'ukrainian' | 'urdu';

/**
 * Defines values for MicrosoftTokenizerLanguage.
 * Possible values include: 'Bangla', 'Bulgarian', 'Catalan', 'ChineseSimplified',
 * 'ChineseTraditional', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'French', 'German',
 * 'Greek', 'Gujarati', 'Hindi', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Kannada',
 * 'Korean', 'Malay', 'Malayalam', 'Marathi', 'NorwegianBokmaal', 'Polish', 'Portuguese',
 * 'PortugueseBrazilian', 'Punjabi', 'Romanian', 'Russian', 'SerbianCyrillic', 'SerbianLatin',
 * 'Slovenian', 'Spanish', 'Swedish', 'Tamil', 'Telugu', 'Thai', 'Ukrainian', 'Urdu', 'Vietnamese'
 * @readonly
 * @enum {string}
 */
export declare type MicrosoftTokenizerLanguage = 'bangla' | 'bulgarian' | 'catalan' | 'chineseSimplified' | 'chineseTraditional' | 'croatian' | 'czech' | 'danish' | 'dutch' | 'english' | 'french' | 'german' | 'greek' | 'gujarati' | 'hindi' | 'icelandic' | 'indonesian' | 'italian' | 'japanese' | 'kannada' | 'korean' | 'malay' | 'malayalam' | 'marathi' | 'norwegianBokmaal' | 'polish' | 'portuguese' | 'portugueseBrazilian' | 'punjabi' | 'romanian' | 'russian' | 'serbianCyrillic' | 'serbianLatin' | 'slovenian' | 'spanish' | 'swedish' | 'tamil' | 'telugu' | 'thai' | 'ukrainian' | 'urdu' | 'vietnamese';

/**
 * Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.
 */
export declare interface NGramTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.NGramTokenFilterV2" | "#Microsoft.Azure.Search.NGramTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of
     * maxGram. Default value: 1.
     */
    minGram?: number;
    /**
     * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.
     */
    maxGram?: number;
}

/**
 * Tokenizes the input into n-grams of the given size(s). This tokenizer is implemented using
 * Apache Lucene.
 */
export declare interface NGramTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.NGramTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of
     * maxGram. Default value: 1.
     */
    minGram?: number;
    /**
     * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.
     */
    maxGram?: number;
    /**
     * Character classes to keep in the tokens.
     */
    tokenChars?: TokenCharacterKind[];
}

/**
 * A skill that extracts text from image files.
 */
export declare interface OcrSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Vision.OcrSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * A value indicating which language code to use. Default is en. Possible values include:
     * 'zh-Hans', 'zh-Hant', 'cs', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'hu', 'it', 'ja', 'ko',
     * 'nb', 'pl', 'pt', 'ru', 'es', 'sv', 'tr', 'ar', 'ro', 'sr-Cyrl', 'sr-Latn', 'sk'
     */
    defaultLanguageCode?: OcrSkillLanguage;
    /**
     * A value indicating to turn orientation detection on or not. Default is false. Default value:
     * false.
     */
    shouldDetectOrientation?: boolean;
}

/**
 * Defines values for OcrSkillLanguage.
 * Possible values include: 'zh-Hans', 'zh-Hant', 'cs', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el',
 * 'hu', 'it', 'ja', 'ko', 'nb', 'pl', 'pt', 'ru', 'es', 'sv', 'tr', 'ar', 'ro', 'sr-Cyrl',
 * 'sr-Latn', 'sk'
 * @readonly
 * @enum {string}
 */
export declare type OcrSkillLanguage = 'zh-Hans' | 'zh-Hant' | 'cs' | 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'hu' | 'it' | 'ja' | 'ko' | 'nb' | 'pl' | 'pt' | 'ru' | 'es' | 'sv' | 'tr' | 'ar' | 'ro' | 'sr-Cyrl' | 'sr-Latn' | 'sk';

/**
 * Escapes an odata filter expression to avoid errors with quoting string literals.
 * Example usage:
 * ```ts
 * const baseRateMax = 200;
 * const ratingMin = 4;
 * const filter = odata`Rooms/any(room: room/BaseRate lt ${baseRateMax}) and Rating ge ${ratingMin}`;
 * ```
 * For more information on supported syntax see: https://docs.microsoft.com/en-us/azure/search/search-query-odata-filter
 * @param strings
 * @param values
 */
export declare function odata(strings: TemplateStringsArray, ...values: unknown[]): string;

/**
 * Output field mapping for a skill.
 */
export declare interface OutputFieldMappingEntry {
    /**
     * The name of the output defined by the skill.
     */
    name: string;
    /**
     * The target name of the output. It is optional and default to name.
     */
    targetName?: string;
}

/**
 * Tokenizer for path-like hierarchies. This tokenizer is implemented using Apache Lucene.
 */
export declare interface PathHierarchyTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.PathHierarchyTokenizerV2";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The delimiter character to use. Default is "/". Default value: '/'.
     */
    delimiter?: string;
    /**
     * A value that, if set, replaces the delimiter character. Default is "/". Default value: '/'.
     */
    replacement?: string;
    /**
     * The maximum token length. Default and maximum is 300. Default value: 300.
     */
    maxTokenLength?: number;
    /**
     * A value indicating whether to generate tokens in reverse order. Default is false. Default
     * value: false.
     */
    reverseTokenOrder?: boolean;
    /**
     * The number of initial tokens to skip. Default is 0. Default value: 0.
     */
    numberOfTokensToSkip?: number;
}

/**
 * Flexibly separates text into terms via a regular expression pattern. This analyzer is
 * implemented using Apache Lucene.
 */
export declare interface PatternAnalyzer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.PatternAnalyzer";
    /**
     * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
     * can only start and end with alphanumeric characters, and is limited to 128 characters.
     */
    name: string;
    /**
     * A value indicating whether terms should be lower-cased. Default is true. Default value: true.
     */
    lowerCaseTerms?: boolean;
    /**
     * A regular expression pattern to match token separators. Default is an expression that matches
     * one or more whitespace characters. Default value: '\W+'.
     */
    pattern?: string;
    /**
     * Regular expression flags. Possible values include: 'CANON_EQ', 'CASE_INSENSITIVE', 'COMMENTS',
     * 'DOTALL', 'LITERAL', 'MULTILINE', 'UNICODE_CASE', 'UNIX_LINES'
     */
    flags?: RegexFlags[];
    /**
     * A list of stopwords.
     */
    stopwords?: string[];
}

/**
 * Uses Java regexes to emit multiple tokens - one for each capture group in one or more patterns.
 * This token filter is implemented using Apache Lucene.
 */
export declare interface PatternCaptureTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.PatternCaptureTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A list of patterns to match against each token.
     */
    patterns: string[];
    /**
     * A value indicating whether to return the original token even if one of the patterns matches.
     * Default is true. Default value: true.
     */
    preserveOriginal?: boolean;
}

/**
 * A character filter that replaces characters in the input string. It uses a regular expression to
 * identify character sequences to preserve and a replacement pattern to identify characters to
 * replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement
 * "$1#$2", the result would be "aa#bb aa#bb". This character filter is implemented using Apache
 * Lucene.
 */
export declare interface PatternReplaceCharFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.PatternReplaceCharFilter";
    /**
     * The name of the char filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A regular expression pattern.
     */
    pattern: string;
    /**
     * The replacement text.
     */
    replacement: string;
}

/**
 * A character filter that replaces characters in the input string. It uses a regular expression to
 * identify character sequences to preserve and a replacement pattern to identify characters to
 * replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement
 * "$1#$2", the result would be "aa#bb aa#bb". This token filter is implemented using Apache
 * Lucene.
 */
export declare interface PatternReplaceTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.PatternReplaceTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A regular expression pattern.
     */
    pattern: string;
    /**
     * The replacement text.
     */
    replacement: string;
}

/**
 * Tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer is
 * implemented using Apache Lucene.
 */
export declare interface PatternTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.PatternTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A regular expression pattern to match token separators. Default is an expression that matches
     * one or more whitespace characters. Default value: '\W+'.
     */
    pattern?: string;
    /**
     * Regular expression flags. Possible values include: 'CANON_EQ', 'CASE_INSENSITIVE', 'COMMENTS',
     * 'DOTALL', 'LITERAL', 'MULTILINE', 'UNICODE_CASE', 'UNIX_LINES'
     */
    flags?: RegexFlags[];
    /**
     * The zero-based ordinal of the matching group in the regular expression pattern to extract into
     * tokens. Use -1 if you want to use the entire pattern to split the input into tokens,
     * irrespective of matching groups. Default is -1. Default value: -1.
     */
    group?: number;
}

/**
 * Defines values for PhoneticEncoder.
 * Possible values include: 'Metaphone', 'DoubleMetaphone', 'Soundex', 'RefinedSoundex',
 * 'Caverphone1', 'Caverphone2', 'Cologne', 'Nysiis', 'KoelnerPhonetik', 'HaasePhonetik',
 * 'BeiderMorse'
 * @readonly
 * @enum {string}
 */
export declare type PhoneticEncoder = 'metaphone' | 'doubleMetaphone' | 'soundex' | 'refinedSoundex' | 'caverphone1' | 'caverphone2' | 'cologne' | 'nysiis' | 'koelnerPhonetik' | 'haasePhonetik' | 'beiderMorse';

/**
 * Create tokens for phonetic matches. This token filter is implemented using Apache Lucene.
 */
export declare interface PhoneticTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.PhoneticTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The phonetic encoder to use. Default is "metaphone". Possible values include: 'Metaphone',
     * 'DoubleMetaphone', 'Soundex', 'RefinedSoundex', 'Caverphone1', 'Caverphone2', 'Cologne',
     * 'Nysiis', 'KoelnerPhonetik', 'HaasePhonetik', 'BeiderMorse'
     */
    encoder?: PhoneticEncoder;
    /**
     * A value indicating whether encoded tokens should replace original tokens. If false, encoded
     * tokens are added as synonyms. Default is true. Default value: true.
     */
    replaceOriginalTokens?: boolean;
}

/**
 * Defines values for QueryType.
 * Possible values include: 'Simple', 'Full'
 * @readonly
 * @enum {string}
 */
export declare type QueryType = 'simple' | 'full';

/**
 * Defines values for RegexFlags.
 * Possible values include: 'CanonEq', 'CaseInsensitive', 'Comments', 'DotAll', 'Literal',
 * 'Multiline', 'UnicodeCase', 'UnixLines'
 * @readonly
 * @enum {string}
 */
export declare type RegexFlags = 'CANON_EQ' | 'CASE_INSENSITIVE' | 'COMMENTS' | 'DOTALL' | 'LITERAL' | 'MULTILINE' | 'UNICODE_CASE' | 'UNIX_LINES';

/**
 * Options for reset indexer operation.
 */
export declare type ResetIndexerOptions = OperationOptions;

/**
 * Represents a resource's usage and quota.
 */
export declare interface ResourceCounter {
    /**
     * The resource usage amount.
     */
    usage: number;
    /**
     * The resource amount quota.
     */
    quota?: number;
}

/**
 * Options for run indexer operation.
 */
export declare type RunIndexerOptions = OperationOptions;

/**
 * Contains the possible cases for ScoringFunction.
 */
export declare type ScoringFunction = DistanceScoringFunction | FreshnessScoringFunction | MagnitudeScoringFunction | TagScoringFunction;

/**
 * Defines values for ScoringFunctionAggregation.
 * Possible values include: 'Sum', 'Average', 'Minimum', 'Maximum', 'FirstMatching'
 * @readonly
 * @enum {string}
 */
export declare type ScoringFunctionAggregation = 'sum' | 'average' | 'minimum' | 'maximum' | 'firstMatching';

/**
 * Defines values for ScoringFunctionInterpolation.
 * Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
 * @readonly
 * @enum {string}
 */
export declare type ScoringFunctionInterpolation = 'linear' | 'constant' | 'quadratic' | 'logarithmic';

/**
 * Defines parameters for a search index that influence scoring in search queries.
 */
export declare interface ScoringProfile {
    /**
     * The name of the scoring profile.
     */
    name: string;
    /**
     * Parameters that boost scoring based on text matches in certain index fields.
     */
    textWeights?: TextWeights;
    /**
     * The collection of functions that influence the scoring of documents.
     */
    functions?: ScoringFunction[];
    /**
     * A value indicating how the results of individual scoring functions should be combined.
     * Defaults to "Sum". Ignored if there are no scoring functions. Possible values include: 'sum',
     * 'average', 'minimum', 'maximum', 'firstMatching'
     */
    functionAggregation?: ScoringFunctionAggregation;
}

/**
 * Class used to perform operations against a search index,
 * including querying documents in the index as well as
 * adding, updating, and removing them.
 */
export declare class SearchClient<T> {
    /**
     * The API version to use when communicating with the service.
     */
    readonly apiVersion: string;
    /**
     * The endpoint of the search service
     */
    readonly endpoint: string;
    /**
     * The name of the index
     */
    readonly indexName: string;
    /* Excluded from this release type: client */
    /**
     * Creates an instance of SearchClient.
     *
     * Example usage:
     * ```ts
     * const { SearchClient, AzureKeyCredential } = require("@azure/search-documents");
     *
     * const client = new SearchClient(
     *   "<endpoint>",
     *   "<indexName>",
     *   new AzureKeyCredential("<Admin Key>");
     * );
     * ```
     * @param {string} endpoint The endpoint of the search service
     * @param {string} indexName The name of the index
     * @param {KeyCredential} credential Used to authenticate requests to the service.
     * @param {SearchClientOptions} [options] Used to configure the Search client.
     */
    constructor(endpoint: string, indexName: string, credential: KeyCredential, options?: SearchClientOptions);
    /**
     * Retrieves the number of documents in the index.
     * @param options Options to the count operation.
     */
    getDocumentsCount(options?: CountDocumentsOptions): Promise<number>;
    /**
     * Based on a partial searchText from the user, return a list
     * of potential completion strings based on a specified suggester.
     * @param searchText The search text on which to base autocomplete results.
     * @param suggesterName The name of the suggester as specified in the suggesters collection that's part of the index definition.
     * @param options Options to the autocomplete operation.
     */
    autocomplete<Fields extends keyof T>(searchText: string, suggesterName: string, options?: AutocompleteOptions<Fields>): Promise<AutocompleteResult>;
    private searchDocuments;
    private listSearchResultsPage;
    private listSearchResultsAll;
    private listSearchResults;
    /**
     * Performs a search on the current index given
     * the specified arguments.
     * @param searchText Text to search
     * @param options Options for the search operation.
     */
    search<Fields extends keyof T>(searchText?: string, options?: SearchOptions<Fields>): Promise<SearchDocumentsResult<Pick<T, Fields>>>;
    /**
     * Returns a short list of suggestions based on the searchText
     * and specified suggester.
     * @param searchText The search text to use to suggest documents. Must be at least 1 character, and no more than 100 characters.
     * @param suggesterName The name of the suggester as specified in the suggesters collection that's part of the index definition.
     * @param options Options for the suggest operation
     */
    suggest<Fields extends keyof T = never>(searchText: string, suggesterName: string, options?: SuggestOptions<Fields>): Promise<SuggestDocumentsResult<Pick<T, Fields>>>;
    /**
     * Retrieve a particular document from the index by key.
     * @param key The primary key value of the document
     * @param options Additional options
     */
    getDocument<Fields extends keyof T>(key: string, options?: GetDocumentOptions<Fields>): Promise<T>;
    /**
     * Perform a set of index modifications (upload, merge, mergeOrUpload, delete)
     * for the given set of documents.
     * This operation may partially succeed and not all document operations will
     * be reflected in the index. If you would like to treat this as an exception,
     * set the `throwOnAnyFailure` option to true.
     * For more details about how merging works, see: https://docs.microsoft.com/en-us/rest/api/searchservice/AddUpdate-or-Delete-Documents
     * @param batch An array of actions to perform on the index.
     * @param options Additional options.
     */
    indexDocuments(batch: IndexDocumentsBatch<T>, options?: IndexDocumentsOptions): Promise<IndexDocumentsResult>;
    /**
     * Upload an array of documents to the index.
     * @param documents The documents to upload.
     * @param options Additional options.
     */
    uploadDocuments(documents: T[], options?: UploadDocumentsOptions): Promise<IndexDocumentsResult>;
    /**
     * Update a set of documents in the index.
     * For more details about how merging works, see https://docs.microsoft.com/en-us/rest/api/searchservice/AddUpdate-or-Delete-Documents
     * @param documents The updated documents.
     * @param options Additional options.
     */
    mergeDocuments(documents: T[], options?: MergeDocumentsOptions): Promise<IndexDocumentsResult>;
    /**
     * Update a set of documents in the index or upload them if they don't exist.
     * For more details about how merging works, see https://docs.microsoft.com/en-us/rest/api/searchservice/AddUpdate-or-Delete-Documents
     * @param documents The updated documents.
     * @param options Additional options.
     */
    mergeOrUploadDocuments(documents: T[], options?: MergeOrUploadDocumentsOptions): Promise<IndexDocumentsResult>;
    /**
     * Delete a set of documents.
     * @param documents Documents to be deleted.
     * @param options Additional options.
     */
    deleteDocuments(documents: T[], options?: DeleteDocumentsOptions): Promise<IndexDocumentsResult>;
    /**
     * Delete a set of documents.
     * @param keyName The name of their primary key in the index.
     * @param keyValues The primary key values of documents to delete.
     * @param options Additional options.
     */
    deleteDocuments(keyName: keyof T, keyValues: string[], options?: DeleteDocumentsOptions): Promise<IndexDocumentsResult>;
    private encodeContinuationToken;
    private decodeContinuationToken;
    private extractOperationOptions;
    private convertSelect;
    private convertSearchFields;
    private convertOrderBy;
}

/**
 * Client options used to configure Cognitive Search API requests.
 */
export declare type SearchClientOptions = PipelineOptions;

/**
 * Response containing search page results from an index.
 */
export declare interface SearchDocumentsPageResult<T> extends SearchDocumentsResultBase {
    /**
     * The sequence of results returned by the query.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly results: SearchResult<T>[];
    /**
     * A token used for retrieving the next page of results when the server
     * enforces pagination.
     */
    continuationToken?: string;
}

/**
 * Response containing search results from an index.
 */
export declare interface SearchDocumentsResult<T> extends SearchDocumentsResultBase {
    /**
     * The sequence of results returned by the query.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly results: SearchIterator<T>;
}

/**
 * Response containing search results from an index.
 */
export declare interface SearchDocumentsResultBase {
    /**
     * The total count of results found by the search operation, or null if the count was not
     * requested. If present, the count may be greater than the number of results in this response.
     * This can happen if you use the $top or $skip parameters, or if Azure Cognitive Search can't
     * return all the requested documents in a single Search response.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly count?: number;
    /**
     * A value indicating the percentage of the index that was included in the query, or null if
     * minimumCoverage was not specified in the request.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly coverage?: number;
    /**
     * The facet query results for the search operation, organized as a collection of buckets for
     * each faceted field; null if the query did not include any facet expressions.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly facets?: {
        [propertyName: string]: FacetResult[];
    };
}

/**
 * Represents a field in an index definition, which describes the name, data type, and search
 * behavior of a field.
 */
export declare type SearchField = SimpleField | ComplexField;

/**
 * Defines values for SearchFieldDataType.
 * Possible values include: 'Edm.String', 'Edm.Int32', 'Edm.Int64', 'Edm.Double', 'Edm.Boolean',
 * 'Edm.DateTimeOffset', 'Edm.GeographyPoint', 'Collection(Edm.String)',
 * 'Collection(Edm.Int32)', 'Collection(Edm.Int64)', 'Collection(Edm.Double)',
 * 'Collection(Edm.Boolean)', 'Collection(Edm.DateTimeOffset)', 'Collection(Edm.GeographyPoint)'
 * @readonly
 * @enum {string}
 */
export declare type SearchFieldDataType = "Edm.String" | "Edm.Int32" | "Edm.Int64" | "Edm.Double" | "Edm.Boolean" | "Edm.DateTimeOffset" | "Edm.GeographyPoint" | "Collection(Edm.String)" | "Collection(Edm.Int32)" | "Collection(Edm.Int64)" | "Collection(Edm.Double)" | "Collection(Edm.Boolean)" | "Collection(Edm.DateTimeOffset)" | "Collection(Edm.GeographyPoint)";

/**
 * Represents a search index definition, which describes the fields and search behavior of an
 * index.
 */
export declare interface SearchIndex {
    /**
     * The name of the index.
     */
    name: string;
    /**
     * The fields of the index.
     */
    fields: SearchField[];
    /**
     * The scoring profiles for the index.
     */
    scoringProfiles?: ScoringProfile[];
    /**
     * The name of the scoring profile to use if none is specified in the query. If this property is
     * not set and no scoring profile is specified in the query, then default scoring (tf-idf) will
     * be used.
     */
    defaultScoringProfile?: string;
    /**
     * Options to control Cross-Origin Resource Sharing (CORS) for the index.
     */
    corsOptions?: CorsOptions;
    /**
     * The suggesters for the index.
     */
    suggesters?: SearchSuggester[];
    /**
     * The analyzers for the index.
     */
    analyzers?: LexicalAnalyzer[];
    /**
     * The tokenizers for the index.
     */
    tokenizers?: LexicalTokenizer[];
    /**
     * The token filters for the index.
     */
    tokenFilters?: TokenFilter[];
    /**
     * The character filters for the index.
     */
    charFilters?: CharFilter[];
    /**
     * A description of an encryption key that you create in Azure Key Vault. This key is used to
     * provide an additional level of encryption-at-rest for your data when you want full assurance
     * that no one, not even Microsoft, can decrypt your data in Azure Cognitive Search. Once you
     * have encrypted your data, it will always remain encrypted. Azure Cognitive Search will ignore
     * attempts to set this property to null. You can change this property as needed if you want to
     * rotate your encryption key; Your data will be unaffected. Encryption with customer-managed
     * keys is not available for free search services, and is only available for paid services
     * created on or after January 1, 2019.
     */
    encryptionKey?: SearchResourceEncryptionKey;
    /**
     * The type of similarity algorithm to be used when scoring and ranking the documents matching a
     * search query. The similarity algorithm can only be defined at index creation time and cannot
     * be modified on existing indexes. If null, the ClassicSimilarity algorithm is used.
     */
    similarity?: SimilarityAlgorithm;
    /**
     * The ETag of the index.
     */
    etag?: string;
}

/**
 * Class to perform operations to manage
 * (create, update, list/delete)
 * indexes, & synonymmaps.
 */
export declare class SearchIndexClient {
    /**
     * The API version to use when communicating with the service.
     */
    readonly apiVersion: string;
    /**
     * The endpoint of the search service
     */
    readonly endpoint: string;
    /* Excluded from this release type: client */
    /**
     * Used to authenticate requests to the service.
     */
    private readonly credential;
    /**
     * Used to configure the Search Index client.
     */
    private readonly options;
    /**
     * Creates an instance of SearchIndexClient.
     *
     * Example usage:
     * ```ts
     * const { SearchIndexClient, AzureKeyCredential } = require("@azure/search-documents");
     *
     * const client = new SearchIndexClient(
     *   "<endpoint>",
     *   new AzureKeyCredential("<Admin Key>");
     * );
     * ```
     * @param {string} endpoint The endpoint of the search service
     * @param {KeyCredential} credential Used to authenticate requests to the service.
     * @param {SearchIndexClientOptions} [options] Used to configure the Search Index client.
     */
    constructor(endpoint: string, credential: KeyCredential, options?: SearchIndexClientOptions);
    private listIndexesPage;
    private listIndexesAll;
    /**
     * Retrieves a list of existing indexes in the service.
     * @param options Options to the list index operation.
     */
    listIndexes(options?: ListIndexesOptions): IndexIterator;
    private listIndexesNamesPage;
    private listIndexesNamesAll;
    /**
     * Retrieves a list of names of existing indexes in the service.
     * @param options Options to the list index operation.
     */
    listIndexesNames(options?: ListIndexesOptions): IndexNameIterator;
    /**
     * Retrieves a list of existing SynonymMaps in the service.
     * @param options Options to the list SynonymMaps operation.
     */
    listSynonymMaps(options?: ListSynonymMapsOptions): Promise<Array<SynonymMap>>;
    /**
     * Retrieves a list of names of existing SynonymMaps in the service.
     * @param options Options to the list SynonymMaps operation.
     */
    listSynonymMapsNames(options?: ListSynonymMapsOptions): Promise<Array<string>>;
    /**
     * Retrieves information about an index.
     * @param indexName The name of the index.
     * @param options Additional optional arguments.
     */
    getIndex(indexName: string, options?: GetIndexOptions): Promise<SearchIndex>;
    /**
     * Retrieves information about a SynonymMap.
     * @param synonymMapName The name of the SynonymMap.
     * @param options Additional optional arguments.
     */
    getSynonymMap(synonymMapName: string, options?: GetSynonymMapsOptions): Promise<SynonymMap>;
    /**
     * Creates a new index.
     * @param index The information describing the index to be created.
     * @param options Additional optional arguments.
     */
    createIndex(index: SearchIndex, options?: CreateIndexOptions): Promise<SearchIndex>;
    /**
     * Creates a new SynonymMap in a search service.
     * @param synonymMap The synonymMap definition to create in a search service.
     * @param options Additional optional arguments.
     */
    createSynonymMap(synonymMap: SynonymMap, options?: CreateSynonymMapOptions): Promise<SynonymMap>;
    /**
     * Creates a new index or modifies an existing one.
     * @param index The information describing the index to be created.
     * @param options Additional optional arguments.
     */
    createOrUpdateIndex(index: SearchIndex, options?: CreateOrUpdateIndexOptions): Promise<SearchIndex>;
    /**
     * Creates a new SynonymMap or modifies an existing one.
     * @param synonymMap The information describing the SynonymMap to be created.
     * @param options Additional optional arguments.
     */
    createOrUpdateSynonymMap(synonymMap: SynonymMap, options?: CreateOrUpdateSynonymMapOptions): Promise<SynonymMap>;
    /**
     * Deletes an existing index.
     * @param indexName Index/Name of the index to delete.
     * @param options Additional optional arguments.
     */
    deleteIndex(index: string | SearchIndex, options?: DeleteIndexOptions): Promise<void>;
    /**
     * Deletes an existing SynonymMap.
     * @param synonymMapName SynonymMap/Name of the synonymMap to delete.
     * @param options Additional optional arguments.
     */
    deleteSynonymMap(synonymMap: string | SynonymMap, options?: DeleteSynonymMapOptions): Promise<void>;
    /**
     * Retrieves statistics about an index, such as the count of documents and the size
     * of index storage.
     * @param indexName The name of the index.
     * @param options Additional optional arguments.
     */
    getIndexStatistics(indexName: string, options?: GetIndexStatisticsOptions): Promise<SearchIndexStatistics>;
    /**
     * Calls an analyzer or tokenizer manually on provided text.
     * @param indexName The name of the index that contains the field to analyze
     * @param text The text to break into tokens.
     * @param options Additional arguments
     */
    analyzeText(indexName: string, options: AnalyzeTextOptions): Promise<AnalyzeResult>;
    /**
     * Retrieves statistics about the service, such as the count of documents, index, etc.
     * @param options Additional optional arguments.
     */
    getServiceStatistics(options?: GetServiceStatisticsOptions): Promise<SearchServiceStatistics>;
    /**
     * Retrieves the SearchClient corresponding to this SearchIndexClient
     * @param indexName Name of the index
     * @param options SearchClient Options
     */
    getSearchClient<T>(indexName: string, options?: SearchClientOptions): SearchClient<T>;
}

/**
 * Client options used to configure Cognitive Search API requests.
 */
export declare type SearchIndexClientOptions = PipelineOptions;

/**
 * Represents an indexer.
 */
export declare interface SearchIndexer {
    /**
     * The name of the indexer.
     */
    name: string;
    /**
     * The description of the indexer.
     */
    description?: string;
    /**
     * The name of the datasource from which this indexer reads data.
     */
    dataSourceName: string;
    /**
     * The name of the skillset executing with this indexer.
     */
    skillsetName?: string;
    /**
     * The name of the index to which this indexer writes data.
     */
    targetIndexName: string;
    /**
     * The schedule for this indexer.
     */
    schedule?: IndexingSchedule;
    /**
     * Parameters for indexer execution.
     */
    parameters?: IndexingParameters;
    /**
     * Defines mappings between fields in the data source and corresponding target fields in the
     * index.
     */
    fieldMappings?: FieldMapping[];
    /**
     * Output field mappings are applied after enrichment and immediately before indexing.
     */
    outputFieldMappings?: FieldMapping[];
    /**
     * A value indicating whether the indexer is disabled. Default is false. Default value: false.
     */
    isDisabled?: boolean;
    /**
     * The ETag of the indexer.
     */
    etag?: string;
}

/**
 * Class to perform operations to manage
 * (create, update, list/delete)
 * indexers, datasources & skillsets.
 */
export declare class SearchIndexerClient {
    /**
     * The API version to use when communicating with the service.
     */
    readonly apiVersion: string;
    /**
     * The endpoint of the search service
     */
    readonly endpoint: string;
    /* Excluded from this release type: client */
    /**
     * Creates an instance of SearchIndexerClient.
     *
     * Example usage:
     * ```ts
     * const { SearchIndexerClient, AzureKeyCredential } = require("@azure/search-documents");
     *
     * const client = new SearchIndexerClient(
     *   "<endpoint>",
     *   new AzureKeyCredential("<Admin Key>");
     * );
     * ```
     * @param {string} endpoint The endpoint of the search service
     * @param {KeyCredential} credential Used to authenticate requests to the service.
     * @param {SearchIndexerClientOptions} [options] Used to configure the Search client.
     */
    constructor(endpoint: string, credential: KeyCredential, options?: SearchIndexerClientOptions);
    /**
     * Retrieves a list of existing indexers in the service.
     * @param options Options to the list indexers operation.
     */
    listIndexers(options?: ListIndexersOptions): Promise<Array<SearchIndexer>>;
    /**
     * Retrieves a list of names of existing indexers in the service.
     * @param options Options to the list indexers operation.
     */
    listIndexersNames(options?: ListIndexersOptions): Promise<Array<string>>;
    /**
     * Retrieves a list of existing data sources in the service.
     * @param options Options to the list indexers operation.
     */
    listDataSourceConnections(options?: ListDataSourceConnectionsOptions): Promise<Array<SearchIndexerDataSourceConnection>>;
    /**
     * Retrieves a list of names of existing data sources in the service.
     * @param options Options to the list indexers operation.
     */
    listDataSourceConnectionsNames(options?: ListDataSourceConnectionsOptions): Promise<Array<string>>;
    /**
     * Retrieves a list of existing Skillsets in the service.
     * @param options Options to the list Skillsets operation.
     */
    listSkillsets(options?: ListSkillsetsOptions): Promise<Array<SearchIndexerSkillset>>;
    /**
     * Retrieves a list of names of existing Skillsets in the service.
     * @param options Options to the list Skillsets operation.
     */
    listSkillsetsNames(options?: ListSkillsetsOptions): Promise<Array<string>>;
    /**
     * Retrieves information about an Indexer.
     * @param indexerName The name of the Indexer.
     * @param options Additional optional arguments.
     */
    getIndexer(indexerName: string, options?: GetIndexerOptions): Promise<SearchIndexer>;
    /**
     * Retrieves information about a DataSource
     * @param dataSourceName The name of the DataSource
     * @param options Additional optional arguments
     */
    getDataSourceConnection(dataSourceConnectionName: string, options?: GetDataSourceConnectionOptions): Promise<SearchIndexerDataSourceConnection>;
    /**
     * Retrieves information about an Skillset.
     * @param indexName The name of the Skillset.
     * @param options Additional optional arguments.
     */
    getSkillset(skillsetName: string, options?: GetSkillSetOptions): Promise<SearchIndexerSkillset>;
    /**
     * Creates a new indexer in a search service.
     * @param indexer The indexer definition to create in a search service.
     * @param options Additional optional arguments.
     */
    createIndexer(indexer: SearchIndexer, options?: CreateIndexerOptions): Promise<SearchIndexer>;
    /**
     * Creates a new dataSource in a search service.
     * @param dataSourceConnection The dataSource definition to create in a search service.
     * @param options Additional optional arguments.
     */
    createDataSourceConnection(dataSourceConnection: SearchIndexerDataSourceConnection, options?: CreateDataSourceConnectionOptions): Promise<SearchIndexerDataSourceConnection>;
    /**
     * Creates a new skillset in a search service.
     * @param skillset The skillset containing one or more skills to create in a search service.
     * @param options Additional optional arguments.
     */
    createSkillset(skillset: SearchIndexerSkillset, options?: CreateSkillsetOptions): Promise<SearchIndexerSkillset>;
    /**
     * Creates a new indexer or modifies an existing one.
     * @param indexer The information describing the indexer to be created/updated.
     * @param options Additional optional arguments.
     */
    createOrUpdateIndexer(indexer: SearchIndexer, options?: CreateorUpdateIndexerOptions): Promise<SearchIndexer>;
    /**
     * Creates a new datasource or modifies an existing one.
     * @param dataSourceConnection The information describing the datasource to be created/updated.
     * @param options Additional optional arguments.
     */
    createOrUpdateDataSourceConnection(dataSourceConnection: SearchIndexerDataSourceConnection, options?: CreateorUpdateDataSourceConnectionOptions): Promise<SearchIndexerDataSourceConnection>;
    /**
     * Creates a new Skillset or modifies an existing one.
     * @param skillset The information describing the index to be created.
     * @param options Additional optional arguments.
     */
    createOrUpdateSkillset(skillset: SearchIndexerSkillset, options?: CreateOrUpdateSkillsetOptions): Promise<SearchIndexerSkillset>;
    /**
     * Deletes an existing indexer.
     * @param indexer Indexer/Name of the indexer to delete.
     * @param options Additional optional arguments.
     */
    deleteIndexer(indexer: string | SearchIndexer, options?: DeleteIndexerOptions): Promise<void>;
    /**
     * Deletes an existing datasource.
     * @param dataSource Datasource/Name of the datasource to delete.
     * @param options Additional optional arguments.
     */
    deleteDataSourceConnection(dataSourceConnection: string | SearchIndexerDataSourceConnection, options?: DeleteDataSourceConnectionOptions): Promise<void>;
    /**
     * Deletes an existing Skillset.
     * @param skillset Skillset/Name of the Skillset to delete.
     * @param options Additional optional arguments.
     */
    deleteSkillset(skillset: string | SearchIndexerSkillset, options?: DeleteSkillsetOptions): Promise<void>;
    /**
     * Returns the current status and execution history of an indexer.
     * @param indexerName The name of the indexer.
     * @param options Additional optional arguments.
     */
    getIndexerStatus(indexerName: string, options?: GetIndexerStatusOptions): Promise<SearchIndexerStatus>;
    /**
     * Resets the change tracking state associated with an indexer.
     * @param indexerName The name of the indexer to reset.
     * @param options Additional optional arguments.
     */
    resetIndexer(indexerName: string, options?: ResetIndexerOptions): Promise<void>;
    /**
     * Runs an indexer on-demand.
     * @param indexerName The name of the indexer to run.
     * @param options Additional optional arguments.
     */
    runIndexer(indexerName: string, options?: RunIndexerOptions): Promise<void>;
}

/**
 * Client options used to configure Cognitive Search API requests.
 */
export declare type SearchIndexerClientOptions = PipelineOptions;

/**
 * Represents information about the entity (such as Azure SQL table or CosmosDB collection) that
 * will be indexed.
 */
export declare interface SearchIndexerDataContainer {
    /**
     * The name of the table or view (for Azure SQL data source) or collection (for CosmosDB data
     * source) that will be indexed.
     */
    name: string;
    /**
     * A query that is applied to this data container. The syntax and meaning of this parameter is
     * datasource-specific. Not supported by Azure SQL datasources.
     */
    query?: string;
}

/**
 * Represents a datasource definition, which can be used to configure an indexer.
 */
export declare interface SearchIndexerDataSourceConnection {
    /**
     * The name of the datasource.
     */
    name: string;
    /**
     * The description of the datasource.
     */
    description?: string;
    /**
     * The type of the datasource. Possible values include: 'AzureSql', 'CosmosDb', 'AzureBlob',
     * 'AzureTable', 'MySql'
     */
    type: SearchIndexerDataSourceType;
    /**
     * The connection string for the datasource.
     */
    connectionString?: string;
    /**
     * The data container for the datasource.
     */
    container: SearchIndexerDataContainer;
    /**
     * The data change detection policy for the datasource.
     */
    dataChangeDetectionPolicy?: DataChangeDetectionPolicy;
    /**
     * The data deletion detection policy for the datasource.
     */
    dataDeletionDetectionPolicy?: DataDeletionDetectionPolicy;
    /**
     * The ETag of the DataSource.
     */
    etag?: string;
}

/**
 * Defines values for SearchIndexerDataSourceType.
 * Possible values include: 'AzureSql', 'CosmosDb', 'AzureBlob', 'AzureTable', 'MySql'
 * @readonly
 * @enum {string}
 */
export declare type SearchIndexerDataSourceType = 'azuresql' | 'cosmosdb' | 'azureblob' | 'azuretable' | 'mysql';

/**
 * Represents an item- or document-level indexing error.
 */
export declare interface SearchIndexerError {
    /**
     * The key of the item for which indexing failed.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly key?: string;
    /**
     * The message describing the error that occurred while processing the item.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly errorMessage: string;
    /**
     * The status code indicating why the indexing operation failed. Possible values include: 400 for
     * a malformed input document, 404 for document not found, 409 for a version conflict, 422 when
     * the index is temporarily unavailable, or 503 for when the service is too busy.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly statusCode: number;
    /**
     * The name of the source at which the error originated. For example, this could refer to a
     * particular skill in the attached skillset. This may not be always available.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly name?: string;
    /**
     * Additional, verbose details about the error to assist in debugging the indexer. This may not
     * be always available.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly details?: string;
    /**
     * A link to a troubleshooting guide for these classes of errors. This may not be always
     * available.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly documentationLink?: string;
}

/**
 * An interface representing SearchIndexerLimits.
 */
export declare interface SearchIndexerLimits {
    /**
     * The maximum duration that the indexer is permitted to run for one execution.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly maxRunTime?: string;
    /**
     * The maximum size of a document, in bytes, which will be considered valid for indexing.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly maxDocumentExtractionSize?: number;
    /**
     * The maximum number of characters that will be extracted from a document picked up for
     * indexing.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly maxDocumentContentCharactersToExtract?: number;
}

/**
 * Contains the possible cases for Skill.
 */
export declare type SearchIndexerSkill = ConditionalSkill | KeyPhraseExtractionSkill | OcrSkill | ImageAnalysisSkill | LanguageDetectionSkill | ShaperSkill | MergeSkill | EntityRecognitionSkill | SentimentSkill | SplitSkill | TextTranslationSkill | WebApiSkill;

/**
 * A list of skills.
 */
export declare interface SearchIndexerSkillset {
    /**
     * The name of the skillset.
     */
    name: string;
    /**
     * The description of the skillset.
     */
    description?: string;
    /**
     * A list of skills in the skillset.
     */
    skills: SearchIndexerSkill[];
    /**
     * Details about cognitive services to be used when running skills.
     */
    cognitiveServicesAccount?: CognitiveServicesAccount;
    /**
     * The ETag of the skillset.
     */
    etag?: string;
}

/**
 * Represents the current status and execution history of an indexer.
 */
export declare interface SearchIndexerStatus {
    /**
     * Overall indexer status. Possible values include: 'Unknown', 'Error', 'Running'
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly status: IndexerStatus;
    /**
     * The result of the most recent or an in-progress indexer execution.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly lastResult?: IndexerExecutionResult;
    /**
     * History of the recent indexer executions, sorted in reverse chronological order.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly executionHistory: IndexerExecutionResult[];
    /**
     * The execution limits for the indexer.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly limits: SearchIndexerLimits;
}

/**
 * Represents an item-level warning.
 */
export declare interface SearchIndexerWarning {
    /**
     * The key of the item which generated a warning.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly key?: string;
    /**
     * The message describing the warning that occurred while processing the item.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly message: string;
    /**
     * The name of the source at which the warning originated. For example, this could refer to a
     * particular skill in the attached skillset. This may not be always available.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly name?: string;
    /**
     * Additional, verbose details about the warning to assist in debugging the indexer. This may not
     * be always available.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly details?: string;
    /**
     * A link to a troubleshooting guide for these classes of warnings. This may not be always
     * available.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly documentationLink?: string;
}

/**
 * Statistics for a given index. Statistics are collected periodically and are not guaranteed to
 * always be up-to-date.
 */
export declare interface SearchIndexStatistics {
    /**
     * The number of documents in the index.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly documentCount: number;
    /**
     * The amount of storage in bytes consumed by the index.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly storageSize: number;
}

/**
 * An iterator for search results of a paticular query. Will make requests
 * as needed during iteration. Use .byPage() to make one request to the server
 * per iteration.
 */
export declare type SearchIterator<Fields> = PagedAsyncIterableIterator<SearchResult<Fields>, SearchDocumentsPageResult<Fields>, ListSearchResultsPageSettings>;

/**
 * Defines values for SearchMode.
 * Possible values include: 'Any', 'All'
 * @readonly
 * @enum {string}
 */
export declare type SearchMode = 'any' | 'all';

/**
 * Options for committing a full search request.
 */
export declare type SearchOptions<Fields> = OperationOptions & SearchRequestOptions<Fields>;

/**
 * Parameters for filtering, sorting, faceting, paging, and other search query behaviors.
 */
export declare interface SearchRequest {
    /**
     * A value that specifies whether to fetch the total count of results. Default is false. Setting
     * this value to true may have a performance impact. Note that the count returned is an
     * approximation.
     */
    includeTotalCount?: boolean;
    /**
     * The list of facet expressions to apply to the search query. Each facet expression contains a
     * field name, optionally followed by a comma-separated list of name:value pairs.
     */
    facets?: string[];
    /**
     * The OData $filter expression to apply to the search query.
     */
    filter?: string;
    /**
     * The comma-separated list of field names to use for hit highlights. Only searchable fields can
     * be used for hit highlighting.
     */
    highlightFields?: string;
    /**
     * A string tag that is appended to hit highlights. Must be set with highlightPreTag. Default is
     * &lt;/em&gt;.
     */
    highlightPostTag?: string;
    /**
     * A string tag that is prepended to hit highlights. Must be set with highlightPostTag. Default
     * is &lt;em&gt;.
     */
    highlightPreTag?: string;
    /**
     * A number between 0 and 100 indicating the percentage of the index that must be covered by a
     * search query in order for the query to be reported as a success. This parameter can be useful
     * for ensuring search availability even for services with only one replica. The default is 100.
     */
    minimumCoverage?: number;
    /**
     * The comma-separated list of OData $orderby expressions by which to sort the results. Each
     * expression can be either a field name or a call to either the geo.distance() or the
     * search.score() functions. Each expression can be followed by asc to indicate ascending, or
     * desc to indicate descending. The default is ascending order. Ties will be broken by the match
     * scores of documents. If no $orderby is specified, the default sort order is descending by
     * document match score. There can be at most 32 $orderby clauses.
     */
    orderBy?: string;
    /**
     * A value that specifies the syntax of the search query. The default is 'simple'. Use 'full' if
     * your query uses the Lucene query syntax. Possible values include: 'Simple', 'Full'
     */
    queryType?: QueryType;
    /**
     * The list of parameter values to be used in scoring functions (for example,
     * referencePointParameter) using the format name-values. For example, if the scoring profile
     * defines a function with a parameter called 'mylocation' the parameter string would be
     * "mylocation--122.2,44.8" (without the quotes).
     */
    scoringParameters?: string[];
    /**
     * The name of a scoring profile to evaluate match scores for matching documents in order to sort
     * the results.
     */
    scoringProfile?: string;
    /**
     * A full-text search query expression; Use "*" or omit this parameter to match all documents.
     */
    searchText?: string;
    /**
     * The comma-separated list of field names to which to scope the full-text search. When using
     * fielded search (fieldName:searchExpression) in a full Lucene query, the field names of each
     * fielded search expression take precedence over any field names listed in this parameter.
     */
    searchFields?: string;
    /**
     * A value that specifies whether any or all of the search terms must be matched in order to
     * count the document as a match. Possible values include: 'Any', 'All'
     */
    searchMode?: SearchMode;
    /**
     * The comma-separated list of fields to retrieve. If unspecified, all fields marked as
     * retrievable in the schema are included.
     */
    select?: string;
    /**
     * The number of search results to skip. This value cannot be greater than 100,000. If you need
     * to scan documents in sequence, but cannot use skip due to this limitation, consider using
     * orderby on a totally-ordered key and filter with a range query instead.
     */
    skip?: number;
    /**
     * The number of search results to retrieve. This can be used in conjunction with $skip to
     * implement client-side paging of search results. If results are truncated due to server-side
     * paging, the response will include a continuation token that can be used to issue another
     * Search request for the next page of results.
     */
    top?: number;
}

/**
 * Parameters for filtering, sorting, faceting, paging, and other search query behaviors.
 */
export declare interface SearchRequestOptions<Fields> {
    /**
     * A value that specifies whether to fetch the total count of results. Default is false. Setting
     * this value to true may have a performance impact. Note that the count returned is an
     * approximation.
     */
    includeTotalCount?: boolean;
    /**
     * The list of facet expressions to apply to the search query. Each facet expression contains a
     * field name, optionally followed by a comma-separated list of name:value pairs.
     */
    facets?: string[];
    /**
     * The OData $filter expression to apply to the search query.
     */
    filter?: string;
    /**
     * The comma-separated list of field names to use for hit highlights. Only searchable fields can
     * be used for hit highlighting.
     */
    highlightFields?: string;
    /**
     * A string tag that is appended to hit highlights. Must be set with highlightPreTag. Default is
     * &lt;/em&gt;.
     */
    highlightPostTag?: string;
    /**
     * A string tag that is prepended to hit highlights. Must be set with highlightPostTag. Default
     * is &lt;em&gt;.
     */
    highlightPreTag?: string;
    /**
     * A number between 0 and 100 indicating the percentage of the index that must be covered by a
     * search query in order for the query to be reported as a success. This parameter can be useful
     * for ensuring search availability even for services with only one replica. The default is 100.
     */
    minimumCoverage?: number;
    /**
     * The list of OData $orderby expressions by which to sort the results. Each
     * expression can be either a field name or a call to either the geo.distance() or the
     * search.score() functions. Each expression can be followed by asc to indicate ascending, or
     * desc to indicate descending. The default is ascending order. Ties will be broken by the match
     * scores of documents. If no $orderby is specified, the default sort order is descending by
     * document match score. There can be at most 32 $orderby clauses.
     */
    orderBy?: string[];
    /**
     * A value that specifies the syntax of the search query. The default is 'simple'. Use 'full' if
     * your query uses the Lucene query syntax. Possible values include: 'simple', 'full'
     */
    queryType?: QueryType;
    /**
     * The list of parameter values to be used in scoring functions (for example,
     * referencePointParameter) using the format name-values. For example, if the scoring profile
     * defines a function with a parameter called 'mylocation' the parameter string would be
     * "mylocation--122.2,44.8" (without the quotes).
     */
    scoringParameters?: string[];
    /**
     * The name of a scoring profile to evaluate match scores for matching documents in order to sort
     * the results.
     */
    scoringProfile?: string;
    /**
     * The comma-separated list of field names to which to scope the full-text search. When using
     * fielded search (fieldName:searchExpression) in a full Lucene query, the field names of each
     * fielded search expression take precedence over any field names listed in this parameter.
     */
    searchFields?: Fields[];
    /**
     * A value that specifies whether any or all of the search terms must be matched in order to
     * count the document as a match. Possible values include: 'any', 'all'
     */
    searchMode?: SearchMode;
    /**
     * The list of fields to retrieve. If unspecified, all fields marked as
     * retrievable in the schema are included.
     */
    select?: Fields[];
    /**
     * The number of search results to skip. This value cannot be greater than 100,000. If you need
     * to scan documents in sequence, but cannot use skip due to this limitation, consider using
     * orderby on a totally-ordered key and filter with a range query instead.
     */
    skip?: number;
    /**
     * The number of search results to retrieve. This can be used in conjunction with $skip to
     * implement client-side paging of search results. If results are truncated due to server-side
     * paging, the response will include a continuation token that can be used to issue another
     * Search request for the next page of results.
     */
    top?: number;
}

/**
 * A customer-managed encryption key in Azure Key Vault. Keys that you create and manage can be
 * used to encrypt or decrypt data-at-rest in Azure Cognitive Search, such as indexes and synonym
 * maps.
 */
export declare interface SearchResourceEncryptionKey {
    /**
     * The name of your Azure Key Vault key to be used to encrypt your data at rest.
     */
    keyName: string;
    /**
     * The version of your Azure Key Vault key to be used to encrypt your data at rest.
     */
    keyVersion: string;
    /**
     * The URI of your Azure Key Vault, also referred to as DNS name, that contains the key to be
     * used to encrypt your data at rest. An example URI might be
     * https://my-keyvault-name.vault.azure.net.
     */
    vaultUrl: string;
    /**
     * An AAD Application ID that was granted the required access permissions to the Azure Key Vault
     * that is to be used when encrypting your data at rest. The Application ID should not be
     * confused with the Object ID for your AAD Application.
     */
    applicationId?: string;
    /**
     * The authentication key of the specified AAD application.
     */
    applicationSecret?: string;
}

/**
 * Contains a document found by a search query, plus associated metadata.
 */
export declare type SearchResult<T> = {
    /**
     * The relevance score of the document compared to other documents returned by the query.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly score: number;
    /**
     * Text fragments from the document that indicate the matching search terms, organized by each
     * applicable field; null if hit highlighting was not enabled for the query.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly highlights?: {
        [propertyName: string]: string[];
    };
    document: T;
};

/**
 * Response from a get service statistics request. If successful, it includes service level
 * counters and limits.
 */
export declare interface SearchServiceStatistics {
    /**
     * Service level resource counters.
     */
    counters: ServiceCounters;
    /**
     * Service level general limits.
     */
    limits: ServiceLimits;
}

/**
 * Defines how the Suggest API should apply to a group of fields in the index.
 */
export declare interface SearchSuggester {
    /**
     * The name of the suggester.
     */
    name: string;
    /**
     * The list of field names to which the suggester applies. Each field must be searchable.
     */
    sourceFields: string[];
}

/**
 * Text analytics positive-negative sentiment analysis, scored as a floating point value in a range
 * of zero to 1.
 */
export declare interface SentimentSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Text.SentimentSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * A value indicating which language code to use. Default is en. Possible values include: 'da',
     * 'nl', 'en', 'fi', 'fr', 'de', 'el', 'it', 'no', 'pl', 'pt-PT', 'ru', 'es', 'sv', 'tr'
     */
    defaultLanguageCode?: SentimentSkillLanguage;
}

/**
 * Defines values for SentimentSkillLanguage.
 * Possible values include: 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'it', 'no', 'pl', 'pt-PT',
 * 'ru', 'es', 'sv', 'tr'
 * @readonly
 * @enum {string}
 */
export declare type SentimentSkillLanguage = 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'it' | 'no' | 'pl' | 'pt-PT' | 'ru' | 'es' | 'sv' | 'tr';

/**
 * Represents service-level resource counters and quotas.
 */
export declare interface ServiceCounters {
    /**
     * Total number of documents across all indexes in the service.
     */
    documentCounter: ResourceCounter;
    /**
     * Total number of indexes.
     */
    indexCounter: ResourceCounter;
    /**
     * Total number of indexers.
     */
    indexerCounter: ResourceCounter;
    /**
     * Total number of data sources.
     */
    dataSourceCounter: ResourceCounter;
    /**
     * Total size of used storage in bytes.
     */
    storageSizeCounter: ResourceCounter;
    /**
     * Total number of synonym maps.
     */
    synonymMapCounter: ResourceCounter;
}

/**
 * Represents various service level limits.
 */
export declare interface ServiceLimits {
    /**
     * The maximum allowed fields per index.
     */
    maxFieldsPerIndex?: number;
    /**
     * The maximum depth which you can nest sub-fields in an index, including the top-level complex
     * field. For example, a/b/c has a nesting depth of 3.
     */
    maxFieldNestingDepthPerIndex?: number;
    /**
     * The maximum number of fields of type Collection(Edm.ComplexType) allowed in an index.
     */
    maxComplexCollectionFieldsPerIndex?: number;
    /**
     * The maximum number of objects in complex collections allowed per document.
     */
    maxComplexObjectsInCollectionsPerDocument?: number;
}

/**
 * A skill for reshaping the outputs. It creates a complex type to support composite fields (also
 * known as multipart fields).
 */
export declare interface ShaperSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Util.ShaperSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
}

/**
 * Creates combinations of tokens as a single token. This token filter is implemented using Apache
 * Lucene.
 */
export declare interface ShingleTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.ShingleTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The maximum shingle size. Default and minimum value is 2. Default value: 2.
     */
    maxShingleSize?: number;
    /**
     * The minimum shingle size. Default and minimum value is 2. Must be less than the value of
     * maxShingleSize. Default value: 2.
     */
    minShingleSize?: number;
    /**
     * A value indicating whether the output stream will contain the input tokens (unigrams) as well
     * as shingles. Default is true. Default value: true.
     */
    outputUnigrams?: boolean;
    /**
     * A value indicating whether to output unigrams for those times when no shingles are available.
     * This property takes precedence when outputUnigrams is set to false. Default is false. Default
     * value: false.
     */
    outputUnigramsIfNoShingles?: boolean;
    /**
     * The string to use when joining adjacent tokens to form a shingle. Default is a single space ("
     * "). Default value: ''.
     */
    tokenSeparator?: string;
    /**
     * The string to insert for each position at which there is no token. Default is an underscore
     * ("_"). Default value: '_'.
     */
    filterToken?: string;
}

/**
 * Contains the possible cases for Similarity.
 */
export declare type SimilarityAlgorithm = ClassicSimilarity | BM25Similarity;

/**
 * Represents a field in an index definition, which describes the name, data type, and search
 * behavior of a field.
 */
export declare interface SimpleField {
    /**
     * The name of the field, which must be unique within the fields collection of the index or
     * parent field.
     */
    name: string;
    /**
     * The data type of the field. Possible values include: 'Edm.String', 'Edm.Int32', 'Edm.Int64',
     * 'Edm.Double', 'Edm.Boolean', 'Edm.DateTimeOffset', 'Edm.GeographyPoint'
     * 'Collection(Edm.String)', 'Collection(Edm.Int32)', 'Collection(Edm.Int64)',
     * 'Collection(Edm.Double)', 'Collection(Edm.Boolean)', 'Collection(Edm.DateTimeOffset)',
     * 'Collection(Edm.GeographyPoint)'
     */
    type: SearchFieldDataType;
    /**
     * A value indicating whether the field uniquely identifies documents in the index. Exactly one
     * top-level field in each index must be chosen as the key field and it must be of type
     * Edm.String. Key fields can be used to look up documents directly and update or delete specific
     * documents. Default is false.
     */
    key?: boolean;
    /**
     * A value indicating whether the field can be returned in a search result. You can enable this
     * option if you want to use a field (for example, margin) as a filter, sorting, or scoring
     * mechanism but do not want the field to be visible to the end user. This property must be false
     * for key fields. This property can be changed on existing fields.
     * Disabling this property does not cause any increase in index storage requirements.
     * Default is false.
     */
    hidden?: boolean;
    /**
     * A value indicating whether the field is full-text searchable. This means it will undergo
     * analysis such as word-breaking during indexing. If you set a searchable field to a value like
     * "sunny day", internally it will be split into the individual tokens "sunny" and "day". This
     * enables full-text searches for these terms. This property must be false for simple
     * fields of other non-string data types.
     * Note: searchable fields consume extra space in your index since Azure Cognitive Search will store an
     * additional tokenized version of the field value for full-text searches.
     * Defaults to false for simple fields.
     */
    searchable?: boolean;
    /**
     * A value indicating whether to enable the field to be referenced in $filter queries. Filterable
     * differs from searchable in how strings are handled. Fields of type Edm.String or
     * Collection(Edm.String) that are filterable do not undergo word-breaking, so comparisons are
     * for exact matches only. For example, if you set such a field f to "sunny day", $filter=f eq
     * 'sunny' will find no matches, but $filter=f eq 'sunny day' will.
     * Default is false.
     */
    filterable?: boolean;
    /**
     * A value indicating whether to enable the field to be referenced in $orderby expressions. By
     * default Azure Cognitive Search sorts results by score, but in many experiences users will want
     * to sort by fields in the documents. A simple field can be sortable only if it is single-valued
     * (it has a single value in the scope of the parent document). Simple collection fields cannot
     * be sortable, since they are multi-valued. Simple sub-fields of complex collections are also
     * multi-valued, and therefore cannot be sortable. This is true whether it's an immediate parent
     * field, or an ancestor field, that's the complex collection. The default for sortable is false.
     */
    sortable?: boolean;
    /**
     * A value indicating whether to enable the field to be referenced in facet queries. Typically
     * used in a presentation of search results that includes hit count by category (for example,
     * search for digital cameras and see hits by brand, by megapixels, by price, and so on).
     * Fields of type Edm.GeographyPoint or Collection(Edm.GeographyPoint) cannot be facetable.
     * Default is false for all other simple fields.
     */
    facetable?: boolean;
    /**
     * The name of the language analyzer to use for the field. This option can be used only with
     * searchable fields and it can't be set together with either searchAnalyzer or indexAnalyzer.
     * Once the analyzer is chosen, it cannot be changed for the field.
     * KnownAnalyzerNames is an enum containing known values.
     */
    analyzerName?: LexicalAnalyzerName;
    /**
     * The name of the analyzer used at search time for the field. This option can be used only with
     * searchable fields. It must be set together with indexAnalyzer and it cannot be set together
     * with the analyzer option. This analyzer can be updated on an existing field.
     * KnownAnalyzerNames is an enum containing known values.
     */
    searchAnalyzerName?: LexicalAnalyzerName;
    /**
     * The name of the analyzer used at indexing time for the field. This option can be used only
     * with searchable fields. It must be set together with searchAnalyzer and it cannot be set
     * together with the analyzer option. Once the analyzer is chosen, it cannot be changed for the
     * field. KnownAnalyzerNames is an enum containing known values.
     */
    indexAnalyzerName?: LexicalAnalyzerName;
    /**
     * A list of the names of synonym maps to associate with this field. This option can be used only
     * with searchable fields. Currently only one synonym map per field is supported. Assigning a
     * synonym map to a field ensures that query terms targeting that field are expanded at
     * query-time using the rules in the synonym map. This attribute can be changed on existing
     * fields.
     */
    synonymMapNames?: string[];
}

/**
 * A filter that stems words using a Snowball-generated stemmer. This token filter is implemented
 * using Apache Lucene.
 */
export declare interface SnowballTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.SnowballTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The language to use. Possible values include: 'Armenian', 'Basque', 'Catalan', 'Danish',
     * 'Dutch', 'English', 'Finnish', 'French', 'German', 'German2', 'Hungarian', 'Italian', 'Kp',
     * 'Lovins', 'Norwegian', 'Porter', 'Portuguese', 'Romanian', 'Russian', 'Spanish', 'Swedish',
     * 'Turkish'
     */
    language: SnowballTokenFilterLanguage;
}

/**
 * Defines values for SnowballTokenFilterLanguage.
 * Possible values include: 'Armenian', 'Basque', 'Catalan', 'Danish', 'Dutch', 'English',
 * 'Finnish', 'French', 'German', 'German2', 'Hungarian', 'Italian', 'Kp', 'Lovins', 'Norwegian',
 * 'Porter', 'Portuguese', 'Romanian', 'Russian', 'Spanish', 'Swedish', 'Turkish'
 * @readonly
 * @enum {string}
 */
export declare type SnowballTokenFilterLanguage = 'armenian' | 'basque' | 'catalan' | 'danish' | 'dutch' | 'english' | 'finnish' | 'french' | 'german' | 'german2' | 'hungarian' | 'italian' | 'kp' | 'lovins' | 'norwegian' | 'porter' | 'portuguese' | 'romanian' | 'russian' | 'spanish' | 'swedish' | 'turkish';

/**
 * Defines a data deletion detection policy that implements a soft-deletion strategy. It determines
 * whether an item should be deleted based on the value of a designated 'soft delete' column.
 */
export declare interface SoftDeleteColumnDeletionDetectionPolicy {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy";
    /**
     * The name of the column to use for soft-deletion detection.
     */
    softDeleteColumnName?: string;
    /**
     * The marker value that identifies an item as deleted.
     */
    softDeleteMarkerValue?: string;
}

/**
 * A skill to split a string into chunks of text.
 */
export declare interface SplitSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Text.SplitSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * A value indicating which language code to use. Default is en. Possible values include: 'da',
     * 'de', 'en', 'es', 'fi', 'fr', 'it', 'ko', 'pt'
     */
    defaultLanguageCode?: SplitSkillLanguage;
    /**
     * A value indicating which split mode to perform. Possible values include: 'Pages', 'Sentences'
     */
    textSplitMode?: TextSplitMode;
    /**
     * The desired maximum page length. Default is 10000.
     */
    maxPageLength?: number;
}

/**
 * Defines values for SplitSkillLanguage.
 * Possible values include: 'da', 'de', 'en', 'es', 'fi', 'fr', 'it', 'ko', 'pt'
 * @readonly
 * @enum {string}
 */
export declare type SplitSkillLanguage = 'da' | 'de' | 'en' | 'es' | 'fi' | 'fr' | 'it' | 'ko' | 'pt';

/**
 * Defines a data change detection policy that captures changes using the Integrated Change
 * Tracking feature of Azure SQL Database.
 */
export declare interface SqlIntegratedChangeTrackingPolicy {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.SqlIntegratedChangeTrackingPolicy";
}

/**
 * Provides the ability to override other stemming filters with custom dictionary-based stemming.
 * Any dictionary-stemmed terms will be marked as keywords so that they will not be stemmed with
 * stemmers down the chain. Must be placed before any stemming filters. This token filter is
 * implemented using Apache Lucene.
 */
export declare interface StemmerOverrideTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.StemmerOverrideTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A list of stemming rules in the following format: "word => stem", for example: "ran => run".
     */
    rules: string[];
}

/**
 * Language specific stemming filter. This token filter is implemented using Apache Lucene.
 */
export declare interface StemmerTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.StemmerTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The language to use. Possible values include: 'Arabic', 'Armenian', 'Basque', 'Brazilian',
     * 'Bulgarian', 'Catalan', 'Czech', 'Danish', 'Dutch', 'DutchKp', 'English', 'LightEnglish',
     * 'MinimalEnglish', 'PossessiveEnglish', 'Porter2', 'Lovins', 'Finnish', 'LightFinnish',
     * 'French', 'LightFrench', 'MinimalFrench', 'Galician', 'MinimalGalician', 'German', 'German2',
     * 'LightGerman', 'MinimalGerman', 'Greek', 'Hindi', 'Hungarian', 'LightHungarian', 'Indonesian',
     * 'Irish', 'Italian', 'LightItalian', 'Sorani', 'Latvian', 'Norwegian', 'LightNorwegian',
     * 'MinimalNorwegian', 'LightNynorsk', 'MinimalNynorsk', 'Portuguese', 'LightPortuguese',
     * 'MinimalPortuguese', 'PortugueseRslp', 'Romanian', 'Russian', 'LightRussian', 'Spanish',
     * 'LightSpanish', 'Swedish', 'LightSwedish', 'Turkish'
     */
    language: StemmerTokenFilterLanguage;
}

/**
 * Defines values for StemmerTokenFilterLanguage.
 * Possible values include: 'Arabic', 'Armenian', 'Basque', 'Brazilian', 'Bulgarian', 'Catalan',
 * 'Czech', 'Danish', 'Dutch', 'DutchKp', 'English', 'LightEnglish', 'MinimalEnglish',
 * 'PossessiveEnglish', 'Porter2', 'Lovins', 'Finnish', 'LightFinnish', 'French', 'LightFrench',
 * 'MinimalFrench', 'Galician', 'MinimalGalician', 'German', 'German2', 'LightGerman',
 * 'MinimalGerman', 'Greek', 'Hindi', 'Hungarian', 'LightHungarian', 'Indonesian', 'Irish',
 * 'Italian', 'LightItalian', 'Sorani', 'Latvian', 'Norwegian', 'LightNorwegian',
 * 'MinimalNorwegian', 'LightNynorsk', 'MinimalNynorsk', 'Portuguese', 'LightPortuguese',
 * 'MinimalPortuguese', 'PortugueseRslp', 'Romanian', 'Russian', 'LightRussian', 'Spanish',
 * 'LightSpanish', 'Swedish', 'LightSwedish', 'Turkish'
 * @readonly
 * @enum {string}
 */
export declare type StemmerTokenFilterLanguage = 'arabic' | 'armenian' | 'basque' | 'brazilian' | 'bulgarian' | 'catalan' | 'czech' | 'danish' | 'dutch' | 'dutchKp' | 'english' | 'lightEnglish' | 'minimalEnglish' | 'possessiveEnglish' | 'porter2' | 'lovins' | 'finnish' | 'lightFinnish' | 'french' | 'lightFrench' | 'minimalFrench' | 'galician' | 'minimalGalician' | 'german' | 'german2' | 'lightGerman' | 'minimalGerman' | 'greek' | 'hindi' | 'hungarian' | 'lightHungarian' | 'indonesian' | 'irish' | 'italian' | 'lightItalian' | 'sorani' | 'latvian' | 'norwegian' | 'lightNorwegian' | 'minimalNorwegian' | 'lightNynorsk' | 'minimalNynorsk' | 'portuguese' | 'lightPortuguese' | 'minimalPortuguese' | 'portugueseRslp' | 'romanian' | 'russian' | 'lightRussian' | 'spanish' | 'lightSpanish' | 'swedish' | 'lightSwedish' | 'turkish';

/**
 * Divides text at non-letters; Applies the lowercase and stopword token filters. This analyzer is
 * implemented using Apache Lucene.
 */
export declare interface StopAnalyzer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.StopAnalyzer";
    /**
     * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
     * can only start and end with alphanumeric characters, and is limited to 128 characters.
     */
    name: string;
    /**
     * A list of stopwords.
     */
    stopwords?: string[];
}

/**
 * Defines values for StopwordsList.
 * Possible values include: 'Arabic', 'Armenian', 'Basque', 'Brazilian', 'Bulgarian', 'Catalan',
 * 'Czech', 'Danish', 'Dutch', 'English', 'Finnish', 'French', 'Galician', 'German', 'Greek',
 * 'Hindi', 'Hungarian', 'Indonesian', 'Irish', 'Italian', 'Latvian', 'Norwegian', 'Persian',
 * 'Portuguese', 'Romanian', 'Russian', 'Sorani', 'Spanish', 'Swedish', 'Thai', 'Turkish'
 * @readonly
 * @enum {string}
 */
export declare type StopwordsList = 'arabic' | 'armenian' | 'basque' | 'brazilian' | 'bulgarian' | 'catalan' | 'czech' | 'danish' | 'dutch' | 'english' | 'finnish' | 'french' | 'galician' | 'german' | 'greek' | 'hindi' | 'hungarian' | 'indonesian' | 'irish' | 'italian' | 'latvian' | 'norwegian' | 'persian' | 'portuguese' | 'romanian' | 'russian' | 'sorani' | 'spanish' | 'swedish' | 'thai' | 'turkish';

/**
 * Removes stop words from a token stream. This token filter is implemented using Apache Lucene.
 */
export declare interface StopwordsTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.StopwordsTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The list of stopwords. This property and the stopwords list property cannot both be set.
     */
    stopwords?: string[];
    /**
     * A predefined list of stopwords to use. This property and the stopwords property cannot both be
     * set. Default is English. Possible values include: 'Arabic', 'Armenian', 'Basque', 'Brazilian',
     * 'Bulgarian', 'Catalan', 'Czech', 'Danish', 'Dutch', 'English', 'Finnish', 'French',
     * 'Galician', 'German', 'Greek', 'Hindi', 'Hungarian', 'Indonesian', 'Irish', 'Italian',
     * 'Latvian', 'Norwegian', 'Persian', 'Portuguese', 'Romanian', 'Russian', 'Sorani', 'Spanish',
     * 'Swedish', 'Thai', 'Turkish'
     */
    stopwordsList?: StopwordsList;
    /**
     * A value indicating whether to ignore case. If true, all words are converted to lower case
     * first. Default is false. Default value: false.
     */
    ignoreCase?: boolean;
    /**
     * A value indicating whether to ignore the last search term if it's a stop word. Default is
     * true. Default value: true.
     */
    removeTrailingStopWords?: boolean;
}

/**
 * Response containing suggestion query results from an index.
 */
export declare interface SuggestDocumentsResult<T> {
    /**
     * The sequence of results returned by the query.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly results: SuggestResult<T>[];
    /**
     * A value indicating the percentage of the index that was included in the query, or null if
     * minimumCoverage was not set in the request.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly coverage?: number;
}

/**
 * Options for retrieving suggestions based on the searchText.
 */
export declare type SuggestOptions<Fields> = OperationOptions & SuggestRequest<Fields>;

/**
 * Parameters for filtering, sorting, fuzzy matching, and other suggestions query behaviors.
 */
export declare interface SuggestRequest<Fields> {
    /**
     * An OData expression that filters the documents considered for suggestions.
     */
    filter?: string;
    /**
     * A value indicating whether to use fuzzy matching for the suggestion query. Default is false.
     * When set to true, the query will find suggestions even if there's a substituted or missing
     * character in the search text. While this provides a better experience in some scenarios, it
     * comes at a performance cost as fuzzy suggestion searches are slower and consume more
     * resources.
     */
    useFuzzyMatching?: boolean;
    /**
     * A string tag that is appended to hit highlights. Must be set with highlightPreTag. If omitted,
     * hit highlighting of suggestions is disabled.
     */
    highlightPostTag?: string;
    /**
     * A string tag that is prepended to hit highlights. Must be set with highlightPostTag. If
     * omitted, hit highlighting of suggestions is disabled.
     */
    highlightPreTag?: string;
    /**
     * A number between 0 and 100 indicating the percentage of the index that must be covered by a
     * suggestion query in order for the query to be reported as a success. This parameter can be
     * useful for ensuring search availability even for services with only one replica. The default
     * is 80.
     */
    minimumCoverage?: number;
    /**
     * The list of OData $orderby expressions by which to sort the results. Each
     * expression can be either a field name or a call to either the geo.distance() or the
     * search.score() functions. Each expression can be followed by asc to indicate ascending, or
     * desc to indicate descending. The default is ascending order. Ties will be broken by the match
     * scores of documents. If no $orderby is specified, the default sort order is descending by
     * document match score. There can be at most 32 $orderby clauses.
     */
    orderBy?: string[];
    /**
     * The comma-separated list of field names to search for the specified search text. Target fields
     * must be included in the specified suggester.
     */
    searchFields?: Fields[];
    /**
     * The list of fields to retrieve. If unspecified, only the key field will be
     * included in the results.
     */
    select?: Fields[];
    /**
     * The number of suggestions to retrieve. This must be a value between 1 and 100. The default is
     * 5.
     */
    top?: number;
}

/**
 * A result containing a document found by a suggestion query, plus associated metadata.
 */
export declare type SuggestResult<T> = {
    /**
     * The text of the suggestion result.
     * **NOTE: This property will not be serialized. It can only be populated by the server.**
     */
    readonly text: string;
    document: T;
};

/**
 * Represents a synonym map definition.
 */
export declare interface SynonymMap {
    /**
     * The name of the synonym map.
     */
    name: string;
    /**
     * An array of synonym rules in the specified synonym map format.
     */
    synonyms: string[];
    /**
     * A description of an encryption key that you create in Azure Key Vault. This key is used to
     * provide an additional level of encryption-at-rest for your data when you want full assurance
     * that no one, not even Microsoft, can decrypt your data in Azure Cognitive Search. Once you
     * have encrypted your data, it will always remain encrypted. Azure Cognitive Search will ignore
     * attempts to set this property to null. You can change this property as needed if you want to
     * rotate your encryption key; Your data will be unaffected. Encryption with customer-managed
     * keys is not available for free search services, and is only available for paid services
     * created on or after January 1, 2019.
     */
    encryptionKey?: SearchResourceEncryptionKey;
    /**
     * The ETag of the synonym map.
     */
    etag?: string;
}

/**
 * Matches single or multi-word synonyms in a token stream. This token filter is implemented using
 * Apache Lucene.
 */
export declare interface SynonymTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.SynonymTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A list of synonyms in following one of two formats: 1. incredible, unbelievable, fabulous =>
     * amazing - all terms on the left side of => symbol will be replaced with all terms on its right
     * side; 2. incredible, unbelievable, fabulous, amazing - comma separated list of equivalent
     * words. Set the expand option to change how this list is interpreted.
     */
    synonyms: string[];
    /**
     * A value indicating whether to case-fold input for matching. Default is false. Default value:
     * false.
     */
    ignoreCase?: boolean;
    /**
     * A value indicating whether all words in the list of synonyms (if => notation is not used) will
     * map to one another. If true, all words in the list of synonyms (if => notation is not used)
     * will map to one another. The following list: incredible, unbelievable, fabulous, amazing is
     * equivalent to: incredible, unbelievable, fabulous, amazing => incredible, unbelievable,
     * fabulous, amazing. If false, the following list: incredible, unbelievable, fabulous, amazing
     * will be equivalent to: incredible, unbelievable, fabulous, amazing => incredible. Default is
     * true. Default value: true.
     */
    expand?: boolean;
}

/**
 * Defines a function that boosts scores of documents with string values matching a given list of
 * tags.
 */
export declare interface TagScoringFunction {
    /**
     * Polymorphic Discriminator
     */
    type: "tag";
    /**
     * The name of the field used as input to the scoring function.
     */
    fieldName: string;
    /**
     * A multiplier for the raw score. Must be a positive number not equal to 1.0.
     */
    boost: number;
    /**
     * A value indicating how boosting will be interpolated across document scores; defaults to
     * "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
     */
    interpolation?: ScoringFunctionInterpolation;
    /**
     * Parameter values for the tag scoring function.
     */
    parameters: TagScoringParameters;
}

/**
 * Provides parameter values to a tag scoring function.
 */
export declare interface TagScoringParameters {
    /**
     * The name of the parameter passed in search queries to specify the list of tags to compare
     * against the target field.
     */
    tagsParameter: string;
}

/**
 * Defines values for TextSplitMode.
 * Possible values include: 'Pages', 'Sentences'
 * @readonly
 * @enum {string}
 */
export declare type TextSplitMode = 'pages' | 'sentences';

/**
 * A skill to translate text from one language to another.
 */
export declare interface TextTranslationSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Text.TranslationSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * The language code to translate documents into for documents that don't specify the to language
     * explicitly. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca', 'zh-Hans',
     * 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el', 'ht',
     * 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg', 'ms',
     * 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl', 'es',
     * 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'
     */
    defaultToLanguageCode: TextTranslationSkillLanguage;
    /**
     * The language code to translate documents from for documents that don't specify the from
     * language explicitly. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca',
     * 'zh-Hans', 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el',
     * 'ht', 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg',
     * 'ms', 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl',
     * 'es', 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'
     */
    defaultFromLanguageCode?: TextTranslationSkillLanguage;
    /**
     * The language code to translate documents from when neither the fromLanguageCode input nor the
     * defaultFromLanguageCode parameter are provided, and the automatic language detection is
     * unsuccessful. Default is en. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue',
     * 'ca', 'zh-Hans', 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de',
     * 'el', 'ht', 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt',
     * 'mg', 'ms', 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk',
     * 'sl', 'es', 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'
     */
    suggestedFrom?: TextTranslationSkillLanguage;
}

/**
 * Defines values for TextTranslationSkillLanguage.
 * Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca', 'zh-Hans', 'zh-Hant', 'hr',
 * 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el', 'ht', 'he', 'hi', 'mww',
 * 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg', 'ms', 'mt', 'nb', 'fa', 'pl',
 * 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl', 'es', 'sv', 'ty', 'ta', 'te',
 * 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'
 * @readonly
 * @enum {string}
 */
export declare type TextTranslationSkillLanguage = 'af' | 'ar' | 'bn' | 'bs' | 'bg' | 'yue' | 'ca' | 'zh-Hans' | 'zh-Hant' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fj' | 'fil' | 'fi' | 'fr' | 'de' | 'el' | 'ht' | 'he' | 'hi' | 'mww' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'sw' | 'tlh' | 'ko' | 'lv' | 'lt' | 'mg' | 'ms' | 'mt' | 'nb' | 'fa' | 'pl' | 'pt' | 'otq' | 'ro' | 'ru' | 'sm' | 'sr-Cyrl' | 'sr-Latn' | 'sk' | 'sl' | 'es' | 'sv' | 'ty' | 'ta' | 'te' | 'th' | 'to' | 'tr' | 'uk' | 'ur' | 'vi' | 'cy' | 'yua';

/**
 * Defines weights on index fields for which matches should boost scoring in search queries.
 */
export declare interface TextWeights {
    /**
     * The dictionary of per-field weights to boost document scoring. The keys are field names and
     * the values are the weights for each field.
     */
    weights: {
        [propertyName: string]: number;
    };
}

/**
 * Defines values for TokenCharacterKind.
 * Possible values include: 'Letter', 'Digit', 'Whitespace', 'Punctuation', 'Symbol'
 * @readonly
 * @enum {string}
 */
export declare type TokenCharacterKind = 'letter' | 'digit' | 'whitespace' | 'punctuation' | 'symbol';

/**
 * Contains the possible cases for TokenFilter.
 */
export declare type TokenFilter = AsciiFoldingTokenFilter | CjkBigramTokenFilter | CommonGramTokenFilter | DictionaryDecompounderTokenFilter | EdgeNGramTokenFilter | ElisionTokenFilter | KeepTokenFilter | KeywordMarkerTokenFilter | LengthTokenFilter | LimitTokenFilter | NGramTokenFilter | PatternCaptureTokenFilter | PatternReplaceTokenFilter | PhoneticTokenFilter | ShingleTokenFilter | SnowballTokenFilter | StemmerTokenFilter | StemmerOverrideTokenFilter | StopwordsTokenFilter | SynonymTokenFilter | TruncateTokenFilter | UniqueTokenFilter | WordDelimiterTokenFilter;

/**
 * Truncates the terms to a specific length. This token filter is implemented using Apache Lucene.
 */
export declare interface TruncateTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.TruncateTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The length at which terms will be truncated. Default and maximum is 300. Default value: 300.
     */
    length?: number;
}

/**
 * Tokenizes urls and emails as one token. This tokenizer is implemented using Apache Lucene.
 */
export declare interface UaxUrlEmailTokenizer {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.UaxUrlEmailTokenizer";
    /**
     * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The
     * maximum token length that can be used is 300 characters. Default value: 255.
     */
    maxTokenLength?: number;
}

/**
 * Filters out tokens with same text as the previous token. This token filter is implemented using
 * Apache Lucene.
 */
export declare interface UniqueTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.UniqueTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A value indicating whether to remove duplicates only at the same position. Default is false.
     * Default value: false.
     */
    onlyOnSamePosition?: boolean;
}

/**
 * Options for the upload documents operation.
 */
export declare type UploadDocumentsOptions = IndexDocumentsOptions;

/**
 * Defines values for VisualFeature.
 * Possible values include: 'Adult', 'Brands', 'Categories', 'Description', 'Faces', 'Objects',
 * 'Tags'
 * @readonly
 * @enum {string}
 */
export declare type VisualFeature = 'adult' | 'brands' | 'categories' | 'description' | 'faces' | 'objects' | 'tags';

/**
 * A skill that can call a Web API endpoint, allowing you to extend a skillset by having it call
 * your custom code.
 */
export declare interface WebApiSkill {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Skills.Custom.WebApiSkill";
    /**
     * The name of the skill which uniquely identifies it within the skillset. A skill with no name
     * defined will be given a default name of its 1-based index in the skills array, prefixed with
     * the character '#'.
     */
    name?: string;
    /**
     * The description of the skill which describes the inputs, outputs, and usage of the skill.
     */
    description?: string;
    /**
     * Represents the level at which operations take place, such as the document root or document
     * content (for example, /document or /document/content). The default is /document.
     */
    context?: string;
    /**
     * Inputs of the skills could be a column in the source data set, or the output of an upstream
     * skill.
     */
    inputs: InputFieldMappingEntry[];
    /**
     * The output of a skill is either a field in a search index, or a value that can be consumed as
     * an input by another skill.
     */
    outputs: OutputFieldMappingEntry[];
    /**
     * The url for the Web API.
     */
    uri: string;
    /**
     * The headers required to make the http request.
     */
    httpHeaders?: {
        [propertyName: string]: string;
    };
    /**
     * The method for the http request.
     */
    httpMethod?: string;
    /**
     * The desired timeout for the request. Default is 30 seconds.
     */
    timeout?: string;
    /**
     * The desired batch size which indicates number of documents.
     */
    batchSize?: number;
    /**
     * If set, the number of parallel calls that can be made to the Web API.
     */
    degreeOfParallelism?: number;
}

/**
 * Splits words into subwords and performs optional transformations on subword groups. This token
 * filter is implemented using Apache Lucene.
 */
export declare interface WordDelimiterTokenFilter {
    /**
     * Polymorphic Discriminator
     */
    odatatype: "#Microsoft.Azure.Search.WordDelimiterTokenFilter";
    /**
     * The name of the token filter. It must only contain letters, digits, spaces, dashes or
     * underscores, can only start and end with alphanumeric characters, and is limited to 128
     * characters.
     */
    name: string;
    /**
     * A value indicating whether to generate part words. If set, causes parts of words to be
     * generated; for example "AzureSearch" becomes "Azure" "Search". Default is true. Default value:
     * true.
     */
    generateWordParts?: boolean;
    /**
     * A value indicating whether to generate number subwords. Default is true. Default value: true.
     */
    generateNumberParts?: boolean;
    /**
     * A value indicating whether maximum runs of word parts will be catenated. For example, if this
     * is set to true, "Azure-Search" becomes "AzureSearch". Default is false. Default value: false.
     */
    catenateWords?: boolean;
    /**
     * A value indicating whether maximum runs of number parts will be catenated. For example, if
     * this is set to true, "1-2" becomes "12". Default is false. Default value: false.
     */
    catenateNumbers?: boolean;
    /**
     * A value indicating whether all subword parts will be catenated. For example, if this is set to
     * true, "Azure-Search-1" becomes "AzureSearch1". Default is false. Default value: false.
     */
    catenateAll?: boolean;
    /**
     * A value indicating whether to split words on caseChange. For example, if this is set to true,
     * "AzureSearch" becomes "Azure" "Search". Default is true. Default value: true.
     */
    splitOnCaseChange?: boolean;
    /**
     * A value indicating whether original words will be preserved and added to the subword list.
     * Default is false. Default value: false.
     */
    preserveOriginal?: boolean;
    /**
     * A value indicating whether to split on numbers. For example, if this is set to true,
     * "Azure1Search" becomes "Azure" "1" "Search". Default is true. Default value: true.
     */
    splitOnNumerics?: boolean;
    /**
     * A value indicating whether to remove trailing "'s" for each subword. Default is true. Default
     * value: true.
     */
    stemEnglishPossessive?: boolean;
    /**
     * A list of tokens to protect from being delimited.
     */
    protectedWords?: string[];
}

export { }
